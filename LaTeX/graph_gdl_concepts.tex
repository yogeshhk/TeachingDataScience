%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Concepts}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Image Manifold}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{High-Dimensional Representation}
    \begin{itemize}
        \item A 2D point is represented by two values (x, y). They can also be represented by two boxes. First box's black-ness depends on x coordinate and 2nd box's with y coordinate. 0 value means white, 1 means black. 
        \item A 16D point is represented using 16 coordinate values, thus 14 boxes, but can be arranged as 4x4 grid or image.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.45\linewidth,keepaspectratio]{gdl17}
\includegraphics[width=0.45\linewidth,keepaspectratio]{gdl18}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Images as Points in a Hypercube}
    \begin{itemize}
        \item A grayscale image = point in a high-dimensional space.
        \item 256×256 images, each pixel is a dimension.
        \item All images are just rare points in this vast space.
        \item Random points mostly lead to noise, not meaningful images.
		\item So, only some points in this high dimensional (hyper) space are valid images.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl19}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Transition Between Images}
    \begin{itemize}
        \item Linear interpolation between points (images) is often not smooth. That gives wierd images.
        \item Intermediate points may not resemble real faces.
        \item Smooth transitions exist along special curved paths, where all are valid images.
        \item These paths lie on the image manifold. E.g. path between morphing from smiling face  to a non-smiling face. Intermediate points are also valid faces.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl20}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}	


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Understanding the Face Manifold}
    \begin{itemize}
        \item Each realistic face image lies on or near a manifold.
        \item Moving on the manifold changes expressions smoothly.
        \item Moving off the manifold leads to unrealistic images.
        \item Faces = low-dimensional structure in high-dimensional space, aka Parametric Space that's one dimensional manifold versus 2 dimensional Model or Image space.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl21}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Manifolds in Ambient Space}
    \begin{itemize}
        \item Data may lie on a 1D manifold in a 2D space. Underlying geometry.
        \item Example: points following a curved line.
        \item Manifolds are homeomorphic to lower-dimensional Euclidean space.
        \item Local coordinates suffice for representation. Rest of the data is all noise.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl22}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Degrees of Freedom and Natural Data}
    \begin{itemize}
        \item Real-world data has constraints: limited degrees of freedom.
        \item Example: height vs. weight correlation in humans.
        \item Physical laws restrict images: no cubic planets or levitating people.
        \item Natural images cluster on specific manifolds.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl23}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Exploring Manifolds Visually}
    \begin{itemize}
        \item Moving on a manifold preserves realistic image features.
        \item Deviating from the manifold introduces noise or anomalies.
        \item Clock images example: midpoints on/off manifold differ significantly.
        \item On-manifold interpolation leads to plausible results.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{gdl24}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Learning the Manifold}
    \begin{itemize}
        \item ML models aim to approximate data manifolds.
        \item VAEs and GANs learn mappings from latent to pixel space.
        \item Latent space is low-dimensional, manifold is high-dimensional.
        \item Models learn this mapping via optimization, not manual rules.
    \end{itemize}
	
\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{gdl25}

{\tiny (Ref: My Understanding of the Manifold Hypothesis - Kartik Chincholikar)}	

\end{center}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Control and Future Directions}
    \begin{itemize}
        \item Better control of latent features is an active research area.
        \item Goal: steer generation with interpretable latent directions.
        \item Understanding manifolds improves generative model performance.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Manifold vs Graph}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Manifolds vs. Graphs: Basic Idea}
    \begin{itemize}
        \item Manifolds are continuous spaces that locally resemble Euclidean space.
        \item Graphs are discrete structures made of nodes and edges.
        \item Manifolds generalize smooth surfaces and curves.
        \item Graphs represent relationships or connections in data.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is a Manifold?}
    \begin{itemize}
        \item A manifold locally looks like \(\mathbb{R}^n\), the Euclidean space.
        \item Every small neighborhood on the manifold is flat.
        \item Tangent space and curvature can be defined on smooth manifolds.
        \item Common in geometry, physics, and ML.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Examples of Manifolds}
    \begin{itemize}
        \item A sphere is a 2D manifold—locally flat, globally curved.
        \item A torus (donut surface) is also a 2D manifold.
        \item Any surface where local patches resemble \(\mathbb{R}^2\).
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is a Graph?}
    \begin{itemize}
        \item Graphs consist of vertices (nodes) and edges (links).
        \item No notion of continuity or smoothness.
        \item Structure depends on how nodes are connected.
        \item Used to represent social networks, maps, trees, etc.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Examples of Graphs}
    \begin{itemize}
        \item Social network: users are nodes, friendships are edges.
        \item Road maps: cities as nodes, roads as edges.
        \item Family trees: people as nodes, lineage as edges.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Key Differences}
    \begin{itemize}
        \item Manifolds are continuous; graphs are discrete.
        \item Manifolds often support calculus; graphs don't.
        \item Manifolds have local Euclidean charts; graphs have neighborhoods.
        \item Different math tools: topology vs. graph theory.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Connecting Manifolds and Graphs}
    \begin{itemize}
        \item Graphs can approximate manifolds in data analysis.
        \item Manifolds can be seen as continuous analogs of graphs.
        \item Concepts can sometimes translate between the two.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Applications of Manifolds}
    \begin{itemize}
        \item Used in differential geometry and general relativity.
        \item Describe smooth, high-dimensional data in ML.
        \item Essential in algorithms like PCA, LLE, and t-SNE.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Applications of Graphs}
    \begin{itemize}
        \item Data structures in computer science and AI.
        \item Modeling networks: internet, traffic, biology.
        \item Powering algorithms like PageRank, graph neural networks.
    \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Lie Groups}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Symmetry in Machine Learning}
  \begin{itemize}
    \item Symmetries help simplify models and reduce redundancy.
    \item Many datasets exhibit invariance (e.g., to rotation or translation).
    \item Identifying symmetries aids in architecture design and efficiency.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Lie Groups: A Primer}
  \begin{itemize}
    \item Lie groups combine group theory and smooth manifolds.
    \item Enable continuous, differentiable transformations.
    \item Key to modeling continuous symmetries in data and physics.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Core Properties of Lie Groups}
  \begin{itemize}
    \item Differentiable structure enables calculus on groups.
    \item Group properties: closure, identity, inverse, associativity.
    \item Lie algebra encodes local, infinitesimal transformations.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Lie Groups in ML: Data Invariance}
  \begin{itemize}
    \item Model invariance via group-equivariant architectures.
    \item Use in image processing: invariance to rotation, translation.
    \item Strengthens generalization by embedding known symmetries.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geometric Deep Learning}
  \begin{itemize}
    \item Uses Lie groups to model data on manifolds and graphs.
    \item SE(3)-equivariant networks preserve 3D physical structure.
    \item Applied in molecular modeling and structural biology.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Lie Groups in Optimization}
  \begin{itemize}
    \item Parameters evolve on curved manifolds (Riemannian opt.).
    \item Lie algebra enables structured gradient computation.
    \item Exponential/log maps link tangent space and group.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Physics-Inspired Applications}
  \begin{itemize}
    \item Used in reinforcement learning and robot control.
    \item Rigid body motion modeled using Lie groups.
    \item Critical for perception and dynamics in robotics.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Manifolds: Geometry Behind ML}
  \begin{itemize}
    \item Manifolds locally resemble Euclidean space.
    \item Enable modeling of complex, nonlinear data spaces.
    \item Examples: circle, sphere, space-time.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Differential \& Riemannian Manifolds}
  \begin{itemize}
    \item Smooth manifolds allow calculus via local coordinates.
    \item Riemannian manifolds include a metric for distances/angles.
    \item Basis for geometry-aware machine learning.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Tangent Spaces and Geodesics}
  \begin{itemize}
    \item Tangent space: local linear approximation of a manifold.
    \item Tangent vectors act as directional derivatives.
    \item Geodesics represent shortest paths on curved spaces.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geomstats: A Practical Library}
  \begin{itemize}
    \item Open-source Python library for geometric ML.
    \item Supports Lie groups, manifolds, and Riemannian geometry.
    \item Follows Scikit-Learn API for easy integration.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geomstats for Learning \& Research}
  \begin{itemize}
    \item Ideal for hands-on exploration of geometric learning.
    \item Facilitates research in symmetry-aware ML models.
    \item Bridges theory and practice in geometric deep learning.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large GDL approaches}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Graph Neural Networks (GNNs)}
  \begin{itemize}
    \item GNNs process data on non-Euclidean domains via graph structures.
    \item Useful when data lies on a manifold approximated by a graph.
    \item Extend neural networks to irregular data topologies.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Types of Graph Neural Networks}
  \begin{itemize}
    \item \textbf{GCNs:} Convolutions on graphs via adjacency matrices.
    \item \textbf{GATs:} Attention-based weighting of neighbors.
    \item \textbf{GraphSAGE:} Sampling-based aggregation for scalability.
    \item \textbf{GINs:} Powerful graph structure discrimination.
    \item \textbf{SGNNs:} Operate in spectral domain using Laplacians.
    \item \textbf{Graph Pooling:} Global/hierarchical graph summarization.
    \item \textbf{Hyperbolic GNNs:} Encode tree-like, hierarchical data.
    \item \textbf{Dynamic GNNs:} For evolving temporal graphs.
    \item \textbf{R-GCNs:} Handle heterogeneous nodes/edges.
    \item \textbf{Graph Transformers:} Adapt Transformer to graphs.
    \item \textbf{Graph Autoencoders:} Learn unsupervised embeddings.
    \item \textbf{Diffusion GNNs:} Use diffusion processes for message passing.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Topological Data Analysis (TDA)}
  \begin{itemize}
    \item Uses algebraic topology to study data shape and structure.
    \item Captures global properties like clusters, loops, voids.
    \item Extracts stable, noise-resistant features from high-dimensional data.
    \item Enables dimensionality reduction while preserving topology.
    \item Extends into Topological Deep Learning with hierarchical abstractions.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Benefits of TDA in ML}
  \begin{itemize}
    \item Reveals structure invisible to traditional ML methods.
    \item Robust to noise and works on various data types.
    \item Enhances interpretability and feature quality.
    \item Detects nonlinear patterns missed by PCA, regression.
    \item Libraries: \texttt{GUDHI}, \texttt{scikit-tda}.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Manifolds in ML}
  \begin{itemize}
    \item A manifold is a locally Euclidean space; many datasets lie on manifolds.
    \item Riemannian manifolds provide metric structure for curvature and gradients.
    \item Latent space in deep models often resides on manifolds.
    \item Estimation tasks like kernel density and classification benefit from manifold-aware geometry.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Analyze on Manifolds?}
  \begin{itemize}
    \item Reduces system complexity and dimensionality.
    \item Improves understanding of temporal evolution of data.
    \item Enhances extrapolation and prediction of future data points.
    \item Libraries: \texttt{Geomstats}, \texttt{PyTorch Geometric}.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Mesh \& Grid-Based Models}
  \begin{itemize}
    \item Special cases of Geometric Deep Learning applied to 3D domains.
    \item \textbf{Mesh:} Collection of vertices, edges, faces for 3D objects.
    \item \textbf{Grid:} Structured data format (e.g., pixels, voxels).
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Mesh-Based Learning}
  \begin{itemize}
    \item Preserves topology and local geometry.
    \item Ideal for irregular surfaces: 3D scans, medical imaging, CAD.
    \item \textbf{Model types:}
    \begin{itemize}
      \item GNNs on meshes (mesh as graph)
      \item Spectral mesh models using Laplacians
      \item Geometric message-passing models
    \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Grid-Based Learning}
  \begin{itemize}
    \item Regular structure suited to natural image/video representations.
    \item \textbf{Common methods:}
    \begin{itemize}
      \item Voxel-based 3D CNNs
      \item Implicit representations via signed distance functions
      \item Physics-based simulations in a structured grid
    \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Applications of Geometric Models}
  \small
    \begin{itemize}
      \item Community detection
      \item Product recommendations
      \item Drug discovery \& repurposing
      \item Protein folding prediction
      \item Gene regulatory networks
      \item Action \& pose recognition
      \item Fraud detection
      \item Portfolio optimization
      \item Circuit design
      \item Risk modeling
      \item Medical diagnosis
      \item Patient care optimization
      \item Knowledge graphs
      \item Research collaboration analysis
      \item Cosmic network analysis
      \item Energy grid optimization
      \item Text classification
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Manifold Learning Applications}
  \small
    \begin{itemize}
      \item Robot trajectory optimization
      \item 3D pose estimation (SO(3), SE(3))
      \item Shape analysis \& alignment
      \item Fluid \& physics simulations
      \item Manifold-based data augmentation
      \item Brain signal decoding (EEG/MEG)
      \item Matrix factorization
      \item Persistent homology
      \item Manifold-aware transformers
      \item Hyperbolic embeddings
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Libraries \& Frameworks}
  \begin{itemize}
    \item \textbf{diffgeom:} Symbolic differential geometry (PyPi).
    \item \textbf{SymPy:} Symbolic math with topology/geometry support.
    \item \textbf{Geomstats:} Computation on nonlinear manifolds.
    \item \textbf{PyG:} PyTorch Geometric for GNNs.
    \item \textbf{PyRiemann:} ML on SPD matrices (Riemannian).
    \item \textbf{PyManOpt:} Optimization on manifolds.
    \item \textbf{GUDHI:} Topological data analysis and geometry.
  \end{itemize}
\end{frame}





















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Geometric Deep Learning: From Grids to Gauges}
\vspace{0.5cm}

{\em Unifying Neural Architectures Through Geometric Principles}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Fundamental Challenge}

\begin{center}
\textbf{Why do we need so many different neural network architectures?}
\end{center}

\begin{columns}
\begin{column}{0.5\textwidth}
\begin{itemize}
\item MLPs: Tabular data
\item CNNs: Images 
\item RNNs: Sequences
\item Transformers: Sets with attention
\item GNNs: Graphs
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{architectures_evolution}
\end{center}
\end{column}
\end{columns}

\vspace{0.5cm}
\begin{alertblock}{Key Insight}
Each architecture encodes specific \textbf{geometric assumptions} about the data structure!
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Has Shape, Shape Has Meaning}

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Data Type} & \textbf{Structure} & \textbf{Key Property} & \textbf{Architecture} \\
\hline
Tabular & Fixed vectors & Independence & MLP \\
Images & Regular grid & Translation & CNN \\
Sequences & 1D chain & Order & RNN \\
Sets & Unordered & Permutation & Attention \\
Graphs & Irregular & Connectivity & GNN \\
\hline
\end{tabular}
\end{center}

\vspace{0.5cm}

\begin{block}{The Geometric Deep Learning Hypothesis}
\begin{center}
\textbf{The success of a neural architecture is determined by how well its geometric biases match the geometric structure of the data.}
\end{center}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Felix Klein's Vision: The Erlangen Program (1872)}

\begin{columns}
\begin{column}{0.6\textwidth}
\textbf{Klein's Revolutionary Idea:}
\begin{itemize}
\item Classify geometries by their \textbf{symmetry groups}
\item A geometry = invariant properties under transformations
\item Unified framework for all geometric spaces
\end{itemize}

\vspace{0.5cm}
\textbf{Modern Translation:}
\begin{itemize}
\item Classify neural networks by their \textbf{equivariances}
\item An architecture = invariant computations under transformations  
\item Unified framework for all data domains
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{klein_program}

\vspace{0.5cm}
{\small "Geometry is the study of invariant properties under group actions"}
\end{center}
\end{column}
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The 5G Framework: Grids, Groups, Graphs, Geodesics, Gauges}

\begin{center}
\begin{tikzpicture}
\node[draw,circle,minimum size=1cm] (grid) at (0,0) {\textbf{Grid}};
\node[draw,circle,minimum size=1cm] (group) at (2,0) {\textbf{Group}};
\node[draw,circle,minimum size=1cm] (graph) at (4,0) {\textbf{Graph}};
\node[draw,circle,minimum size=1cm] (geodesic) at (6,0) {\textbf{Geodesic}};
\node[draw,circle,minimum size=1cm] (gauge) at (8,0) {\textbf{Gauge}};

\draw[->] (grid) -- (group);
\draw[->] (group) -- (graph);
\draw[->] (graph) -- (geodesic);
\draw[->] (geodesic) -- (gauge);

\node[below=0.5cm] at (0,0) {CNNs};
\node[below=0.5cm] at (2,0) {Spherical CNNs};
\node[below=0.5cm] at (4,0) {GNNs};
\node[below=0.5cm] at (6,0) {Mesh CNNs};
\node[below=0.5cm] at (8,0) {Gauge CNNs};
\end{tikzpicture}
\end{center}

\vspace{0.5cm}

\begin{block}{Progression of Geometric Complexity}
\begin{itemize}
\item \textbf{Grid}: Regular lattices (images) → Translation equivariance
\item \textbf{Group}: Homogeneous spaces (sphere) → Group equivariance  
\item \textbf{Graph}: Discrete structures → Permutation invariance
\item \textbf{Geodesic}: Continuous manifolds → Diffeomorphism invariance
\item \textbf{Gauge}: Vector bundles → Gauge equivariance
\end{itemize}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{From Invariance to Equivariance}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Invariance:} Output unchanged
$$f(g \cdot x) = f(x)$$

\textbf{Equivariance:} Output transforms predictably  
$$f(g \cdot x) = \rho(g) \cdot f(x)$$

where $g \in G$ (group), $\rho$ is representation
\end{column}
\begin{column}{0.5\textwidth}
% \begin{center}
% \includegraphics[width=\linewidth]{invariance_equivariance}
% \end{center}
\end{column}
\end{columns}

\vspace{0.5cm}

\begin{exampleblock}{CNN Translation Equivariance}
If we translate input image → CNN features translate by same amount

\texttt{conv(translate(image)) = translate(conv(image))}
\end{exampleblock}

\begin{alertblock}{Why Equivariance > Invariance?}
Equivariance preserves spatial relationships, enabling hierarchical processing
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Permutation Challenge}

\begin{center}
\textbf{Consider a set $\{A, B, C\}$ - How many ways to order it?}
\end{center}

\begin{columns}
\begin{column}{0.35\textwidth}
\textbf{All permutations:}
\begin{itemize}
\item $\{A, B, C\}$
\item $\{A, C, B\}$  
\item $\{B, A, C\}$
\item $\{B, C, A\}$
\item $\{C, A, B\}$
\item $\{C, B, A\}$
\end{itemize}

\textbf{Total: $3! = 6$ orderings}
\end{column}
\begin{column}{0.65\textwidth}
\textbf{Traditional ML approach:}
\begin{itemize}
\item Learn function for each ordering
\item Requires 6× more data
\item No generalization across orderings
\end{itemize}

\vspace{0.2cm}
\textbf{Geometric ML approach:}
\begin{itemize}
\item Learn \textbf{permutation invariant} function
\item Same function works for all orderings
\item Dramatic sample efficiency gain
\end{itemize}
\end{column}
\end{columns}

\begin{block}{Mathematical Formulation}
$$f(\pi \cdot X) = f(X) \quad \forall \pi \in S_n$$
where $\pi$ is any permutation, $S_n$ is symmetric group
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Grid Domain: CNNs and Translation Equivariance}

\begin{center}
\textbf{Regular Grid Structure}
\end{center}

\begin{columns}
\begin{column}{0.6\textwidth}
\textbf{Key Properties:}
\begin{itemize}
\item Fixed neighborhood structure
\item Translation symmetry
\item Local connectivity
\item Regular sampling
\end{itemize}

% \vspace{0.5cm}
\textbf{CNN Operation:}
$$(\mathbf{f} * \mathbf{g})[i,j] = \sum_{m,n} \mathbf{f}[m,n] \cdot \mathbf{g}[i-m, j-n]$$

\textbf{Equivariance Property:}
$$\text{Conv}(T_v(\mathbf{x})) = T_v(\text{Conv}(\mathbf{x}))$$
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{cnn_grid}

\vspace{0.5cm}
{\small Convolution preserves spatial relationships}
\end{center}
\end{column}
\end{columns}

\begin{alertblock}{Limitation}
CNNs fail on irregular, non-grid data (graphs, point clouds, meshes)
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Group Domain: Beyond Translation}

\begin{center}
\textbf{Homogeneous Spaces with Global Symmetries}
\end{center}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Examples:}
\begin{itemize}
\item \textbf{Sphere}: $SO(3)$ rotations
\item \textbf{Scale space}: Scaling transformations  
\item \textbf{Roto-translation}: $SE(2)$ group
\end{itemize}

% \vspace{0.5cm}
\textbf{Group Convolution:}
$$[f *_G \psi](g) = \int_G f(h) \psi(g^{-1}h) dh$$

\textbf{Applications:}
\begin{itemize}
\item Spherical CNNs for 360° images
\item Scale-equivariant networks
\item Rotation-invariant vision
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{spherical_cnn}

\vspace{0.5cm}
{\small Spherical convolution on global data}
\end{center}
\end{column}
\end{columns}

\begin{block}{Key Insight}
Group equivariance generalizes translation equivariance to richer symmetries
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Graph Domain: Permutation Invariance}

\begin{center}
\textbf{Irregular Connectivity Structures}
\end{center}

\begin{columns}
\begin{column}{0.6\textwidth}
\textbf{Key Challenges:}
\begin{itemize}
\item Variable node degrees
\item No canonical ordering
\item Irregular neighborhoods
\item Arbitrary connectivity patterns
\end{itemize}

% \vspace{0.5cm}
\textbf{Graph Convolution:}
$$\mathbf{H}^{(l+1)} = \sigma\left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}\mathbf{H}^{(l)}\mathbf{W}^{(l)}\right)$$


\end{column}
\begin{column}{0.4\textwidth}
{\small Graph with irregular connectivity}

\begin{center}
\includegraphics[width=\linewidth]{graph_structure}
\end{center}

\textbf{Permutation Invariance:}
$$f(\mathbf{P}\mathbf{X}, \mathbf{P}\mathbf{A}\mathbf{P}^T) = f(\mathbf{X}, \mathbf{A})$$
\end{column}
\end{columns}

\begin{exampleblock}{Message Passing Framework}
$$\mathbf{m}_{ij}^{(l)} = M^{(l)}(\mathbf{h}_i^{(l)}, \mathbf{h}_j^{(l)}, \mathbf{e}_{ij})$$
$$\mathbf{h}_i^{(l+1)} = U^{(l)}\left(\mathbf{h}_i^{(l)}, \text{AGG}\left(\{\mathbf{m}_{ij}^{(l)} : j \in \mathcal{N}(i)\}\right)\right)$$
\end{exampleblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Geodesic Domain: Manifold Learning}

\begin{center}
\textbf{Continuous Curved Spaces}
\end{center}

\begin{columns}
\begin{column}{0.4\textwidth}
\textbf{Key Concepts:}
\begin{itemize}
\item Riemannian manifolds
\item Geodesic distances
\item Intrinsic geometry
\item Curvature effects
\end{itemize}

% \vspace{0.5cm}

\textbf{Applications:}
\begin{itemize}
\item 3D shape analysis
\item Molecular surfaces
\item Brain cortex modeling
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}
{\small 3D mesh with geodesic paths}

\begin{center}
\includegraphics[width=0.5\linewidth]{manifold_mesh}
\end{center}

\textbf{Geodesic Convolution:}
$$[f * \psi]_{\mathcal{M}}(x) = \int_{\mathcal{M}} f(y) \psi(d_{\mathcal{M}}(x,y)) d\mu(y)$$

\end{column}
\end{columns}

\begin{block}{Diffeomorphism Invariance}
Functions invariant to smooth, invertible deformations of the manifold
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Gauge Domain: Vector Bundles}

\begin{center}
\textbf{Most General Geometric Structure}
\end{center}


\textbf{Key Concepts:}
\begin{itemize}
\item Vector bundles over manifolds
\item Gauge transformations
\item Connection and curvature
\item Parallel transport
\end{itemize}

% \vspace{0.5cm}
\textbf{Gauge Equivariant Convolution:}
$$[f *_{\text{gauge}} \psi](x) = \int_{\mathcal{M}} \Phi(x,y) f(y) \psi(x,y) d\mu(y)$$

where $\Phi(x,y)$ is parallel transport operator

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Gauge Domain: Vector Bundles}

\begin{columns}
\begin{column}{0.6\textwidth}
\textbf{Applications:}
\begin{itemize}
\item Weather prediction on sphere
\item Fluid dynamics
\item Electromagnetic fields
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width=\linewidth]{vector_bundle}

% \vspace{0.5cm}
{\small Vector bundle with gauge connection}
\end{center}
\end{column}
\end{columns}

\begin{alertblock}{Ultimate Generalization}
Gauge networks represent the most general form of geometric deep learning
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Unified Message Passing Framework}

\begin{center}
\textbf{All Geometric Networks Share Common Structure}
\end{center}

\begin{block}{General Message Passing Layer}
\begin{align}
\mathbf{m}_{ij}^{(l)} &= M^{(l)}(\mathbf{h}_i^{(l)}, \mathbf{h}_j^{(l)}, \mathbf{e}_{ij}) \\
\mathbf{h}_i^{(l+1)} &= U^{(l)}\left(\mathbf{h}_i^{(l)}, \text{AGG}\left(\{\mathbf{m}_{ij}^{(l)} : j \in \mathcal{N}(i)\}\right)\right)
\end{align}
\end{block}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Domain} & \textbf{Neighborhood $\mathcal{N}(i)$} & \textbf{Message Function $M$} \\
\hline
Grid & Fixed spatial neighbors & Convolution kernel \\
Group & Group orbit & Group convolution \\
Graph & Adjacent nodes & Learned function \\
Geodesic & Geodesic neighbors & Distance-based \\
Gauge & Parallel transport & Gauge connection \\
\hline
\end{tabular}
\end{center}

\begin{alertblock}{Universal Framework}
The 5G domains differ only in their definition of neighborhood and message function!
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Spectral vs Spatial Graph Convolutions}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Spectral Approach:}
\begin{itemize}
\item Based on graph Fourier transform
\item Uses eigen-decomposition of Laplacian
\item Global receptive field
\item Computationally expensive
\end{itemize}

\textbf{Graph Fourier Transform:}
$\hat{f}(\lambda_i) = \sum_{j=1}^n f(j) u_i(j)$

\textbf{Spectral Convolution:}
$g_\theta * f = U g_\theta(\Lambda) U^T f$
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Spatial Approach:}
\begin{itemize}
\item Direct neighborhood aggregation
\item Local message passing
\item Inductive capability
\item Computationally efficient
\end{itemize}

\textbf{Spatial Convolution:}
$h_i^{(l+1)} = \sigma\left(\sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{d_i d_j}} h_j^{(l)} W^{(l)}\right)$
\end{column}
\end{columns}

% \vspace{0.5cm}

\begin{center}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Property} & \textbf{Spectral} & \textbf{Spatial} \\
\hline
Localization & Global & Local \\
Inductive & No & Yes \\
Efficiency & Low & High \\
Interpretability & Mathematical & Intuitive \\
\hline
\end{tabular}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Success Stories Across Domains}

\begin{center}
\textbf{Geometric Deep Learning in Action}
\end{center}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Computer Vision:}
\begin{itemize}
\item Spherical CNNs for 360° video
\item Scale-equivariant detection
\item 3D shape analysis
\end{itemize}

\textbf{Natural Language:}
\begin{itemize}
\item Graph attention networks
\item Syntactic parsing trees
\item Knowledge graph reasoning
\end{itemize}

\textbf{Physical Sciences:}
\begin{itemize}
\item Molecular property prediction
\item Climate modeling on sphere
\item Particle physics simulations
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Social Networks:}
\begin{itemize}
\item Social influence modeling
\item Recommendation systems
\item Community detection
\end{itemize}

\textbf{Biology:}
\begin{itemize}
\item Protein folding prediction
\item Drug discovery
\item Brain network analysis
\end{itemize}

\textbf{Robotics:}
\begin{itemize}
\item Manipulation planning
\item SLAM with geometric constraints
\item Multi-agent coordination
\end{itemize}
\end{column}
\end{columns}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Key Success Factors}

\begin{enumerate}
\item Matching geometric biases to data structure
\item Exploiting inherent symmetries
\item Achieving sample efficiency through invariances
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Current Challenges and Research Frontiers}

\begin{columns}
\begin{column}{0.5\textwidth}
\textbf{Theoretical Challenges:}
\begin{itemize}
\item Expressivity vs efficiency tradeoffs
\item Universal approximation on manifolds
\item Stability analysis for geometric networks
\item Generalization bounds
\end{itemize}

\textbf{Computational Challenges:}
\begin{itemize}
\item Scalability to large graphs
\item GPU-efficient implementations
\item Memory complexity issues
\item Distributed training strategies
\end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
\textbf{Application Challenges:}
\begin{itemize}
\item Heterogeneous data integration
\item Dynamic graph evolution
\item Multi-scale representations
\item Uncertainty quantification
\end{itemize}

\textbf{Future Directions:}
\begin{itemize}
\item Automated architecture search
\item Continuous-discrete bridges
\item Quantum geometric learning
\item Causal geometric models
\end{itemize}
\end{column}
\end{columns}

\begin{alertblock}{The Grand Challenge}
\begin{center}
Can we develop a unified theory that automatically discovers the right geometric structure for any given data and task?
\end{center}
\end{alertblock}

\end{frame}
