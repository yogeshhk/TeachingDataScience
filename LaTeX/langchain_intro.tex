%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction to LangChain}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{What is LangChain?}

\textbf{LangChain}: A comprehensive framework for building LLM-powered applications

\begin{itemize}
\item \textbf{Core Purpose}: Simplify development of applications using Large Language Models
\item \textbf{Key Principles}:
    \begin{itemize}
    \item \textbf{Data Aware}: Connect language models to external data sources
    \item \textbf{Agentic}: Allow language models to interact with their environment
    \end{itemize}
\item \textbf{Main Features}:
    \begin{itemize}
    \item Generic interface to various foundation models
    \item Advanced prompt management framework
    \item Central interface to memory, external data, and agents
    \end{itemize}
\item \textbf{Availability}: Python and JavaScript libraries
\item \textbf{Open Source}: MIT License, created by Harrison Chase
\item \textbf{Repository}: https://github.com/langchain-ai/langchain
\end{itemize}

{\tiny (Ref: Getting Started with LangChain: A Beginner's Guide to Building LLM-Powered Applications)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangChain vs LLMs vs ChatGPT}

{\tiny (Ref: LangChain 101: Building Simple Q\&A App)}


\begin{lstlisting}[language=python, basicstyle=\tiny]
+==========+========================+====================+====================+
|          | LangChain              | LLM                | ChatGPT            | 
+==========+========================+====================+====================+
| Type     | Framework              | Model              | Model              | 
+----------+------------------------+--------------------+--------------------+
| Purpose  | Build applications     | Generate text      | Generate chat      | 
|          | with LLMs              |                    | conversations      | 
+----------+------------------------+--------------------+--------------------+
| Features | Chains, prompts, LLMs, | Large dataset of   | Large dataset of   | 
|          | memory, index, agents  | text and code      | chat conversations | 
+----------+------------------------+--------------------+--------------------+
| Pros     | Composable, modular,   | Generates nearly   | Generates realistic| 
|          | production-ready       | human-quality text | chat conversations | 
+----------+------------------------+--------------------+--------------------+
| Use Case | Building RAG apps,     | Direct text        | Conversational AI  | 
|          | agents, workflows      | generation         | applications       | 
+----------+------------------------+--------------------+--------------------+
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why You Need LangChain}
    \begin{itemize}
        \item Chatbots need to manage chat history and company knowledge bases.
        \item LLMs alone lack context, memory, and retrieval abilities.
        \item Storing, retrieving, and reasoning over data is complex manually.
        \item LangChain acts as an abstraction layer for building AI agents.
        \item It integrates LLMs, memory, and tools seamlessly.
        \item Enables vendor flexibility — easy switch between OpenAI, Anthropic, Gemini.
        \item Reduces code complexity and accelerates AI app development.
        \item Simplifies connecting models, databases, and APIs into a single framework.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLMs vs Agentic Software}
    \begin{itemize}
        \item LLMs are static — respond from training data without awareness.
        \item Agents have autonomy, memory, and tool use for dynamic actions.
        \item Example: Refund query — agent retrieves policy, product, and chat history.
        \item Agents can access vector databases and retrieve company knowledge.
        \item LangChain enables memory persistence across user conversations.
        \item Traditional software follows fixed logic; agents make adaptive decisions.
        \item Agentic software uses modular components (LLMs, memory, retrievers).
        \item LangChain provides prebuilt tools for these agentic capabilities.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangChain Core Components and Labs}
    \begin{itemize}
        \item Core modules: LLM connectors, memory, embeddings, vector stores.
        \item Simplifies API setup — e.g., ChatOpenAI() replaces manual integration.
        \item Supports databases like Chroma or Pinecone for knowledge retrieval.
        \item Prompt templates manage dynamic, reusable prompts and chat context.
        \item LCEL (LangChain Expression Language) builds clean data pipelines.
        \item Enables async, streaming, and batch workflows with type safety.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{So, Why LangChain?}

  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{itemize}
        \item \textbf{RAG Applications}: Build Retrieval Augmented Generation apps with external data
        \item \textbf{Agent Systems}: Create intelligent agents with tool access
        \item \textbf{Production Ready}: Logging, callbacks, monitoring built-in
        \item \textbf{Model Agnostic}: Switch between LLM providers easily
        \item \textbf{Modular Design}: Compose components as needed
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
			\begin{center}
			\includegraphics[width=\linewidth,keepaspectratio]{langchain15}
			\end{center}	  
			{\tiny (Ref: LangChain tutorial: Build an LLM-powered app)}
    \end{column}
  \end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangChain Community \& Ecosystem}

\begin{itemize}
\item \textbf{Community Growth}:
	\begin{itemize}
	\item Over 80,000+ GitHub stars (as of 2024)
	\item 2,000+ contributors
	\item Millions of monthly downloads
	\item Active Discord, Twitter/X presence
	\end{itemize}
	
\item \textbf{License}: MIT License - freely modifiable and commercial-friendly

\item \textbf{Ecosystem Components}:
	\begin{itemize}
	\item \textbf{LangChain Core}: Foundation library
	\item \textbf{LangGraph}: Stateful, multi-actor applications
	\item \textbf{LangServe}: Deploy as REST APIs
	\item \textbf{LangSmith}: Debugging, testing, monitoring
	\end{itemize}
\end{itemize}

{\tiny (Ref: What is Langchain and why should I care as a developer? - Logan Kilpatrick)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Installation \& Setup}

\textbf{Prerequisites:}
\begin{itemize}
\item Python version $\geq 3.9$ and $< 4.0$
\item Modern Installation (2024)
\item {\tiny (Ref: Official LangChain Documentation 2024)}
\end{itemize}

\textbf{:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
# Core packages
pip install langchain langchain-core

# Provider-specific packages for Groq
pip install langchain-groq

# Community integrations for embeddings
pip install langchain-community langchain-huggingface sentence-transformers

# Optional components
pip install chromadb              # Vector store
pip install langchain-text-splitters  # Document processing

//API Keys Setup:

import os
os.environ["GROQ_API_KEY"] = "your-groq-api-key-here"
# Or use .env file with python-dotenv
\end{lstlisting}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Core Components Overview}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{langchain16}
\end{center}

\begin{itemize}
\item \textbf{Models}: LLMs, Chat Models, Embeddings
\item \textbf{Prompts}: Template management and optimization
\item \textbf{Chains/LCEL}: Compose components with pipes
\item \textbf{Memory}: Conversation state management
\item \textbf{Retrievers}: Access external data
\item \textbf{Agents}: Dynamic tool selection and execution
\end{itemize}

{\tiny (Ref: How LangChain Makes Large Language Models More Powerful)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Modern LangChain: LCEL (LangChain Expression Language)}

\textbf{What is LCEL?}
\begin{itemize}
\item Modern, declarative way to build chains (introduced 2023)
\item Uses pipe operator \texttt{|} to chain components
\item Replaces old \texttt{LLMChain} pattern
\item Built-in streaming, async, and batch support
\end{itemize}

\textbf{Key Benefits:}
\begin{itemize}
\item \textbf{Simplicity}: More readable and concise
\item \textbf{Streaming}: Built-in streaming by default
\item \textbf{Async}: Native async/await support
\item \textbf{Observability}: Better tracing and debugging
\item \textbf{Fallbacks}: Easy error handling and retries
\end{itemize}

{\tiny (Ref: LangChain LCEL Documentation 2024)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Basic Example}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Old Pattern (Deprecated):
from langchain.llms import OpenAI
from langchain.chains import LLMChain

llm = OpenAI()
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run("input")

// Modern LCEL Pattern with Groq:

from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Set up Groq model, e.g., Gemma or Llama 3
llm = ChatGroq(model_name="gemma-7b-it")
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
output_parser = StrOutputParser()

# Build chain with pipe operator
chain = prompt | llm | output_parser

# Invoke the chain
result = chain.invoke({"topic": "LangChain"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Advanced Features}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Streaming Support:

chain = prompt | llm | output_parser

# Stream tokens as they're generated
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)

// Async Execution:
# Async invocation
result = await chain.ainvoke({"topic": "AI"})

# Async streaming
async for chunk in chain.astream({"topic": "AI"}):
    print(chunk, end="", flush=True)

// Batch Processing:
# Process multiple inputs in parallel
results = chain.batch([
    {"topic": "AI"},
    {"topic": "ML"},
    {"topic": "LangChain"}
])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Complex Chains}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Multi-step Chain with RunnablePassthrough:
from langchain_core.runnables import RunnablePassthrough

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

result = chain.invoke("What is LangChain?")

// Parallel Execution with RunnableParallel:

from langchain_core.runnables import RunnableParallel

chain = RunnableParallel(
    summary=prompt1 | llm | output_parser,
    keywords=prompt2 | llm | output_parser
)

result = chain.invoke({"text": "Long document..."})
# Returns: {"summary": "...", "keywords": "..."}
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Model Integration: Modern Approach}

\begin{lstlisting}[language=python, basicstyle=\tiny]
Updated Import Structure:

# OpenAI models
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# Hugging Face models
from langchain_huggingface import HuggingFaceEndpoint

# Google models (Gemini)
from langchain_google_genai import ChatGoogleGenerativeAI

# Anthropic models
from langchain_anthropic import ChatAnthropic

// Example Usage:
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

chat = ChatOpenAI(model="gpt-4", temperature=0.7)

messages = [
    SystemMessage(content="You are a helpful assistant"),
    HumanMessage(content="Explain LangChain in 2 sentences")
]

response = chat.invoke(messages)
print(response.content)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Quick Start Example}

\textbf{Complete RAG Application in Modern LangChain with Groq:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Load and process documents
loader = WebBaseLoader("https://example.com/doc")
docs = loader.load()
splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
splits = splitter.split_documents(docs)

# Create vector store with open-source embeddings
embeddings = HuggingFaceEmbeddings()
vectorstore = Chroma.from_documents(splits, embeddings)
retriever = vectorstore.as_retriever()

# Build RAG chain
prompt = ChatPromptTemplate.from_template("""
Answer based on context: {context}
Question: {question}
""")
model = ChatGroq(model_name="llama3-8b-8192")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt | model | StrOutputParser()
)

response = chain.invoke("What is the main topic?")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Key Concepts: Chains vs Agents}

\begin{itemize}
\item \textbf{Chains (LCEL)}:
    \begin{itemize}
    \item Predetermined sequence of operations
    \item Composed with pipe operator: \texttt{prompt | llm | parser}
    \item Fixed execution path
    \item Best for: Structured, predictable workflows
    \end{itemize}

\item \textbf{Agents}:
    \begin{itemize}
    \item Dynamic decision-making with LLM reasoning
    \item Choose tools based on input
    \item Adaptive execution path
    \item Best for: Complex, unpredictable scenarios
    \end{itemize}
\end{itemize}

\textbf{When to use what?}
\begin{itemize}
\item Use \textbf{Chains/LCEL} when: Steps are known, workflow is fixed
\item Use \textbf{Agents} when: Need dynamic tool selection, multi-step reasoning
\end{itemize}

{\tiny (Ref: Superpower LLMs with Conversational Agents)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{LangChain Use Cases}

\begin{itemize}
\item \textbf{Retrieval Augmented Generation (RAG)}:
    \begin{itemize}
    \item Document Q\&A systems
    \item Knowledge base search
    \item Semantic search applications
    \end{itemize}

\item \textbf{Conversational AI}:
    \begin{itemize}
    \item Chatbots with memory
    \item Customer support agents
    \item Personal assistants
    \end{itemize}

\item \textbf{Data Analysis}:
    \begin{itemize}
    \item SQL query generation
    \item Tabular data Q\&A
    \item Report generation
    \end{itemize}

\item \textbf{Autonomous Agents}:
    \begin{itemize}
    \item Web scraping and research
    \item Multi-tool workflows
    \item API integration
    \end{itemize}
\end{itemize}

{\tiny (Ref: LangChain Use Cases Documentation)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{What's Next?}

\textbf{In Following Sections:}
\begin{itemize}
\item \textbf{Framework Deep Dive}: Models, Prompts, Memory, Retrievers
\item \textbf{LCEL Advanced Patterns}: Complex chains, fallbacks, error handling
\item \textbf{Agents \& Tools}: Building intelligent agents
\item \textbf{Vector Stores}: Document indexing and retrieval
\item \textbf{LangChain Ecosystem}: LangGraph, LangServe, LangSmith
\item \textbf{Best Practices}: Production deployment, monitoring, optimization
\item \textbf{Real-world Applications}: Complete project examples
\end{itemize}

\end{frame}