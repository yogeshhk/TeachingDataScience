%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LangChain Framework Components}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Framework Architecture}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{langchain1}
\end{center}

\textbf{Core Building Blocks:}
\begin{itemize}
\item \textbf{Models}: LLMs, Chat Models, Embeddings
\item \textbf{Prompts}: Dynamic template management
\item \textbf{Output Parsers}: Structured output extraction
\item \textbf{Retrievers}: Document and data access
\item \textbf{Memory}: Conversation state persistence
\item \textbf{Agents \& Tools}: Dynamic reasoning and actions
\end{itemize}

{\tiny (Ref: Building the Future with LLMs, LangChain, \& Pinecone)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Models}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Models in LangChain}

\textbf{Three Types of Models:}

\begin{itemize}
\item \textbf{LLMs (Large Language Models)}:
    \begin{itemize}
    \item Input: String (prompt)
    \item Output: String (completion)
    \item Examples: GPT-4, Claude, Gemma, Llama 3, Mixtral.
    \item Use case: Text generation, completion
    \end{itemize}

\item \textbf{Chat Models}:
    \begin{itemize}
    \item Input: List of messages
    \item Output: Chat message
    \item Examples: ChatGPT, Claude Chat, ChatGroq
    \item Use case: Conversational AI
    \end{itemize}

\item \textbf{Embedding Models}:
    \begin{itemize}
    \item Input: Text
    \item Output: Vector (list of floats)
    \item Examples: OpenAI Embeddings, HuggingFace, Sentence Transformers
    \item Use case: Semantic search, similarity
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Model Integration: Modern Syntax}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

# Chat Model with Groq
chat = ChatGroq(model_name="llama3-8b-8192", temperature=0.7)

messages = [
    SystemMessage(content="You are a helpful assistant"),
    HumanMessage(content="Explain quantum computing briefly")
]
response = chat.invoke(messages)

# Embedding Model (from Hugging Face)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vector = embeddings.embed_query("Machine learning is...")

# Using with LCEL
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = prompt | chat | StrOutputParser()
result = chain.invoke({"topic": "blockchain"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Alternative Model Providers}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Groq (Recommended for Fast Inference):
from langchain_groq import ChatGroq

# Gemma 2 - Very fast, efficient
llm = ChatGroq(model_name="gemma2-9b-it", temperature=0.7)
response = llm.invoke("What is the Groq LPU?")

# Llama 3 - Balanced performance
llm = ChatGroq(model_name="llama3-70b-8192", temperature=0.7)

// Hugging Face (Self-hosted):
from langchain_huggingface import HuggingFaceEndpoint
llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    temperature=0.7
)

// Google Gemini (Multimodal):
from langchain_google_genai import ChatGoogleGenerativeAI
llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.7)

// Anthropic Claude (Reasoning):
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-3-5-sonnet-20241022")
\end{lstlisting}


% \begin{lstlisting}[language=python, basicstyle=\tiny]
% //Groq:
% from langchain_groq import ChatGroq
% llm = ChatGroq(model_name="gemma-7b-it", temperature=0.7)
% response = llm.invoke("What is the Groq LPU?")

% //Hugging Face:
% from langchain_huggingface import HuggingFaceEndpoint

% llm = HuggingFaceEndpoint(
    % repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    % temperature=0.7
% )
% response = llm.invoke("What is AI?")

% //Google Gemini:
% from langchain_google_genai import ChatGoogleGenerativeAI

% llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.7)
% response = llm.invoke("Explain neural networks")

% // Anthropic Claude:
% from langchain_anthropic import ChatAnthropic

% llm = ChatAnthropic(model="claude-3-opus-20240229")
% response = llm.invoke("What is machine learning?")
% \end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Prompts}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Prompts in LangChain}

\textbf{Prompt Management Strategies:}

\begin{itemize}
\item \textbf{Prompt Templates}:
    \begin{itemize}
    \item Parameterized templates with variables
    \item Dynamic input insertion
    \item Reusable prompt structures
    \end{itemize}

\item \textbf{Chat Prompt Templates}:
    \begin{itemize}
    \item Multi-message conversations
    \item System, human, AI message roles
    \item Better for chat models
    \end{itemize}

\item \textbf{Few-Shot Prompts}:
    \begin{itemize}
    \item Include example inputs/outputs
    \item Guide model response style
    \item Improve accuracy
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Prompt Templates: Modern Examples}

\textbf{Basic Prompt Template:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.prompts import PromptTemplate

template = """You are a {role} assistant.
Task: {task}
Context: {context}
Provide a {format} response."""

prompt = PromptTemplate(
    template=template,
    input_variables=["role", "task", "context", "format"]
)

formatted = prompt.format(
    role="helpful",
    task="explain quantum computing",
    context="for beginners",
    format="simple"
)

// Chat Prompt Template:
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}"),
    ("human", "{input}"),
    ("ai", "I understand. Let me help with that."),
    ("human", "{follow_up}")
])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Few-Shot Prompting}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Define examples
examples = [
    {"input": "happy", "output": "joyful, cheerful, delighted"},
    {"input": "sad", "output": "unhappy, sorrowful, dejected"},
]

# Example template
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nSynonyms: {output}"
)

# Few-shot template
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Provide synonyms for the following word:",
    suffix="Input: {word}\nSynonyms:",
    input_variables=["word"]
)

# Use in chain with Groq
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser

chain = few_shot_prompt | ChatGroq(model_name="gemma-7b-it") | StrOutputParser()
result = chain.invoke({"word": "angry"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Memory}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Memory in LangChain}

\textbf{Why Memory?}
\begin{itemize}
\item LLMs are stateless by default
\item Chatbots need conversation context
\item Memory stores and retrieves conversation history
\end{itemize}

\textbf{Memory Types (Consolidated):}
\begin{itemize}
\item \textbf{ConversationBufferMemory}:
    \begin{itemize}
    \item Stores entire conversation history
    \item Simple but can exceed token limits
    \item Best for: Short conversations
    \end{itemize}

\item \textbf{ConversationBufferWindowMemory}:
    \begin{itemize}
    \item Keeps only last K interactions
    \item Prevents token overflow
    \item Best for: Longer conversations with recent context
    \end{itemize}

\item \textbf{ConversationSummaryMemory}:
    \begin{itemize}
    \item Summarizes old messages
    \item Reduces token usage
    \item Best for: Very long conversations
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory: Implementation Examples}

\textbf{Buffer Memory (Stores Everything):}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough

memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="history"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = (
    RunnablePassthrough.assign(
        history=lambda x: memory.load_memory_variables({})["history"]
    )
    | prompt
    | ChatGroq(model_name="gemma2-9b-it")
)

# Use the chain
response = chain.invoke({"input": "Hi, I'm Alice"})
memory.save_context({"input": "Hi, I'm Alice"}, {"output": response.content})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory: Window Memory}
\textbf{Benefits:}
\begin{itemize}
\item Fixed memory footprint
\item Maintains recent context
\item Prevents token limit issues
\end{itemize}

\textbf{Buffer Window Memory (Keeps Last K):}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=3, # Only keeps last 3 interactions
    return_messages=True,
    memory_key="history"
)

conversations = [ # Add conversations
    ("Tell me about Python", "Python is a versatile language..."),
    ("What makes it popular?", "Its simplicity and libraries..."),
    ("Give an example", "Like NumPy for computing..."),
    ("What about AI?", "Python excels in AI with TensorFlow...")
]

for input_text, output_text in conversations:
    memory.save_context({"input": input_text}, {"output": output_text})

# Retrieves only last 3 interactions
history = memory.load_memory_variables({})
print(f"Stored messages: {len(history['history'])}")  # Will be 6 (3 pairs)
\end{lstlisting}



\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Token Management Best Practices}

% \textbf{Strategies to Manage Token Limits:}
% \begin{itemize}
% \item \textbf{Monitor token count}: Use tiktoken library
% \item \textbf{Set limits}: Configure max\_tokens in memory
% \item \textbf{Use window memory}: Keep only recent messages
% \item \textbf{Summarize old context}: Use ConversationSummaryMemory
% \item \textbf{Truncate intelligently}: Remove less important messages
% \end{itemize}

% \textbf{Token Counting Example:}

% \begin{lstlisting}[language=python, basicstyle=\tiny]
% # Note: Groq API does not charge by token (currently), but limits apply.
% # For exact counts with Gemma, use HuggingFace tokenizers.

% from transformers import AutoTokenizer

% # Load tokenizer for Gemma
% tokenizer = AutoTokenizer.from_pretrained("google/gemma-7b-it")

% text = "Your conversation history..."

% # Count tokens
% tokens = tokenizer.encode(text)
% token_count = len(tokens)

% print(f"Token count: {token_count}")

% # Gemma context window is typically 8192
% if token_count > 8000:
    % print("Warning: Approaching context limit!")
% \end{lstlisting}



% % \begin{lstlisting}[language=python, basicstyle=\tiny]
% % import tiktoken

% % def count_tokens(text, model="gpt-3.5-turbo"):
    % % encoding = tiktoken.encoding_for_model(model)
    % % return len(encoding.encode(text))

% % # Example
% % text = "Your conversation history..."
% % tokens = count_tokens(text)
% % print(f"Token count: {tokens}")

% % # GPT-3.5-turbo: 4,096 tokens max (16,385 for newer versions)
% % # GPT-4: 8,192 tokens max (32,768/128,000 for GPT-4 Turbo)
% % if tokens > 3000:
    % % print("Warning: Approaching token limit!")
% % \end{lstlisting}

% \end{frame}

% \begin{frame}[fragile]\frametitle{Memory: Modern Approaches with Groq}

% \textbf{Groq-Specific Considerations:}
% \begin{itemize}
% \item Groq offers extremely fast token processing
% \item Context windows: Llama 3 (8K/128K), Gemma 2 (8K)
% \item No token-based pricing, but rate limits apply
% \item Ideal for high-throughput conversational applications
% \end{itemize}

% \begin{lstlisting}[language=python, basicstyle=\tiny]
% from langchain_groq import ChatGroq
% from langchain.memory import ConversationBufferWindowMemory

% # Fast model for conversation
% chat = ChatGroq(model_name="gemma2-9b-it", temperature=0.7)

% # Keep last 5 exchanges (10 messages)
% memory = ConversationBufferWindowMemory(
    % k=5,
    % return_messages=True,
    % memory_key="chat_history"
% )

% # Groq's speed allows for larger context windows without latency issues
% \end{lstlisting}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Document Loaders \& Retrievers}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Document Loading}

\textbf{Wide Range of Loaders:}
\begin{itemize}
\item \textbf{Files}: PDF, Word, PowerPoint, CSV, Markdown
\item \textbf{Web}: HTML, URLs, sitemaps, web scraping
\item \textbf{Cloud}: S3, GCS, Google Drive, Notion
\item \textbf{Databases}: SQL, MongoDB, Elasticsearch
\item \textbf{Communication}: Email, Slack, Discord
\item \textbf{Code}: GitHub, GitLab, Jupyter notebooks
\end{itemize}

\textbf{Modern Document Processing:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_community.document_loaders import (
    PyPDFLoader,
    WebBaseLoader,
    TextLoader)
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = PyPDFLoader("document.pdf")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200)
	
chunks = splitter.split_documents(documents)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Vector Stores \& Retrievers}

\textbf{Popular Vector Stores:}
\begin{itemize}
\item \textbf{Chroma}: Open-source, local-first, easy setup
\item \textbf{Pinecone}: Managed service, production-ready
\item \textbf{Weaviate}: GraphQL API, hybrid search
\item \textbf{Qdrant}: High performance, filtering support
\item \textbf{LanceDB}: Serverless, embedded option
\end{itemize}

\textbf{Complete Example with Chroma \& HuggingFace Embeddings:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
# from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com/article")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = splitter.split_documents(docs)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"))

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrieval Strategies}

\textbf{Different Retrieval Methods:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
# Returns the most semantically similar documents to the user 
# query based purely on vector distance (top-k closest matches).
retriever = vectorstore.as_retriever(search_type="similarity", 
			search_kwargs={"k": 4})

# MMR (Maximum Marginal Relevance)Balances relevance and diversity, 
# ensuring the retrieved documents aren't repetitive by reducing semantic redundancy among the top-k results.
retriever = vectorstore.as_retriever(search_type="mmr", 
			search_kwargs={"k": 4, "fetch_k": 20})

# Filters out documents that fall below a minimum similarity score, 
# ensuring only high-confidence matches are returned (still limited by k)
retriever = vectorstore.as_retriever(search_type = "similarity_score_threshold",
    search_kwargs={"score_threshold": 0.8, "k": 4})

# Manual search: Directly retrieves the top-k most similar documents 
# without using a retriever wrapper same as search_type "similarity" but explicitly called.
results = vectorstore.similarity_search("What is machine learning?", k=3)

# With scores: Same as above but also returns the similarity 
# score for each document, helping you inspect retrieval quality
results_with_scores = vectorstore.similarity_search_with_score(
    "What is machine learning?", k=3)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Chains with LCEL}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Modern LangChain: LCEL (LangChain Expression Language)}

\textbf{What is LCEL?}
\begin{itemize}
\item Modern, declarative way to build chains (introduced 2023)
\item Uses pipe operator \texttt{|} to chain components
\item Replaces old \texttt{LLMChain} pattern
\item Built-in streaming, async, and batch support
\end{itemize}

\textbf{Key Benefits:}
\begin{itemize}
\item \textbf{Simplicity}: More readable and concise
\item \textbf{Streaming}: Built-in streaming by default
\item \textbf{Async}: Native async/await support
\item \textbf{Observability}: Better tracing and debugging
\item \textbf{Fallbacks}: Easy error handling and retries
\end{itemize}

{\tiny (Ref: LangChain LCEL Documentation)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Modern Chains: LCEL Overview}

\textbf{What Changed?}
\begin{itemize}
\item \textbf{Old}: \texttt{LLMChain}, \texttt{SimpleSequentialChain} (Deprecated)
\item \textbf{New}: LCEL with pipe operator \texttt{|}
\end{itemize}


\textbf{Core Runnables:}
\begin{itemize}
\item \texttt{RunnablePassthrough}: Pass data through
\item \texttt{RunnableParallel}: Execute in parallel
\item \texttt{RunnableLambda}: Custom functions
\item \texttt{RunnableBranch}: Conditional execution
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Basic Chain Patterns}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Simple Chain:
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatGroq(model_name="gemma-7b-it")
output_parser = StrOutputParser()

chain = prompt | model | output_parser
result = chain.invoke({"topic": "programming"})

// Chain with Multiple Steps:
# Step 1: Generate topic
topic_chain = (
    ChatPromptTemplate.from_template("Suggest a {genre} topic")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser()
)

# Step 2: Write content
content_chain = (
    ChatPromptTemplate.from_template("Write a story about: {topic}")
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser()
)

# Combine: topic generates input for content
full_chain = {"topic": topic_chain} | content_chain
result = full_chain.invoke({"genre": "science fiction"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: RAG Chain Pattern}

\textbf{Retrieval Augmented Generation:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.runnables import RunnablePassthrough
from langchain_groq import ChatGroq
# Assume 'retriever' from previous slide is defined

# Format documents
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG chain
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | ChatPromptTemplate.from_template("""
        Answer the question based on the context:
        
        Context: {context}
        
        Question: {question}
    """)
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser()
)

# Use it
answer = rag_chain.invoke("What is LangChain?")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Parallel Execution}
\textbf{Benefits:}
\begin{itemize}
\item Faster execution
\item Clean code structure
\item Easy to add/remove tasks
\end{itemize}

\textbf{Run Multiple Chains in Parallel:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.runnables import RunnableParallel
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Define parallel tasks
parallel_chain = RunnableParallel(
    summary=ChatPromptTemplate.from_template("Summarize: {text}")
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser(),
    
    keywords=ChatPromptTemplate.from_template("Extract keywords: {text}")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser(),
    
    sentiment=ChatPromptTemplate.from_template("Analyze sentiment: {text}")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser()
)

# Execute all in parallel
results = parallel_chain.invoke({"text": "Long document content..."})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Error Handling \& Fallbacks}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Fallback to Alternative Model:
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic

primary = ChatGroq(model_name="llama3-70b-8192")
fallback = ChatAnthropic(model="claude-3-opus-20240229")

chain = prompt | primary.with_fallbacks([fallback]) | output_parser

// Retry Logic: If Groq fails, automatically retries
from langchain_core.runnables import RunnableRetry

chain_with_retry = (
    prompt
    | RunnableRetry(max_attempts=3, wait_exponential_jitter=True)
    | ChatGroq(model_name="gemma-7b-it")
    | output_parser
)

// Custom Error Handling:
from langchain_core.runnables import RunnableLambda

def handle_errors(x):
    try:
        return x
    except Exception as e:
        return {"error": str(e)}

chain = prompt | model | RunnableLambda(handle_errors)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Streaming}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Stream Tokens as Generated:
from langchain_groq import ChatGroq
# Assume prompt and output_parser are defined
chain = prompt | ChatGroq(model_name="gemma-7b-it") | StrOutputParser()

# Stream output
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)

// Async Streaming:
import asyncio

async def stream_response():
    async for chunk in chain.astream({"topic": "AI"}):
        print(chunk, end="", flush=True)

asyncio.run(stream_response())

// Streaming with Events:
# Get detailed streaming events
async for event in chain.astream_events({"topic": "AI"}, version="v1"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        print(content, end="", flush=True)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Agents \& Tools}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Modern Agents Overview}

\textbf{What Are Agents?}
\begin{itemize}
\item Use LLMs as reasoning engines
\item Dynamically choose which tools to use
\item Iterative: observe, think, act, repeat
\item Best for complex, multi-step tasks
\end{itemize}

\textbf{Modern Agent Types :}
\begin{itemize}
\item \textbf{Tool Calling Agent}: Uses function calling (recommended)
\item \textbf{JSON Chat Agent}: Output formatting based on Json
\item \textbf{Structured Chat Agent}: For multi-input tools
\item \textbf{ReAct Agent}: Reasoning and acting pattern
\end{itemize}

\textbf{Deprecated (Don't Use):}
\begin{itemize}
\item ~~zero-shot-react-description~~
\item ~~conversational-react-description~~
\item ~~self-ask-with-search~~
\end{itemize}

{\tiny (Ref: LangChain Agents Documentation)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Creating Tools: Modern Approach}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Method 1: Using @tool Decorator:
from langchain_core.tools import tool

@tool
def search_wikipedia(query: str) -> str:
    """Search Wikipedia for information about a topic."""
    from wikipedia import summary
    try:
        return summary(query, sentences=3)
    except:
        return "Could not find information."

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    try:
        return str(eval(expression))
    except:
        return "Invalid expression"

// Method 2: From Function:
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.tools import Tool

def get_weather(location: str) -> str:
    """Get weather for a location."""
    return f"Weather in {location}: Sunny, 72F"

weather_tool = Tool.from_function(
    func=get_weather,
    name="weather",
    description="Get current weather for a location")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Tool Calling Agent Example}

% \textbf{Complete Agent Setup with Groq:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

@tool
def search_wikipedia(query: str) -> str:
    """Search Wikipedia for a topic."""
    # Dummy implementation
    return f"Results for {query} from Wikipedia."

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    return str(eval(expression))

tools = [search_wikipedia, calculate]

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

llm = ChatGroq(model_name="llama3-70b-8192", temperature=0)
agent = create_tool_calling_agent(llm, tools, prompt)

agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

result = agent_executor.invoke({"input": "What is the capital of France?"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Modern Approach with bind\_tools}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

@tool  
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

# Bind tools to model
llm = ChatGroq(model_name="llama3-8b-8192")
llm_with_tools = llm.bind_tools([multiply, add])

# Invoke
response = llm_with_tools.invoke("What is 3 times 4 plus 5?")

# Check if tool was called
if response.tool_calls:
    tool_call = response.tool_calls[0]
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Output Parsers}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Output Parsers Overview}

\textbf{Why Output Parsers?}
\begin{itemize}
\item LLMs return unstructured text
\item Applications need structured data
\item Parsers extract and validate output
\end{itemize}

\textbf{Common Parser Types:}
\begin{itemize}
\item \textbf{StrOutputParser}: Basic string extraction
\item \textbf{JsonOutputParser}: Parse JSON responses
\item \textbf{PydanticOutputParser}: Structured data with validation
\item \textbf{StructuredOutputParser}: Multiple fields
\item \textbf{CommaSeparatedListOutputParser}: Lists
\end{itemize}

\textbf{Modern Best Practice:}
\begin{itemize}
\item Use OpenAI's \texttt{with\_structured\_output()} when possible
\item More reliable than prompt-based parsing
\item Leverages function calling internally
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Structured Output: Modern Approach}

\textbf{Benefits:}
\begin{itemize}
\item Type-safe responses
\item Automatic validation
\item More reliable than prompt-based parsing
\end{itemize}


% \textbf{Using with\_structured\_output():}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field

# Define output schema
class Person(BaseModel):
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="Person's job")

# Create model with structured output (relies on tool-calling)
llm = ChatGroq(model_name="llama3-70b-8192")
structured_llm = llm.with_structured_output(Person)

# Use in chain
response = structured_llm.invoke(
    "Tell me about a software engineer named Alice who is 28"
)

# Response is a Pydantic object
print(response.name)
print(response.age)
print(response.occupation)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Pydantic Output Parser (Alternative)}

% \textbf{When with\_structured\_output() Not Available:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.output_parsers import PydanticOutputParser
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field, validator
from typing import List

class MovieReview(BaseModel):
    title: str = Field(description="Movie title")
    rating: int = Field(description="Rating from 1-10")
   
parser = PydanticOutputParser(pydantic_object=MovieReview)

# Add to prompt
prompt = ChatPromptTemplate.from_template("""
Review the movie: {movie}

{format_instructions}""")

chain = (prompt.partial(format_instructions=parser.get_format_instructions())
    | ChatGroq(model_name="gemma-7b-it") | parser)

result = chain.invoke({"movie": "Inception"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Output Parser Error Handling}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
# Assume 'parser' is a defined PydanticOutputParser

// OutputFixingParser (Auto-fix Errors):
from langchain.output_parsers import OutputFixingParser

# If parsing fails, use LLM to fix
fixing_parser = OutputFixingParser.from_llm(
    parser=parser,
    llm=ChatGroq(model_name="gemma-7b-it")
)

// Automatically fixes malformed output
# result = fixing_parser.parse(malformed_output)

// RetryOutputParser (Retry with Context):
from langchain.output_parsers import RetryWithErrorOutputParser

retry_parser = RetryWithErrorOutputParser.from_llm(
    parser=parser,
    llm=ChatGroq(model_name="llama3-8b-8192")
)

# Retries with both output and original prompt
# result = retry_parser.parse_with_prompt(...)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LangChain Ecosystem}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{LangChain Ecosystem Components}

\begin{itemize}
\item \textbf{LangChain Core}:
    \begin{itemize}
    \item Base abstractions and LCEL
    \item Foundation for all other packages
    \end{itemize}

\item \textbf{LangChain Community}:
    \begin{itemize}
    \item Third-party integrations
    \item Vector stores, document loaders
    \item Community-maintained tools
    \end{itemize}

\item \textbf{LangGraph}:
    \begin{itemize}
    \item Build stateful, multi-actor applications
    \item Complex agent workflows with cycles
    \item State management for agents
    \end{itemize}

\item \textbf{LangServe}:
    \begin{itemize}
    \item Deploy chains as REST APIs
    \item Automatic FastAPI generation
    \item Production deployment
    \end{itemize}

\item \textbf{LangSmith}:
    \begin{itemize}
    \item Debugging and monitoring
    \item Tracing and evaluation
    \item Dataset management
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangGraph: Stateful Agents}

% \textbf{What is LangGraph?}
\begin{itemize}
\item Build complex, stateful agent workflows
\item Support for cycles and conditional logic
\item Persist state across interactions
\item Multiple agents working together
\end{itemize}

% \textbf{Basic Example:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langgraph.graph import StateGraph
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    next: str

def call_model(state):
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

def should_continue(state):
    if len(state["messages"]) > 5:
        return "end"
    return "continue"

workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_conditional_edges("agent", should_continue)
workflow.set_entry_point("agent")

app = workflow.compile()
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangServe: Deploy as API}
\textbf{Automatic Features:}
\begin{itemize}
\item Interactive playground at \texttt{/chat/playground}
\item OpenAPI docs at \texttt{/docs}
\item Streaming support
\item Batch processing
\end{itemize}

\textbf{Deploy Any Chain as REST API:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from fastapi import FastAPI
from langserve import add_routes
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

# Create chain
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
chain = prompt | ChatGroq(model_name="gemma-7b-it")

# Create FastAPI app
app = FastAPI(
    title="LangChain API", version="1.0",
    description="API for my LangChain application")

# Add chain as route
add_routes(app, chain, path="/chat")

# Run: uvicorn app:app --reload
# API available at: http://localhost:8000/chat
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangSmith: Monitoring \& Debugging}

\textbf{What is LangSmith?}
\begin{itemize}
\item Platform for debugging LLM applications
\item Trace every step of chain execution
\item Evaluate model performance
\item Manage test datasets
\item Monitor production applications
\end{itemize}

\textbf{Setup:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
import os
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-groq-project"

# Now all chain executions are automatically traced
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatGroq(model_name="gemma-7b-it")
output_parser = StrOutputParser()
chain = prompt | llm | output_parser
result = chain.invoke({"topic": "Large Language Models"})

# View traces at: https://smith.langchain.com
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Best Practices}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Error Handling}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// 1. Use Fallbacks:
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic

primary = ChatGroq(model_name="llama3-70b-8192")
fallback = ChatAnthropic(model="claude-3-opus-20240229")

chain = prompt | primary.with_fallbacks([fallback]) | parser

// 2. Implement Retries:
from langchain_core.runnables import RunnableRetry

chain_with_retry = (
    prompt
    | RunnableRetry(
        max_attempts=3,
        wait_exponential_jitter=True
    )
    | ChatGroq(model_name="gemma-7b-it")
    | parser
)

// 3. Graceful Degradation:
try:
    result = chain.invoke(input_data)
except Exception as e:
    # logger.error(f"Chain failed: {e}")
    # result = fallback_response()
    pass
\end{lstlisting}

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Token Management with Groq}

\textbf{Groq Models - Fast and Free Tier:}
\begin{itemize}
\item gemma2-9b-it: Ultra-fast, 8K context, great for chat
\item llama3-8b-8192: Balanced, 8K context, general purpose
\item llama3-70b-8192: Most capable, 8K context, complex tasks
\item llama-3.1-8b-instant: Fastest, optimized for low latency
\item Groq billed by requests/day on free tier, not tokens
\end{itemize}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq

# Choose model based on needs
llm_fast = ChatGroq(
    model_name="gemma2-9b-it",
    max_tokens=500,
    temperature=0.7,
    max_retries=2
)

# Monitor context length manually
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it")
token_count = len(tokenizer.encode(text))

# Groq's speed allows batch processing without latency concerns
\end{lstlisting}

\end{frame}

% \begin{frame}[fragile]\frametitle{Best Practices: Token Management}
% Use Appropriate Models:
% \begin{itemize}
% \item gemma2-9b-it: Very fast, general purpose
% \item llama3-8b-8192: Good balance of speed and capability
% \item llama3-70b-8192: High quality, more powerful
% \item Groq is billed by time, not tokens, but token limits still apply.
% \end{itemize}

% \begin{lstlisting}[language=python, basicstyle=\tiny]
% from langchain_groq import ChatGroq

% // 1. Monitor Token Usage (Manually):
% # Groq does not have a direct cost callback like OpenAI.
% # Monitor usage in your Groq Console dashboard.
% # You can use tiktoken to estimate input tokens before sending.

% // 2. Set Token Limits:
% llm = ChatGroq(
    % model_name="gemma2-9b-it",
    % max_tokens=500,  # Limit output tokens
    % temperature=0.7
% )

% // 3. Implement Caching:
% from langchain.cache import InMemoryCache
% from langchain.globals import set_llm_cache

% set_llm_cache(InMemoryCache())  # Cache identical requests
% \end{lstlisting}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Security \& Privacy}

% \textbf{Data Privacy:}
\begin{itemize}
\item Be aware of the data usage policies of your LLM provider.
\item Groq has a zero-retention policy for API data.
\item Consider self-hosted models for maximum data control.
\item Implement PII detection and redaction before sending data.
\end{itemize}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// 1. API Key Management:
import os
from dotenv import load_dotenv

load_dotenv()  # Load from .env file
api_key = os.getenv("GROQ_API_KEY")

// 2. Timeouts and Retries:
from langchain_groq import ChatGroq

llm = ChatGroq(model_name="llama3-8b-8192",
    temperature=0,
    max_retries=2,
    request_timeout=30) # seconds
	
// 3. Input Validation:
def validate_input(user_input: str) -> str:
    if len(user_input) > 8000:     # Sanitize input
        raise ValueError("Input too long for the model context.")
    # Remove potential injection attempts
    return user_input.strip()
\end{lstlisting}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Choosing the Right Model}

\textbf{Groq Model Selection Guide:}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Use Case} & \textbf{Model} & \textbf{Why} \\
\hline
Chatbots & gemma2-9b-it & Fast, efficient, good reasoning \\
\hline
Summarization & llama3-8b-8192 & Balanced speed/quality \\
\hline
Complex reasoning & llama3-70b-8192 & Most capable \\
\hline
Ultra-low latency & llama-3.1-8b-instant & Optimized for speed \\
\hline
Code generation & llama3-70b-8192 & Better instruction following \\
\hline
\end{tabular}
\end{center}

\begin{lstlisting}[language=python, basicstyle=\tiny]
# Pattern: Start with fast model, fallback to capable model
from langchain_groq import ChatGroq

primary = ChatGroq(model_name="gemma2-9b-it", temperature=0.7)
fallback = ChatGroq(model_name="llama3-70b-8192", temperature=0.7)

chain = prompt | primary.with_fallbacks([fallback]) | parser
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Async Patterns}

\textbf{Benefits:}
\begin{itemize}
\item Faster parallel processing
\item Better resource utilization
\item Improved user experience with streaming
\end{itemize}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Use Async for Better Performance:
import asyncio

async def process_multiple_queries(queries):
    # Process queries concurrently
    tasks = [chain.ainvoke({"input": q}) for q in queries]
    results = await asyncio.gather(*tasks)
    return results

# Run
queries = ["Query 1", "Query 2", "Query 3"]
results = asyncio.run(process_multiple_queries(queries))
// Async Streaming:
async def stream_response(input_text):
    async for chunk in chain.astream({"input": input_text}):
        print(chunk, end="", flush=True)

asyncio.run(stream_response("Tell me about AI"))
\end{lstlisting}



\end{frame}
