%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LangChain Framework Components}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Framework Architecture}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{langchain1}
\end{center}

\textbf{Core Building Blocks:}
\begin{itemize}
\item \textbf{Models}: LLMs, Chat Models, Embeddings
\item \textbf{Prompts}: Dynamic template management
\item \textbf{Output Parsers}: Structured output extraction
\item \textbf{Retrievers}: Document and data access
\item \textbf{Memory}: Conversation state persistence
\item \textbf{Agents \& Tools}: Dynamic reasoning and actions
\end{itemize}

{\tiny (Ref: Building the Future with LLMs, LangChain, \& Pinecone)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Models}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Models in LangChain}

\textbf{Three Types of Models:}

\begin{itemize}
\item \textbf{LLMs (Large Language Models)}:
    \begin{itemize}
    \item Input: String (prompt)
    \item Output: String (completion)
    \item Examples: GPT-4, Claude, Llama
    \item Use case: Text generation, completion
    \end{itemize}

\item \textbf{Chat Models}:
    \begin{itemize}
    \item Input: List of messages
    \item Output: Chat message
    \item Examples: ChatGPT, Claude Chat
    \item Use case: Conversational AI
    \end{itemize}

\item \textbf{Embedding Models}:
    \begin{itemize}
    \item Input: Text
    \item Output: Vector (list of floats)
    \item Examples: OpenAI Embeddings, Sentence Transformers
    \item Use case: Semantic search, similarity
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Model Integration: Modern Syntax}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

# Chat Model with Groq
chat = ChatGroq(model_name="llama3-8b-8192", temperature=0.7)

messages = [
    SystemMessage(content="You are a helpful assistant"),
    HumanMessage(content="Explain quantum computing briefly")
]
response = chat.invoke(messages)

# Embedding Model (from Hugging Face)
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vector = embeddings.embed_query("Machine learning is...")

# Using with LCEL
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_template("Explain {topic}")
chain = prompt | chat | StrOutputParser()
result = chain.invoke({"topic": "blockchain"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Alternative Model Providers}


\begin{lstlisting}[language=python, basicstyle=\tiny]
//Groq:
from langchain_groq import ChatGroq
llm = ChatGroq(model_name="gemma-7b-it", temperature=0.7)
response = llm.invoke("What is the Groq LPU?")

//Hugging Face:
from langchain_huggingface import HuggingFaceEndpoint

llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    temperature=0.7
)
response = llm.invoke("What is AI?")

//Google Gemini:
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.7)
response = llm.invoke("Explain neural networks")

// Anthropic Claude:
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(model="claude-3-opus-20240229")
response = llm.invoke("What is machine learning?")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Prompts}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Prompts in LangChain}

\textbf{Prompt Management Strategies:}

\begin{itemize}
\item \textbf{Prompt Templates}:
    \begin{itemize}
    \item Parameterized templates with variables
    \item Dynamic input insertion
    \item Reusable prompt structures
    \end{itemize}

\item \textbf{Chat Prompt Templates}:
    \begin{itemize}
    \item Multi-message conversations
    \item System, human, AI message roles
    \item Better for chat models
    \end{itemize}

\item \textbf{Few-Shot Prompts}:
    \begin{itemize}
    \item Include example inputs/outputs
    \item Guide model response style
    \item Improve accuracy
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Prompt Templates: Modern Examples}

\textbf{Basic Prompt Template:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.prompts import PromptTemplate

template = """You are a {role} assistant.
Task: {task}
Context: {context}
Provide a {format} response."""

prompt = PromptTemplate(
    template=template,
    input_variables=["role", "task", "context", "format"]
)

formatted = prompt.format(
    role="helpful",
    task="explain quantum computing",
    context="for beginners",
    format="simple"
)

// Chat Prompt Template:
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}"),
    ("human", "{input}"),
    ("ai", "I understand. Let me help with that."),
    ("human", "{follow_up}")
])
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Few-Shot Prompting}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Define examples
examples = [
    {"input": "happy", "output": "joyful, cheerful, delighted"},
    {"input": "sad", "output": "unhappy, sorrowful, dejected"},
]

# Example template
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nSynonyms: {output}"
)

# Few-shot template
few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix="Provide synonyms for the following word:",
    suffix="Input: {word}\nSynonyms:",
    input_variables=["word"]
)

# Use in chain with Groq
from langchain_groq import ChatGroq
from langchain_core.output_parsers import StrOutputParser

chain = few_shot_prompt | ChatGroq(model_name="gemma-7b-it") | StrOutputParser()
result = chain.invoke({"word": "angry"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Memory}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Memory in LangChain}

\textbf{Why Memory?}
\begin{itemize}
\item LLMs are stateless by default
\item Chatbots need conversation context
\item Memory stores and retrieves conversation history
\end{itemize}

\textbf{Memory Types (Consolidated):}
\begin{itemize}
\item \textbf{ConversationBufferMemory}:
    \begin{itemize}
    \item Stores entire conversation history
    \item Simple but can exceed token limits
    \item Best for: Short conversations
    \end{itemize}

\item \textbf{ConversationBufferWindowMemory}:
    \begin{itemize}
    \item Keeps only last K interactions
    \item Prevents token overflow
    \item Best for: Longer conversations with recent context
    \end{itemize}

\item \textbf{ConversationSummaryMemory}:
    \begin{itemize}
    \item Summarizes old messages
    \item Reduces token usage
    \item Best for: Very long conversations
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory: Implementation Examples}

\textbf{Buffer Memory (Stores Everything):}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough

memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="history"
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = (
    RunnablePassthrough.assign(
        history=lambda x: memory.load_memory_variables({})["history"]
    )
    | prompt
    | ChatGroq(model_name="llama3-8b-8192")
)

# Use the chain
response = chain.invoke({"input": "Hi, I'm Alice"})
memory.save_context({"input": "Hi, I'm Alice"}, {"output": response.content})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory: Window Memory}
\textbf{Benefits:}
\begin{itemize}
\item Fixed memory footprint
\item Maintains recent context
\item Prevents token limit issues
\end{itemize}

\textbf{Buffer Window Memory (Keeps Last K):}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    k=3, # Only keeps last 3 interactions
    return_messages=True,
    memory_key="history"
)

conversations = [ # Add conversations
    ("Tell me about Python", "Python is a versatile language..."),
    ("What makes it popular?", "Its simplicity and libraries..."),
    ("Give an example", "Like NumPy for computing..."),
    ("What about AI?", "Python excels in AI with TensorFlow...")
]

for input_text, output_text in conversations:
    memory.save_context({"input": input_text}, {"output": output_text})

# Retrieves only last 3 interactions
history = memory.load_memory_variables({})
print(f"Stored messages: {len(history['history'])}")  # Will be 6 (3 pairs)
\end{lstlisting}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Token Management Best Practices}

\textbf{Strategies to Manage Token Limits:}
\begin{itemize}
\item \textbf{Monitor token count}: Use tiktoken library
\item \textbf{Set limits}: Configure max\_tokens in memory
\item \textbf{Use window memory}: Keep only recent messages
\item \textbf{Summarize old context}: Use ConversationSummaryMemory
\item \textbf{Truncate intelligently}: Remove less important messages
\end{itemize}

\textbf{Token Counting Example:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
import tiktoken

def count_tokens(text, model="gpt-3.5-turbo"):
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

# Example
text = "Your conversation history..."
tokens = count_tokens(text)
print(f"Token count: {tokens}")

# GPT-3.5-turbo: 4,096 tokens max (16,385 for newer versions)
# GPT-4: 8,192 tokens max (32,768/128,000 for GPT-4 Turbo)
if tokens > 3000:
    print("Warning: Approaching token limit!")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Document Loaders \& Retrievers}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Document Loading}

\textbf{Wide Range of Loaders:}
\begin{itemize}
\item \textbf{Files}: PDF, Word, PowerPoint, CSV, Markdown
\item \textbf{Web}: HTML, URLs, sitemaps, web scraping
\item \textbf{Cloud}: S3, GCS, Google Drive, Notion
\item \textbf{Databases}: SQL, MongoDB, Elasticsearch
\item \textbf{Communication}: Email, Slack, Discord
\item \textbf{Code}: GitHub, GitLab, Jupyter notebooks
\end{itemize}

\textbf{Modern Document Processing:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_community.document_loaders import (
    PyPDFLoader,
    WebBaseLoader,
    TextLoader)
from langchain_text_splitters import RecursiveCharacterTextSplitter

loader = PyPDFLoader("document.pdf")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200)
	
chunks = splitter.split_documents(documents)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Vector Stores \& Retrievers}

\textbf{Popular Vector Stores (2024):}
\begin{itemize}
\item \textbf{Chroma}: Open-source, local-first, easy setup
\item \textbf{Pinecone}: Managed service, production-ready
\item \textbf{Weaviate}: GraphQL API, hybrid search
\item \textbf{Qdrant}: High performance, filtering support
\item \textbf{LanceDB}: Serverless, embedded option
\end{itemize}

\textbf{Complete Example with Chroma & HuggingFace Embeddings:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader

loader = WebBaseLoader("https://example.com/article")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = splitter.split_documents(docs)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"))

retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrieval Strategies}

\textbf{Different Retrieval Methods:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
# Similarity search (most common)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

# MMR (Maximum Marginal Relevance) - diverse results
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 4, "fetch_k": 20}
)

# Similarity with score threshold
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.8, "k": 4}
)

# Manual search
results = vectorstore.similarity_search(
    "What is machine learning?",
    k=3
)

# With scores
results_with_scores = vectorstore.similarity_search_with_score(
    "What is machine learning?",
    k=3
)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Chains with LCEL}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{Modern Chains: LCEL Overview}

\textbf{What Changed?}
\begin{itemize}
\item \textbf{Old}: \texttt{LLMChain}, \texttt{SimpleSequentialChain} (Deprecated)
\item \textbf{New}: LCEL with pipe operator \texttt{|}
\end{itemize}

\textbf{LCEL Advantages:}
\begin{itemize}
\item \textbf{Composability}: Chain components naturally
\item \textbf{Streaming}: Built-in streaming support
\item \textbf{Async}: Native async/await
\item \textbf{Batch}: Process multiple inputs
\item \textbf{Fallbacks}: Error handling built-in
\item \textbf{Parallelization}: Run steps in parallel
\item \textbf{Observability}: Better tracing
\end{itemize}

\textbf{Core Runnables:}
\begin{itemize}
\item \texttt{RunnablePassthrough}: Pass data through
\item \texttt{RunnableParallel}: Execute in parallel
\item \texttt{RunnableLambda}: Custom functions
\item \texttt{RunnableBranch}: Conditional execution
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Basic Chain Patterns}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Simple Chain:
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("Tell me a joke about {topic}")
model = ChatGroq(model_name="gemma-7b-it")
output_parser = StrOutputParser()

chain = prompt | model | output_parser
result = chain.invoke({"topic": "programming"})

// Chain with Multiple Steps:
# Step 1: Generate topic
topic_chain = (
    ChatPromptTemplate.from_template("Suggest a {genre} topic")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser()
)

# Step 2: Write content
content_chain = (
    ChatPromptTemplate.from_template("Write a story about: {topic}")
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser()
)

# Combine: topic generates input for content
full_chain = {"topic": topic_chain} | content_chain
result = full_chain.invoke({"genre": "science fiction"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: RAG Chain Pattern}

\textbf{Retrieval Augmented Generation:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.runnables import RunnablePassthrough
from langchain_groq import ChatGroq
# Assume 'retriever' from previous slide is defined

# Format documents
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG chain
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | ChatPromptTemplate.from_template("""
        Answer the question based on the context:
        
        Context: {context}
        
        Question: {question}
    """)
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser()
)

# Use it
answer = rag_chain.invoke("What is LangChain?")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Parallel Execution}
\textbf{Benefits:}
\begin{itemize}
\item Faster execution
\item Clean code structure
\item Easy to add/remove tasks
\end{itemize}

\textbf{Run Multiple Chains in Parallel:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.runnables import RunnableParallel
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Define parallel tasks
parallel_chain = RunnableParallel(
    summary=ChatPromptTemplate.from_template("Summarize: {text}")
    | ChatGroq(model_name="llama3-8b-8192")
    | StrOutputParser(),
    
    keywords=ChatPromptTemplate.from_template("Extract keywords: {text}")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser(),
    
    sentiment=ChatPromptTemplate.from_template("Analyze sentiment: {text}")
    | ChatGroq(model_name="gemma-7b-it")
    | StrOutputParser()
)

# Execute all in parallel
results = parallel_chain.invoke({"text": "Long document content..."})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Error Handling \& Fallbacks}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Fallback to Alternative Model:
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic

primary = ChatGroq(model_name="llama3-70b-8192")
fallback = ChatAnthropic(model="claude-3-opus-20240229")

chain = prompt | primary.with_fallbacks([fallback]) | output_parser

// Retry Logic: If Groq fails, automatically retries
from langchain_core.runnables import RunnableRetry

chain_with_retry = (
    prompt
    | RunnableRetry(max_attempts=3, wait_exponential_jitter=True)
    | ChatGroq(model_name="gemma-7b-it")
    | output_parser
)

// Custom Error Handling:
from langchain_core.runnables import RunnableLambda

def handle_errors(x):
    try:
        return x
    except Exception as e:
        return {"error": str(e)}

chain = prompt | model | RunnableLambda(handle_errors)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LCEL: Streaming}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Stream Tokens as Generated:
from langchain_groq import ChatGroq
# Assume prompt and output_parser are defined
chain = prompt | ChatGroq(model_name="gemma-7b-it") | StrOutputParser()

# Stream output
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)

// Async Streaming:
import asyncio

async def stream_response():
    async for chunk in chain.astream({"topic": "AI"}):
        print(chunk, end="", flush=True)

asyncio.run(stream_response())

// Streaming with Events:
# Get detailed streaming events
async for event in chain.astream_events({"topic": "AI"}, version="v1"):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        content = event["data"]["chunk"].content
        print(content, end="", flush=True)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Agents \& Tools}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Modern Agents Overview}

\textbf{What Are Agents?}
\begin{itemize}
\item Use LLMs as reasoning engines
\item Dynamically choose which tools to use
\item Iterative: observe, think, act, repeat
\item Best for complex, multi-step tasks
\end{itemize}

\textbf{Modern Agent Types (2024):}
\begin{itemize}
\item \textbf{OpenAI Functions Agent}: Uses function calling (recommended)
\item \textbf{Tool Calling Agent}: Generic tool calling interface
\item \textbf{Structured Chat Agent}: For multi-input tools
\item \textbf{ReAct Agent}: Reasoning and acting pattern
\end{itemize}

\textbf{Deprecated (Don't Use):}
\begin{itemize}
\item ~~zero-shot-react-description~~
\item ~~conversational-react-description~~
\item ~~self-ask-with-search~~
\end{itemize}

{\tiny (Ref: LangChain Agents Documentation 2024)}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Creating Tools: Modern Approach}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// Method 1: Using @tool Decorator:
from langchain_core.tools import tool

@tool
def search_wikipedia(query: str) -> str:
    """Search Wikipedia for information about a topic."""
    from wikipedia import summary
    try:
        return summary(query, sentences=3)
    except:
        return "Could not find information."

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    try:
        return str(eval(expression))
    except:
        return "Invalid expression"

// Method 2: From Function:
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_core.tools import Tool

def get_weather(location: str) -> str:
    """Get weather for a location."""
    return f"Weather in {location}: Sunny, 72F"

weather_tool = Tool.from_function(
    func=get_weather,
    name="weather",
    description="Get current weather for a location")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Tool Calling Agent Example}

\textbf{Complete Agent Setup with Groq:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

# Define tools
@tool
def search_wikipedia(query: str) -> str:
    """Search Wikipedia for a topic."""
    # Dummy implementation
    return f"Results for {query} from Wikipedia."

@tool
def calculate(expression: str) -> str:
    """Evaluate a mathematical expression."""
    return str(eval(expression))

tools = [search_wikipedia, calculate]

# Create prompt
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant"),
    ("human", "{input}"),
    MessagesPlaceholder(variable_name="agent_scratchpad")
])

# Create agent
llm = ChatGroq(model_name="llama3-70b-8192", temperature=0)
agent = create_tool_calling_agent(llm, tools, prompt)

# Create executor
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# Run agent
result = agent_executor.invoke({"input": "What is the capital of France?"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Tool Calling with LCEL}

\textbf{Modern Approach with bind\_tools:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """Multiply two numbers."""
    return a * b

@tool  
def add(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

# Bind tools to model
llm = ChatGroq(model_name="llama3-8b-8192")
llm_with_tools = llm.bind_tools([multiply, add])

# Invoke
response = llm_with_tools.invoke("What is 3 times 4 plus 5?")

# Check if tool was called
if response.tool_calls:
    tool_call = response.tool_calls[0]
    print(f"Tool: {tool_call['name']}")
    print(f"Args: {tool_call['args']}")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Output Parsers}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Output Parsers Overview}

\textbf{Why Output Parsers?}
\begin{itemize}
\item LLMs return unstructured text
\item Applications need structured data
\item Parsers extract and validate output
\end{itemize}

\textbf{Common Parser Types:}
\begin{itemize}
\item \textbf{StrOutputParser}: Basic string extraction
\item \textbf{JsonOutputParser}: Parse JSON responses
\item \textbf{PydanticOutputParser}: Structured data with validation
\item \textbf{StructuredOutputParser}: Multiple fields
\item \textbf{CommaSeparatedListOutputParser}: Lists
\end{itemize}

\textbf{Modern Best Practice:}
\begin{itemize}
\item Use OpenAI's \texttt{with\_structured\_output()} when possible
\item More reliable than prompt-based parsing
\item Leverages function calling internally
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Structured Output: Modern Approach}

\textbf{Benefits:}
\begin{itemize}
\item Type-safe responses
\item Automatic validation
\item More reliable than prompt-based parsing
\end{itemize}


\textbf{Using with\_structured\_output():}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field

# Define output schema
class Person(BaseModel):
    name: str = Field(description="Person's name")
    age: int = Field(description="Person's age")
    occupation: str = Field(description="Person's job")

# Create model with structured output (relies on tool-calling)
llm = ChatGroq(model_name="llama3-70b-8192")
structured_llm = llm.with_structured_output(Person)

# Use in chain
response = structured_llm.invoke(
    "Tell me about a software engineer named Alice who is 28"
)

# Response is a Pydantic object
print(response.name)
print(response.age)
print(response.occupation)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Pydantic Output Parser (Alternative)}

\textbf{When with\_structured\_output() Not Available:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain.output_parsers import PydanticOutputParser
from langchain_groq import ChatGroq
from pydantic import BaseModel, Field, validator
from typing import List

class MovieReview(BaseModel):
    title: str = Field(description="Movie title")
    rating: int = Field(description="Rating from 1-10")
   
parser = PydanticOutputParser(pydantic_object=MovieReview)

# Add to prompt
prompt = ChatPromptTemplate.from_template("""
Review the movie: {movie}

{format_instructions}""")

chain = (prompt.partial(format_instructions=parser.get_format_instructions())
    | ChatGroq(model_name="gemma-7b-it") | parser)

result = chain.invoke({"movie": "Inception"})
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Output Parser Error Handling}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq
# Assume 'parser' is a defined PydanticOutputParser

// OutputFixingParser (Auto-fix Errors):
from langchain.output_parsers import OutputFixingParser

# If parsing fails, use LLM to fix
fixing_parser = OutputFixingParser.from_llm(
    parser=parser,
    llm=ChatGroq(model_name="gemma-7b-it")
)

// Automatically fixes malformed output
# result = fixing_parser.parse(malformed_output)

// RetryOutputParser (Retry with Context):
from langchain.output_parsers import RetryWithErrorOutputParser

retry_parser = RetryWithErrorOutputParser.from_llm(
    parser=parser,
    llm=ChatGroq(model_name="llama3-8b-8192")
)

# Retries with both output and original prompt
# result = retry_parser.parse_with_prompt(...)
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LangChain Ecosystem}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}\frametitle{LangChain Ecosystem Components}

\begin{itemize}
\item \textbf{LangChain Core}:
    \begin{itemize}
    \item Base abstractions and LCEL
    \item Foundation for all other packages
    \end{itemize}

\item \textbf{LangChain Community}:
    \begin{itemize}
    \item Third-party integrations
    \item Vector stores, document loaders
    \item Community-maintained tools
    \end{itemize}

\item \textbf{LangGraph}:
    \begin{itemize}
    \item Build stateful, multi-actor applications
    \item Complex agent workflows with cycles
    \item State management for agents
    \end{itemize}

\item \textbf{LangServe}:
    \begin{itemize}
    \item Deploy chains as REST APIs
    \item Automatic FastAPI generation
    \item Production deployment
    \end{itemize}

\item \textbf{LangSmith}:
    \begin{itemize}
    \item Debugging and monitoring
    \item Tracing and evaluation
    \item Dataset management
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangGraph: Stateful Agents}

\textbf{What is LangGraph?}
\begin{itemize}
\item Build complex, stateful agent workflows
\item Support for cycles and conditional logic
\item Persist state across interactions
\item Multiple agents working together
\end{itemize}

\textbf{Basic Example:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from langgraph.graph import StateGraph
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    next: str

def call_model(state):
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

def should_continue(state):
    if len(state["messages"]) > 5:
        return "end"
    return "continue"

workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_conditional_edges("agent", should_continue)
workflow.set_entry_point("agent")

app = workflow.compile()
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangServe: Deploy as API}
\textbf{Automatic Features:}
\begin{itemize}
\item Interactive playground at \texttt{/chat/playground}
\item OpenAPI docs at \texttt{/docs}
\item Streaming support
\item Batch processing
\end{itemize}

\textbf{Deploy Any Chain as REST API:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
from fastapi import FastAPI
from langserve import add_routes
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

# Create chain
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
chain = prompt | ChatGroq(model_name="gemma-7b-it")

# Create FastAPI app
app = FastAPI(
    title="LangChain API", version="1.0",
    description="API for my LangChain application")

# Add chain as route
add_routes(app, chain, path="/chat")

# Run: uvicorn app:app --reload
# API available at: http://localhost:8000/chat
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangSmith: Monitoring \& Debugging}

\textbf{What is LangSmith?}
\begin{itemize}
\item Platform for debugging LLM applications
\item Trace every step of chain execution
\item Evaluate model performance
\item Manage test datasets
\item Monitor production applications
\end{itemize}

\textbf{Setup:}
\begin{lstlisting}[language=python, basicstyle=\tiny]
import os
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"
os.environ["LANGCHAIN_PROJECT"] = "my-groq-project"

# Now all chain executions are automatically traced
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
llm = ChatGroq(model_name="gemma-7b-it")
output_parser = StrOutputParser()
chain = prompt | llm | output_parser
result = chain.invoke({"topic": "Large Language Models"})

# View traces at: https://smith.langchain.com
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Best Practices}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Error Handling}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// 1. Use Fallbacks:
from langchain_groq import ChatGroq
from langchain_anthropic import ChatAnthropic

primary = ChatGroq(model_name="llama3-70b-8192")
fallback = ChatAnthropic(model="claude-3-opus-20240229")

chain = prompt | primary.with_fallbacks([fallback]) | parser

// 2. Implement Retries:
from langchain_core.runnables import RunnableRetry

chain_with_retry = (
    prompt
    | RunnableRetry(
        max_attempts=3,
        wait_exponential_jitter=True
    )
    | ChatGroq(model_name="gemma-7b-it")
    | parser
)

// 3. Graceful Degradation:
try:
    result = chain.invoke(input_data)
except Exception as e:
    # logger.error(f"Chain failed: {e}")
    # result = fallback_response()
    pass
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Token Management}
Use Appropriate Models:
\begin{itemize}
\item gemma-7b-it: Very fast, general purpose
\item llama3-8b-8192: Good balance of speed and capability
\item llama3-70b-8192: High quality, more powerful
\item Groq is billed by time, not tokens, but token limits still apply.
\end{itemize}

\begin{lstlisting}[language=python, basicstyle=\tiny]
from langchain_groq import ChatGroq

// 1. Monitor Token Usage (Manually):
# Groq does not have a direct cost callback like OpenAI.
# Monitor usage in your Groq Console dashboard.
# You can use tiktoken to estimate input tokens before sending.

// 2. Set Token Limits:
llm = ChatGroq(
    model_name="gemma-7b-it",
    max_tokens=500,  # Limit output tokens
    temperature=0.7
)

// 3. Implement Caching:
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())  # Cache identical requests
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Security \& Privacy}

\textbf{Data Privacy:}
\begin{itemize}
\item Be aware of the data usage policies of your LLM provider.
\item Groq has a zero-retention policy for API data.
\item Consider self-hosted models for maximum data control.
\item Implement PII detection and redaction before sending data.
\end{itemize}

\begin{lstlisting}[language=python, basicstyle=\tiny]
// 1. API Key Management:
import os
from dotenv import load_dotenv

load_dotenv()  # Load from .env file
api_key = os.getenv("GROQ_API_KEY")

// 2. Timeouts and Retries:
from langchain_groq import ChatGroq

llm = ChatGroq(model_name="llama3-8b-8192",
    temperature=0,
    max_retries=2,
    request_timeout=30, # seconds
)
	
// 3. Input Validation:
def validate_input(user_input: str) -> str:
    if len(user_input) > 8000:     # Sanitize input
        raise ValueError("Input too long for the model context.")
    # Remove potential injection attempts
    return user_input.strip()
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Best Practices: Async Patterns}

\textbf{Benefits:}
\begin{itemize}
\item Faster parallel processing
\item Better resource utilization
\item Improved user experience with streaming
\end{itemize}


\begin{lstlisting}[language=python, basicstyle=\tiny]
// Use Async for Better Performance:
import asyncio

async def process_multiple_queries(queries):
    # Process queries concurrently
    tasks = [chain.ainvoke({"input": q}) for q in queries]
    results = await asyncio.gather(*tasks)
    return results

# Run
queries = ["Query 1", "Query 2", "Query 3"]
results = asyncio.run(process_multiple_queries(queries))
// Async Streaming:
async def stream_response(input_text):
    async for chunk in chain.astream({"input": input_text}):
        print(chunk, end="", flush=True)

asyncio.run(stream_response("Tell me about AI"))
\end{lstlisting}



\end{frame}
