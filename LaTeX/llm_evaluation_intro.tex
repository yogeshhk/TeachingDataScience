%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LLM Evaluation}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Evaluate?}
  \begin{itemize}
  \item Like Teacher grading your exam essay!!
  \item Was it really $6/10$ or it should have been $8/10$?
  \item Words for everyone are different, but still the Teacher has to assess, fairly?
  \item Subjectivity? Bias? Numerical output for Qualitative answers.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Evaluate?}
  \begin{itemize}
    \item Identifying strengths and weaknesses
    \item User trust and reliability
    \item Resource Optimization
    \item Ethical and Bias considerations
    \item Regulatory Compliance
    \item Variability in models
    \item Model Improvements
    \item Usability
  \end{itemize}
  
  {\tiny (Ref: The Science of LLM Benchmarks: Methods, Metrics, and Meanings | LLMOps - LLMOps Space)}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{How to Evaluate?}
  \begin{itemize}
    \item Exact matching approach
    \item Similarity approach
    \item Functional Correctness
    \item Evaluation Benchmarks
    \item Human Evaluation
    \item Model based Approaches (cross val)
  \end{itemize}
  
  {\tiny (Ref: Evaluating LLMs - Rajiv Shah)}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Reliability of Leader-board}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm_eval12}
\end{center}		
  
  {\tiny (Ref: Evaluating LLMs - Rajiv Shah)}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Metrics}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is a Metric}
  \begin{itemize}
    \item Given ``supervised data'' how do we evaluate?
    \item Example: Summarizing news articles - metrics may include:
      \begin{itemize}
        \item Run the model on the `inputs' to get the `predictions'.
        \item Define the `metric' (or score) that estimates how well the model `predictions' reflect the `gold' `outputs'.
        \item Compute the metric
      \end{itemize}
    \item How to compute the score?
      \begin{itemize}
        \item Compute it (Automatic Evaluation)
        \item Let humans do it (Human Evaluation)
      \end{itemize}
  \end{itemize}
		
{\tiny (Ref: LLM Evaluation Basics: Datasets \& Metrics - Generative AI at MIT)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Automatic Evaluation}

\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{llm_eval7}
\end{center}		
		
{\tiny (Ref: LLM Evaluation Basics: Datasets \& Metrics - Generative AI at MIT)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Human Evaluation}
  \begin{itemize}
    \item Some tasks need more nuanced evaluation which cannot be done automatically
    \item Example: text generation
    \item Humans, Crowd Turker, compares model answers with the real answers, against:
      \begin{itemize}
        \item Coherence, readability, fluency
        \item Grammaticality
        \item Extend to which the model follows instructions
      \end{itemize}
    \item Can be done via preference judgment
  \end{itemize}
		
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm_eval8}
\end{center}	
		
{\tiny (Ref: LLM Evaluation Basics: Datasets \& Metrics - Generative AI at MIT)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM: Difference in eval vs Classical ML}
  \begin{itemize}
    \item LLMs are to be evaluated for
      \begin{itemize}
        \item Knowledge
        \item Reasoning
      \end{itemize}
    \item Generative nature makes evaluation complex: semantic matching, format, length etc
    \item Subjectivity of the textual output
    \item Evaluation needs expertise, domain knowledge and reasoning
    \item Must be fast and cheap (even though LLMs are huge and take lots of time to infer)
  \end{itemize}
		
{\tiny (Ref: Evaluating LLM Models for Production Systems: Methods and Practices - Andrei Lopatenko)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluating Large Language Models}
  \begin{itemize}
    \item \textbf{Understanding Needs:} Crucial to evaluate whether LLMs meet specific requirements.
    \item \textbf{Clear Metrics:} Establish clear metrics to gauge the value added by LLM applications.
    \item \textbf{Comprehensive Evaluation:} Encompasses assessing the entire pipeline, including prompts, retrieved documents, and processed content.
    \item \textbf{Pipeline Evaluation:} Assess the effectiveness of individual components within the LLM pipeline.
    \item \textbf{Model Evaluation:} Evaluate the performance of the LLM model itself.
    \item \textbf{Prompt Quality:} Assess the appropriateness and effectiveness of prompts used for LLMs.
    \item \textbf{Output Quality:} Evaluate the quality of the generated output by the LLM.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What are LLM Evaluation Metrics?}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm_eval1}
\end{center}		
		
{\tiny (Ref: LLM Evaluation Metrics: Everything You Need for LLM Evaluation - Jeffrey Ip)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Evaluation Metrics}
  \begin{itemize}
    \item Metrics for scoring an LLM's output based on specific criteria.
    \item Example: Summarizing news articles - metrics may include:
      \begin{itemize}
        \item Sufficient information in the summary.
        \item Absence of contradictions or hallucinations.
      \end{itemize}
    \item For RAG-based architecture, assess the quality of the retrieval context.
    \item LLM evaluation metrics align with the tasks designed for the application.
    \item Note: LLM application can be the LLM itself.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Pipeline Evaluation}
  \begin{itemize}
    \item \textbf{Types of Evaluation:}
      \begin{itemize}
        \item Evaluating Prompts.
        \item Evaluating the Retrieval Pipeline.
      \end{itemize}  
    \item \textbf{Evaluating Prompts:}
      \begin{itemize}
        \item Evaluate prompts' impact on LLM output.
        \item Utilize prompt testing frameworks.
        \item Tools like Promptfoo, PromptLayer, etc., are commonly used.
      \end{itemize}
    \item \textbf{Automatic Prompt Generation:}
      \begin{itemize}
        \item Recent methods automate prompt optimization.
        \item Example: Automatic Prompt Engineer APE.
      \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Pipeline Evaluation}

\begin{center}
\includegraphics[width=0.9\linewidth,keepaspectratio]{llm140}
\end{center}				

{\tiny (Ref: Applied LLMs Mastery 2024 - Aishwarya Reganti)}  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Dimensions of LLM Evaluation}
  \begin{itemize}
    \item \textbf{Relevance Metrics:} Assess pertinence of response to user's query and context.
    \item \textbf{Alignment Metrics:} Evaluate alignment with human preferences. Consider fairness, robustness, and privacy.
    \item \textbf{Task-Specific Metrics:} Gauge LLM performance across various tasks. Examples: multihop reasoning, mathematical reasoning, etc.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Relevance Metrics}
  \begin{itemize}
    \item \textbf{Perplexity:} Measures text prediction quality. Lower values indicate better performance.
    \item \textbf{Human Evaluation:} Human assessors judge relevance, fluency, coherence, and overall quality.
    \item \textbf{BLEU:} Compares generated output with a reference answer. Higher scores indicate better performance.
    \item \textbf{Diversity Metric:} Measures variety and uniqueness of LLM responses using n-gram diversity or semantic similarity.
    \item \textbf{ROUGE:} Evaluates LLM-generated text quality by comparing it with reference text via precision, recall, and F1-score.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Alignment Metrics in LLMs}
  \begin{itemize}
    \item \textbf{Importance:} Crucial for applications directly interacting with people, ensuring conformity to acceptable human standards.
    \item \textbf{Evaluation Challenge:} Difficult to quantify mathematically; relies on specific tests and benchmarks.
    \item \textbf{Key Dimensions:}
      \begin{itemize}
        \item Truthfulness: Accurate representation of information.
        \item Safety: Avoidance of unsafe outputs, promotion of healthy conversations.
        \item Fairness: Prevention of biased outcomes, assessment of stereotypes.
        \item Robustness: Stability across various input conditions.
        \item Privacy: Preservation of human and data autonomy.
      \end{itemize}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Alignment Dimensions}
  \begin{itemize}
    \item \textbf{Machine Ethics:} Divided into implicit ethics, explicit ethics, and emotional awareness.
    \item \textbf{Transparency:} Concerns the availability of information about LLMs and their outputs.
    \item \textbf{Accountability:} Ability to autonomously provide explanations for behavior.
    \item \textbf{Regulations and Laws:} Abiding by rules and regulations posed by nations and organizations.
    \item \textbf{Detailed Analysis:} Each dimension dissected into specific categories with corresponding datasets and metrics.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Task-Specific Metrics}
  \begin{itemize}
    \item \textbf{GLUE:} Collection of nine tasks measuring English text understanding including sentiment analysis, question answering.
    \item \textbf{SuperGLUE:} Extension of GLUE with more challenging comprehension tasks.
    \item \textbf{SQuAD:} Evaluates models on reading comprehension.
    \item \textbf{Commonsense Reasoning:} Winograd Schema Challenge, SWAG for predicting sentence endings.
    \item \textbf{NLI Benchmarks:} MultiNLI, SNLI for predicting entailment, contradiction, or neutrality.
    \item \textbf{Other Benchmarks:} WMT for machine translation, MultiWOZ for dialogue, MBPP for code generation, ChartQA for chart understanding.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Popular Benchmarks}
For text based large language models
  \begin{itemize}
    \item MT-Bench
    \item MMLU
    \item ARC
    \item HELLASWAG
    \item TRUTHFULQA
    \item WINOGRADE
    \item GSM8K
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{MT-Bench}
For chatbots
  \begin{itemize}
    \item LLM as judge to evaluate conversational and instruction following abilities
    \item 8 primary categories such as Writing, Role-play, Coding etc
    \item 10 multi-turn questions in each category
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{MMLU}
Massive Multitask Language Understanding
  \begin{itemize}
    \item Evaluates how well the LLM can multitask, multi-modal capabilities
    \item Multi-choice, variety of tasks, different domains such as STEM, humanities, etc
    \item 15k hand collected questions dataset across 57 tasks
    \item Averages scores per category then averages them
  \end{itemize}
  
\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{llm_eval9}
\end{center}		
		
{\tiny (Ref: Everything WRONG with LLM Benchmarks (ft. MMLU)!!! - 1littlecoder)}
		
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{ARC}
AI2 (Allen Institute also) Reasoning Challenge
  \begin{itemize}
    \item Evaluates how well a model can reason.
    \item Easy set, Challenge set for reasoning and understanding
    \item Hand collected multi-choice set from standardized tests
    \item Difficulty level 3 to 9th grade
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{HELLASWAG}
Harder Ending Longer-Context Low-shot Activities Situations With Adversarial Generations
  \begin{itemize}
    \item Evaluates Common sense
    \item Presents scenarios with multi-choice endings
    \item Data has actions in videos, and there is only one right answer
    \item Dataset of 70k sentence completions (10 shot)
    \item Humans are at 95\%, GPT-4 at 95\%, Palm 87\%
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{TRUTHFULQA}
The Winograd Schema Challenge
  \begin{itemize}
    \item Evaluates Truthfulness
    \item Common sense reasoning benchmark with 44k fill-in-the-blanks with binary options only.
    \item Check against facts, say ``Is Earth flat?''
    \item Random 800 questions, generally misleading
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{WINOGRADE}
  \begin{itemize}
    \item ``The doctor diagnosed Justin with bipolar and Robert with anxiety. ---- had terrible nerves recently.''
    \item Choose between Justin and Robert.
    \item Tests ability to resolve ambiguous pronouns
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{GSM8K}
Grade School Math 8K
  \begin{itemize}
    \item 8.5K basic math problems, needing step-by-step reasoning (2-8 steps).
    \item Tests logic and mathematical abilities
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Leader-board}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm_eval5}
\end{center}		
		
{\tiny (Ref: The Science of LLM Benchmarks: Methods, Metrics, and Meanings | LLMOps - LLMOps Space)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Challenges}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm_eval6}
\end{center}		
		
{\tiny (Ref: The Science of LLM Benchmarks: Methods, Metrics, and Meanings | LLMOps - LLMOps Space)}
			
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Conclusions}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Key Characteristics}
  \begin{itemize}
    \item Quantitative: Metrics should provide a numerical score for task evaluation.
    \item Set a minimum passing threshold for determining LLM application adequacy.
    \item Monitor score changes over time to iterate and improve implementation.
    \item Reliable: Ensure consistency in metric performance, especially with unpredictable LLM outputs.
    \item Beware of inconsistency in LLM-Evals like G-Eval; traditional scoring methods may be more stable.
    \item Accurate: Align metrics with human expectations for meaningful evaluation.
    \item Reliable scores are meaningless if they do not truly reflect LLM application performance.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Takeaways}
  \begin{itemize}
    \item Larger Model $\rightarrow$ Richer Knowledge
    \item Prompting $\rightarrow$ Need to model to provide explanations
    \item Experiment with prompting!
    \item Consider KNN/Few shot approach
    \item In Domain $\rightarrow$ Can't expect explanations outside of the training data
  \end{itemize}
\end{frame}