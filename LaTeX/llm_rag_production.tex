%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large RAG in Production}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Vector DB in Production}

{\tiny (Ref: LinkedIn post by Nirant Kasliwal)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Problem}
\begin{itemize}
    \item Vector search choices often spark endless technical debates.
    \item Teams struggle to balance speed, quality, and cost.
    \item Poor decisions stem from unclear priorities.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Tradeoff Triangle}
\begin{itemize}
    \item Vector search is a balance of:
    \begin{itemize}
        \item Speed (low latency)
        \item Quality (high recall)
        \item Cost (infrastructure)
    \end{itemize}
    \item You can only optimize two; the third will suffer.
    \item Overpromising all three leads to failure.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why a Decision Framework Helps}
\begin{itemize}
    \item Avoids circular discussions.
    \item Focuses on business needs, not technical preferences.
    \item Saves time and clarifies tradeoffs.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Concrete Questions Over Abstract Debates}
\begin{itemize}
    \item Is cost a hard constraint?
    \item Do we need latency under 50ms?
    \item What recall can users actually perceive?
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{A Real-World Example}
\begin{itemize}
    \item Product team wanted ``Google recall'' + ``Stripe latency'' + startup budget.
    \item Framework helped align on priorities.
    \item Solution: quantized index optimized for user-impacting metrics.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Vector Search = Navigation}
\begin{itemize}
    \item Optimization is like choosing a GPS route.
    \item Fastest isn’t always best—consider tolls and risk.
    \item Decision tree guides you to the best compromise.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Vector Db Descion Tree}
		\begin{center}
		\includegraphics[width=0.8\linewidth,keepaspectratio]{rag38}
		\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Key Takeaway}
\begin{itemize}
    \item Use structured questions to steer tradeoffs.
    \item Let business goals—not tech idealism—drive choices.
    \item This framework turns friction into clarity.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Embedding Model}

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is RAG?}
    \begin{itemize}
        \item RAG augments LLM responses with context retrieved from a vector store.
        \item Similarity search retrieves relevant documents via query-document embedding comparisons.
        \item Embedding quality critically impacts retrieval and overall RAG performance.
        \item Choice of embedding model directly influences LLM response quality.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Choosing an Embedding Model}
    \begin{itemize}
        \item Use MTEB leaderboard to assess embedding model performance.
        \item MTEB ranks models across diverse NLP tasks for comprehensive evaluation.
        \item However, top rank does not always imply best fit for your use case.
		\item Which Embedding dimension? As much as you can, though more meaningful, can make search slwoer
		\item Most models don't allow size change so be sure at first.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{MTEB Leaderboard}
Hugging Face MTEB (Massive Text Embedding Benchmark) leaderboard, as shown below:

		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llm169}
		
		{\tiny (Ref: https://huggingface.co/spaces/mteb/leaderboard on 1 July 2025)}
		
		\end{center}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Understanding the MTEB Leaderboard}
    \begin{itemize}
        \item Hosted on Hugging Face to benchmark embedding models.
        \item Covers tasks like classification, clustering, retrieval, STS (Semantic Texutal Similarity), summarization.
		\item STS focuses on measuring the semantic similarity between sentence pairs, while retrieval aims to find relevant documents from a corpus given a query
        \item Enables holistic model comparison across varied NLP scenarios.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Factors in Model Selection}
    \begin{itemize}
        \item Evaluate performance on tasks relevant to your use case.
        \item Consider compute requirements and inference speed.
        \item Prefer domain-specific models for specialized applications.
        \item Test models using your actual data for best alignment.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Domain-Specific Models}
    \begin{itemize}
        \item \textbf{Medicine}: PubMedBERT, BioLORD for clinical and biomedical texts.
        \item \textbf{Finance}: Investopedia, Voyage, BGE Financial Matryoshka.
        \item \textbf{Law}: Legal-specific models for legal research and analysis.
        \item \textbf{Code}: CodeBERT, GraphCodeBERT for programming-related tasks.
        \item \textbf{Math}: Math Similarity Model for mathematical expressions.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Models for Other Languages}
    \begin{itemize}
        \item \textbf{Japanese}: RoSEtta-base-ja
        \item \textbf{Korean}: KoSimCSE-roberta
        \item \textbf{Chinese}: GTE-Qwen2-7B-instruct
        \item \textbf{French}: Sentence-Camembert-large
        \item \textbf{Arabic}: Arabic-STS-Matryoshka
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large LLM/RAG in Production}

{\tiny (Ref:  Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Factors to Consider Before LLM/RAG Deployment}
  \begin{itemize}
    \item Evaluate metrics before putting LLMs or RAG into production.
    \item Consider cost, latency, and throughput during model selection.
    \item Choose between open-source vs. proprietary models.
    \item Ensure high response quality and relevance.
    \item Establish a unified framework for evaluating production readiness.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Four Major Industrial Focus Areas}
  \begin{itemize}
    \item Document processing and information extraction.
    \item Knowledge base and question answering systems.
    \item Domain-specific conversational agents (e.g., logistics, customer support).
    \item Workflow automation using generative AI (e.g., email handling).
    \item These are not exhaustive but reflect current industry trends.
  \end{itemize}
  
	\begin{center}
	\includegraphics[width=0.8\linewidth,keepaspectratio]{llm170}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RAG Pipeline Overview}
  \begin{itemize}
    \item Data ingestion: chunking and embedding documents.
    \item Data retrieval: semantic search from vector databases.
    \item Data synthesis: LLM generates answer using retrieved chunks.
    \item Pipeline involves multiple tightly coupled components.
    \item Output: curated, context-aware response to user query.
  \end{itemize}
  
	\begin{center}
	\includegraphics[width=0.8\linewidth,keepaspectratio]{llm171}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Production Challenges in RAG Systems}
  \begin{itemize}
    \item Building a demo is easier than productionizing RAG.
    \item Ensure high-quality responses with minimal hallucinations.
    \item Evaluate and compare different RAG approaches.
    \item Validate sub-pipeline performance independently.
    \item Continuously monitor system via ML Ops practices.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrieval Issues in RAG}
  \begin{itemize}
    \item \textbf{Low precision}: irrelevant chunks retrieved (e.g., Banur example).
    \item \textbf{Low recall}: important chunks may be missed.
    \item Retrieval quality directly impacts final response relevance.
    \item Fixing retrieval quality is crucial before tuning generation.
    \item Retrieval performance is often the primary bottleneck.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Generation Issues in RAG}
  \begin{itemize}
    \item \textbf{Hallucination}: generating incorrect or fabricated facts.
    \item \textbf{Irrelevance}: off-topic responses from the LLM.
    \item \textbf{Toxicity}: potentially offensive content.
    \item Importance of implementing guardrails and filters.
    \item Regular evaluation and reinforcement needed for safe deployment.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Metrics for Retrieval}
  \begin{itemize}
    \item \textbf{Context recall}: fraction of relevant chunks retrieved.
    \item \textbf{Context precision}: fraction of retrieved chunks that are relevant.
    \item \textbf{Hit rate}: binary metric of whether any relevant chunk was retrieved.
    \item \textbf{MRR (Mean Reciprocal Rank)}: rank of first relevant item.
    \item Helps quantify retrieval pipeline effectiveness.
  \end{itemize}

	\begin{center}
	\includegraphics[width=0.8\linewidth,keepaspectratio]{llm172}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Metrics for Retrieval}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm173}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Metrics for Generation}
  \begin{itemize}
    \item \textbf{ROUGE}: overlap of n-grams for summarization accuracy.
    \item \textbf{BLEU}: word overlap between generated and reference text.
    \item \textbf{METEOR}: synonym-aware overlap evaluation.
    \item \textbf{LLM metrics}: faithfulness, correctness, and toxicity.
    \item Enables robust scoring of output text quality.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Metrics for Generation}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm174}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Metrics for Frameworks}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm175}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{End-to-End Evaluation: RAG Triad}
  \begin{itemize}
    \item \textbf{Answer relevance}: does output match the query intent?
    \item \textbf{Context relevance}: are retrieved chunks relevant?
    \item \textbf{Groundedness}: is answer supported by retrieved content?
    \item Context relevance is most critical for response quality.
    \item Use TrueLens or similar tools for triad analysis.
  \end{itemize}
  
	\begin{center}
	\includegraphics[width=0.8\linewidth,keepaspectratio]{llm176}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}     
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Sample Two Columns Slide}
% \begin{columns}
    % \begin{column}[T]{0.5\linewidth}
		\begin{center}
		\includegraphics[width=0.7\linewidth,keepaspectratio]{llm177}
		\end{center}  
    % \end{column}
    % \begin{column}[T]{0.5\linewidth}
		\begin{center}
		\includegraphics[width=0.7\linewidth,keepaspectratio]{llm178}
		\end{center}  
    % \end{column}
  % \end{columns}
  
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Improving RAG: Optimization Techniques}
  \begin{itemize}
    \item \textbf{Sentence Window Retrieval}: retrieve context with surrounding sentences.
    \item Enhances context richness, increasing relevance and groundedness.
    \item \textbf{Auto-merging Retrieval}: combine overlapping chunks for completeness.
    \item Better context results in higher-quality LLM responses.
    \item Evaluate and tune using metrics before production.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Sentence Window Retrieval}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm179}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Auto-merging Retrieval}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm180}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{More \ldots}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm181}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Frameworks and Tools}
  \begin{itemize}
    \item Popular frameworks: LangChain, LlamaIndex, TruLens, RAGAS.
    \item Each offers tools for retrieval and generation evaluation.
    \item Some provide full support for RAG Triad scoring.
    \item Choose tools based on needs (e.g., preference for RAGAS).
    \item Experiment with multiple frameworks to validate performance.
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{More \ldots}

	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm181}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Cost Analysis Components}
      \begin{itemize}
	\item Initial Setup \& Inference: Model storage and prediction costs
	\item Maintenance: Fine-tuning, training, and data labeling expenses
	\item Associated Costs: CO2 emissions, carbon footprint impact
	\item Human Resources: Training costs and specialized talent acquisition
	\item Daily usage can exceed AWS costs for extensive applications
	  \end{itemize}
	  
	\begin{center}
	\includegraphics[width=\linewidth,keepaspectratio]{llm182}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   	  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Third-Party API vs On-Premise Hosting}
      \begin{itemize}
	\item OpenAI APIs: Separate costs for input tokens, output tokens, per-request
	\item Local hosting viable for 3-7 billion parameter models with decent hardware
	\item Cloud instances: \$0.6 to \$45 per hour for large models (30B-70B parameters)
	\item A800 Nvidia GPUs: Up to 640GB GPU RAM for massive models
	\item Decision based on parameter count and cost comparison analysis
	  \end{itemize}

	\begin{center}
	\includegraphics[width=0.8\linewidth,keepaspectratio]{llm183}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Maintenance Cost Structures}
      \begin{itemize}
	\item Vertex AI AutoML: Upload, training, deployment, prediction costs
	\item OpenAI: Training cost plus input/output usage fees
	\item Data labeling: Third-party platforms or human annotators
	\item Fine-tuning expenses vary by model size and training duration
	\item Free data upload up to first 1000 pages in some platforms
	  \end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Maintenance Cost Structures}

	\begin{center}
	\includegraphics[width=0.7\linewidth,keepaspectratio]{llm184}
	
	\includegraphics[width=0.7\linewidth,keepaspectratio]{llm185}

	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center}   
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RAG vs Fine-tuning Cost Comparison}
      \begin{itemize}
	\item Example: 10 million tokens, 15 days monthly usage
	\item Fine-tuning: \$251 total (LLM + embedding + compute costs)
	\item RAG: \$723 total (\$437 output tokens + \$70 Pinecone + compute)
	\item Output token costs dominate RAG expenses
	\item No universal answer - depends on task, model, and token usage
	  \end{itemize}
	  
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RAG vs Fine-tuning Cost Comparison}

	\begin{center}
	\includegraphics[width=0.7\linewidth,keepaspectratio]{llm186}
	
	\includegraphics[width=0.7\linewidth,keepaspectratio]{llm187}
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center} 	  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Cost Optimization Strategies}
      \begin{itemize}
	\item Prompt engineering for efficient token usage
	\item Caching with vector stores to reduce repeated computations
	\item Fine-tuning for specific use cases
	\item Chain strategies for long documents (MapReduce, MapRerank, Refine)
	\item Conversation summary memory for chat history optimization
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Conversation Summary Memory}
      \begin{itemize}
	\item Maintains context across long conversations without token limit issues
	\item Traditional buffer memory hits 2048 token limits and loses context
	\item Summary memory condenses chat history before appending to context
	\item LangChain provides conversation summary memory class
	\item Example: "Human asks about AI, AI responds" becomes curated summary
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Advanced Context Management Solution}
      \begin{itemize}
	\item Summarization still loses context after extended conversations
	\item Solution: Apply RAG pipeline approach to chat history
	\item Generate embeddings for conversation history and store in vector database
	\item Query vector database with current conversation context
	\item Retrieve relevant historical context dynamically for each interaction
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory Efficiency Comparison}
      \begin{itemize}
	\item Buffer memory grows linearly with conversation length (blue line)
	\item Summary memory less efficient initially due to limited conversation data
	\item Summary memory becomes superior after 50-70 conversation dialogues
	\item Early conversations lack sufficient data for effective summarization
	\item Token efficiency improves significantly in longer conversation sessions
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Latency and Throughput Metrics}
      \begin{itemize}
	\item Time to First Token: Initial delay before response generation begins
	\item Time per Output Token: Generation time for each individual output token
	\item Latency: Time to first token + (time per output token × tokens generated)
	\item Throughput: Rate of output token generation across all users
	\item These metrics critical for optimizing inference speed performance
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Performance Trade-offs and Heuristics}
      \begin{itemize}
	\item Trade-off between throughput and time per output token exists
	\item Supporting 16 concurrent users improves throughput but increases per-token time
	\item Output length dominates overall response latency calculation
	\item Input length affects hardware requirements, not performance significantly
	\item Overall latency scales sublinearly with model size parameters
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Hardware and Performance Considerations}
      \begin{itemize}
	\item Input token count doesn't significantly impact latency or throughput
	\item Longer inputs require models with higher context length limits
	\item Bigger models or better-trained parameters needed for extended contexts
	\item Memory and compute costs increase with hardware requirements
	\item Speed ratio doesn't match parameter count ratio across different models
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Analysis Tool}
      \begin{itemize}
	\item Open-source library for latency, throughput, memory, and cost analysis
	\item Test different training and inference combinations before production
	\item Identify potential out-of-memory errors theoretically
	\item Optimize batch size for peak hardware utilization
	\item Determine optimal data types (fp16, int) for specific setups
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Efficiency Metrics and Analysis}
      \begin{itemize}
	\item FLOP Hardware Efficiency: 0.5 for training, 0.7 for inference (hardware utilization)
	\item Memory Efficiency: Measures read/write efficiency between RAM, CPU, GPU
	\item Tool provides latency, throughput, and pricing based on token parameters
	\item Batch size optimization through comparative analysis
	\item Tokens per second throughput and completion time calculations
	  \end{itemize}

	\begin{center}
	\includegraphics[width=0.6\linewidth,keepaspectratio]{llm188}
	
	
	{\tiny (Ref: Shipping LLM: Addressing Production Challenges - Venkatesh \& Suman)}
	\end{center} 	  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Orchestration and Monitoring}
      \begin{itemize}
	\item Cost tracking tools for anomaly detection (e.g., AIO AI)
	\item Monitor sudden price spikes and request increases globally
	\item Track compute and memory consumption patterns
	\item Observability for production LLM applications
	\item Part of comprehensive monitoring and alerting strategy
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation Input Strategy}
      \begin{itemize}
	\item Research recommends 20 questions minimum for initial evaluation
	\item 100 evaluation questions needed for production-ready assessment
	\item Domain expert curation for specialized fields (healthcare, dialysis)
	\item Automated question generation using GPT-4 as alternative approach
	\item Manual curation provides better rationalization than automated methods
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Domain-Specific Evaluation Approaches}
      \begin{itemize}
	\item Life-critical applications require domain expert involvement
	\item Generic applications (FAQ, customer service) allow broader user input
	\item TrueLens provides detailed scoring with root cause analysis
	\item Answer relevance, context relevance, and groundedness evaluation
	\item Risk assessment determines evaluation rigor requirements
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Production Observability and Monitoring}
      \begin{itemize}
	\item Deploy evaluation pipeline chunks for regular performance monitoring
	\item Set thresholds for context relevance score degradation (0.55 to 0.39)
	\item Proactive measures through fortnightly evaluation cycles
	\item Monitor user query patterns and response success ratios
	\item Reactive vs proactive monitoring strategies for different use cases
	  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Anthropic :  How to Build AI Agents}

{\tiny (Ref; LinkedIn post by Maryam Miradi)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{When to Build AI Agents}
    \begin{itemize}
        \item Don’t build agents for every task.
        \item Ideal for complex, ambiguous, high-value problems.
        \item Use workflows when decision paths are clear.
        \item Agents consume tokens—ensure value justifies cost.
        \item Avoid if error discovery is slow or high-risk.
        \item Limit autonomy when safety is critical.
        \item Use checklist: complexity, value, bottlenecks, risks.
        \item Coding is a great fit: complex but verifiable.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Designing Simple, Scalable Agents}
    \begin{itemize}
        \item Every agent = Model + Tools + Environment.
        \item Start with extremely simple components.
        \item Avoid early complexity—hurts iteration speed.
        \item Reuse agent backbones for many use cases.
        \item Recombine code, tools, and prompts easily.
        \item Don’t optimize until behavior is stable.
        \item Visual clarity builds trust in the agent.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Optimization \& Performance}
    \begin{itemize}
        \item Parallelize tools to lower latency.
        \item Cache action paths in coding agents to save tokens.
        \item Show step-by-step progress to build trust.
        \item Optimize costs only after validating core loop.
        \item Keep environments simple before scaling up.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Think Like Your Agent}
    \begin{itemize}
        \item Agent only sees its limited context window.
        \item Don’t expect magic—expect bounded reasoning.
        \item Weird actions often mean missing context.
        \item Simulate tasks from the agent’s view.
        \item Debug using only the agent’s available info.
        \item Poor UI? Add metadata or improve resolution.
        \item Replay full trajectory—ask the model “why?”
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Tools \& Self-Improvement}
    \begin{itemize}
        \item Define tools with clear inputs and expected outputs.
        \item Use the LLM to review tool clarity.
        \item Let agents self-critique prompts and tools.
        \item Build meta-tools that improve agent tooling.
        \item Better ergonomics reduce errors and retries.
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Future: Multi-Agent \& Budget-Aware}
    \begin{itemize}
        \item Solo agents dominate now—but not for long.
        \item Multi-agent = modular reasoning + parallel tasks.
        \item Sub-agents help preserve main context window.
        \item Async interactions beat synchronous limitations.
        \item Role-based coordination is the next big step.
        \item Budget-awareness = tokens, time, latency constraints.
        \item Define strict resource limits before deployment.
    \end{itemize}
\end{frame}

