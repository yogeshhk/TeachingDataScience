%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Installation \& Imports}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Installation (recommended current packages)
pip install langchain langchain-core langchain-openai langchain-community
pip install langchain-anthropic langchain-google-genai

# Core Imports (modern runnable-style usage)
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate
from langchain.schema import HumanMessage, SystemMessage, AIMessage

# Runnables & Output Parsers (core)
from langchain_core.runnables import RunnableSequence  # for explicit sequences if needed
from langchain_core.output_parsers import StrOutputParser, StructuredOutputParser, PydanticOutputParser

# Agents / Tools (new helpers)
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import Tool, create_retriever_tool

# Memory (same import path)
from langchain.memory import ConversationBufferMemory

# Community connectors (example)
from langchain_community.embeddings import HuggingFaceEmbeddings
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Message Types \& Chat}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Message Schema (unchanged)
from langchain.schema import HumanMessage, SystemMessage, AIMessage

messages = [
    SystemMessage(content="You are a helpful AI assistant"),
    HumanMessage(content="What is machine learning?"),
    AIMessage(content="ML is a subset of AI..."),
]

# Chat prompt templates (runnable-friendly)
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate

human = HumanMessagePromptTemplate.from_template("{question}")
chat_prompt = ChatPromptTemplate.from_messages([human])

llm = ChatOpenAI(temperature=0.0)
# Use pipe operator to run prompt -> llm
chain = chat_prompt | llm
# invoke with a dict containing prompt variables
resp = chain.invoke({"question": "Explain supervised learning in 3 bullets."})
print(resp)  # raw LLM response
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Prompt Templates \& Output Parsers}
\begin{lstlisting}[language=python,basicstyle=\tiny]
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, StructuredOutputParser
from pydantic import BaseModel, Field

# Classic PromptTemplate -> used with runnables
prompt = PromptTemplate(
    input_variables=["text","language"],
    template="Translate the following text to {language}:\n\n{text}"
)

# Example of piping prompt -> llm and parsing to string
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(temperature=0.2)
chain = prompt | llm | StrOutputParser()
out = chain.invoke({"text": "Hello", "language": "French"})
print(out)  # plain string

# Structured output parser example (schema-backed)
from langchain_core.output_parsers import StructuredOutputParser, ResponseSchema
schemas = [
    ResponseSchema(name="summary", description="Short summary"),
    ResponseSchema(name="source", description="source of info")
]
parser = StructuredOutputParser.from_response_schemas(schemas)
prompt2 = PromptTemplate(
    input_variables=["doc"],
    template="Read the document and return json with keys 'summary' and 'source'.\n\n{doc}\n\n{parser_instructions}"
)
# inject the parser instructions into the prompt
prompt_text = prompt2.format(doc="...", parser_instructions=parser.get_format_instructions())
chain2 = PromptTemplate(
    input_variables=["doc"],
    template=prompt_text
) | llm
result = chain2.invoke({"doc": "Long document text"})
# Then parse the result using the parser (or include parser as a runnable)
parsed = parser.parse(result)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Loaders \& Documents (unchanged interfaces)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Text file loader
from langchain.document_loaders import TextLoader
loader = TextLoader("file.txt")
docs = loader.load()

# CSV Loader
from langchain.document_loaders import CSVLoader
loader = CSVLoader("data.csv")
docs = loader.load()

# Web Page Loader
from langchain.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://example.com")
docs = loader.load()

# Directory Loader
from langchain.document_loaders import DirectoryLoader
loader = DirectoryLoader("./docs", glob="**/*.txt")
docs = loader.load()

# Unstructured Loader (docx/pdf)
from langchain.document_loaders import UnstructuredFileLoader
loader = UnstructuredFileLoader("file.docx")
docs = loader.load()
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Text Splitters \& Embeddings}
\begin{lstlisting}[language=python,basicstyle=\tiny]
from langchain.text_splitter import CharacterTextSplitter
splitter = CharacterTextSplitter(separator="\n\n", chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_text(long_text)

# Example: embeddings -> vectorstore
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

embeddings = OpenAIEmbeddings()
vectors = FAISS.from_documents(chunks, embeddings)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Runnables: Replacing LLMChain}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Legacy:
# chain = LLMChain(llm=llm, prompt=prompt)
# out = chain.run("input")

# Modern (runnable/LCEL) pattern:
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

prompt = PromptTemplate(input_variables=["name"], template="Say hello to {name}.")
llm = ChatOpenAI(temperature=0.0)

# Compose with | (prompt -> llm)
pipeline = prompt | llm
# Invoke: pass a dict of the prompt variables
result = pipeline.invoke({"name": "Alice"})
print(result)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Sequential Pipelines (Replacing SimpleSequentialChain)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Legacy: SimpleSequentialChain([...])
# Modern: chain runnables with '|' or RunnableSequence

from langchain_core.runnables import RunnableSequence

p1 = PromptTemplate(input_variables=["text"], template="Summarize: {text}")
p2 = PromptTemplate(input_variables=["summary"], template="Translate to French: {summary}")

pipeline = (p1 | llm) | (p2 | llm)  # outputs of first feed second automatically
# or build an explicit sequence:
sequence = RunnableSequence([p1 | llm, p2 | llm])
out = sequence.invoke({"text": "Long text to summarize"})
print(out)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Callbacks \& Tracing (same handlers, new manager)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManager

class MyCallback(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print("LLM started:", prompts)
    def on_llm_end(self, response, **kwargs):
        print("LLM finished")

cb_manager = CallbackManager([MyCallback()])
llm = ChatOpenAI(callback_manager=cb_manager)
# run as usual with runnables; callbacks will be invoked
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Local \& HF Models (examples kept)]
\begin{lstlisting}[language=python,basicstyle=\tiny]
# HuggingFace Pipeline (local)
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
llm_local = HuggingFacePipeline(pipeline=pipe)

# Use as any other llm runnable:
prompt = PromptTemplate(input_variables=["q"], template="{q}")
resp = (prompt | llm_local).invoke({"q": "Write a haiku about code."})
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrieval \& RAG (Replacing RetrievalQA)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Legacy: qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
# Modern: use create_retrieval_chain or build a retriever tool

from langchain.chains import create_retrieval_chain

retriever = vectorstore.as_retriever(search_kwargs={"k":4})

# create_retrieval_chain returns a runnable LCEL-style chain
rag_chain = create_retrieval_chain(llm=llm, retriever=retriever, chain_type="stuff")
# invoke using {"query": ...}
ans = rag_chain.invoke({"query": "What is the capital of France?"})
print(ans)

# Alternatively: create a retriever-based tool for agents
retriever_tool = create_retriever_tool(retriever=retriever, name="docs", description="Useful for answering questions from the docs")
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Agents \& Tools (Replacing initialize_agent)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Legacy:
# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)

# Modern: use create_react_agent (or other create_* agents) + AgentExecutor
from langchain.agents import create_react_agent, AgentExecutor

# Build Tool objects (example)
from langchain.tools import Tool

def calc_func(arg: str) -> str:
    # local tool example
    return str(eval(arg))

calculator_tool = Tool.from_function(fn=calc_func, name="calc", description="Evaluate math expressions")
# include retriever_tool from above for docs lookup
tools = [calculator_tool, retriever_tool]

# create a React-style agent and wrap in executor
agent = create_react_agent(llm=llm, tools=tools)
agent_executor = AgentExecutor(agent=agent, tools=tools)

# Run the agent:
response = agent_executor.run("What's 2**10 and also: what is the summary of the docs about deployment?")
print(response)

# For advanced workflows consider LangGraph for agent graphs (see docs)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Memory (unchanged usage)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key="chat_history")
# Pass memory into an agent or a custom runnable (AgentExecutor can accept memory in its run)
res = agent_executor.run("Starter question")
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLM Config \& Parameters (examples)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# temperature / max_tokens unchanged at model construction
llm = ChatOpenAI(temperature=0.7, max_tokens=500)

# streaming & callbacks are supported via callback_manager when constructing the model
from langchain.callbacks.manager import CallbackManager
llm_streaming = ChatOpenAI(streaming=True, callback_manager=cb_manager)

# Using prompt | llm | parser pipeline:
from langchain_core.output_parsers import StrOutputParser
pipeline = PromptTemplate(input_variables=["q"], template="{q}") | llm | StrOutputParser()
answer = pipeline.invoke({"q": "Write a short definition of RAG."})
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Short Examples / Patterns (cheat sheet)}
\begin{lstlisting}[language=python,basicstyle=\tiny]
# Basic prompt -> llm
prompt = PromptTemplate(input_variables=["x"], template="Reverse: {x}")
resp = (prompt | llm).invoke({"x":"abc"})
print(resp)

# Chain with parsing
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel
class Person(BaseModel):
    name: str
    age: int

parser = PydanticOutputParser(pydantic_object=Person)
chain = PromptTemplate(input_variables=["txt"], template="Return json matching Person for: {txt}") | llm | parser
out = chain.invoke({"txt": "Name Bob age 30"})
print(out)
\end{lstlisting}
\end{frame}
