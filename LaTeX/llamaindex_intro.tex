%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Introduction}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Background}

\begin{itemize}
\item LLMs (Large Language Models) are pre-trained on large amounts of publicly available data. Sometimes having a cut-off date, say, Nov 2021.
\item How do we best augment LLMs with our own private data?
\item But how to ingest private knowledge? Many ways \ldots
\item How to use them with private data: Few shots, RAG, Fine-Tuning. 
\item Fine-tuning: adding last layer, and retraining the weights, but downsides are:
	\begin{itemize}
	\item Data preparation effort
	\item Lack of transparency
	\item May not work well
	\item High upfront cost
	\end{itemize}	
\item Most effective-middle-ground: RAG (Retriveal Augmented Generation) ie In-context learning - putting context into the prompt
\end{itemize}	


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{In-context learning}

Key challenges:

\begin{itemize}
\item How to retrieve the right context for the prompt?
\item How to deal with long context? 
\item How to deal with source data that is potentially very large? (GB’s, TB’s) 
\item How to trade-off between: Accuracy, Latency, Cost
\end{itemize}	

Solution \ldots LlamaIndex

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is LlamaIndex?}

\begin{itemize}
\item LlamaIndex, previously known as the GPT Index, is the advanced data framework for your LLM applications
\item Data Management: Data connectors (LlamaHub), Data ingestion, data parsing/slicing, data storage/indexing.
\item Data Querying: Data retrieval, response synthesis, multi-step interactions over your data.
\item LlamaIndex allows you to seamlessly integrate individual or enterprise data, including files, workplace apps, and databases, with LLM applications. 
\item Calling an LLM API is easy. Setting up a software system that can extract insights from your private data is harder.
\end{itemize}	

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{What is LlamaIndex?}

% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{llamaindex19}

% {\tiny (Ref: What is LlamaIndex? - Leeway Hertz)}
% \end{center}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{From Jerry Liu, co-founder, LlamaIndex?}

\begin{itemize}
\item LLMs are fantastic reasoning engines, capable of question-answering, summarization, planning, and more. They had the promise of becoming the “neural” compute unit at the core of a new age of AI-enabled software.
\item Yet, LLMs inherently have no awareness of your own data.
\item No one really knew the best practices for feeding your data into the LLM. Models had limited context windows and were expensive to finetune.
\item If we could offer a toolkit to help set up the data architecture for LLM apps, then we could enable anyone to build LLM-powered knowledge workers and transform the way that software is written over private data.
\end{itemize}	

LlamaIndex became viewed as a critical data orchestration component of the emerging LLM software landscape.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What is Context Augmentation?}
\begin{itemize}
  \item Bridges the gap between LLMs and your private data.
  \item Ingests and makes data (PDFs, APIs, SQL, etc.) usable by LLMs.
  \item Supports full pipeline: ingest, parse, index, query.
  \item Popular use case: Retrieval-Augmented Generation (RAG).
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LlamaIndex Framework Overview}
\begin{itemize}
  \item No restriction on how LLMs are used — auto-complete, agents, etc.
  \item \textbf{Data Connectors:} Ingest from APIs, PDFs, SQL, and more.
  \item \textbf{Data Indexes:} Prepare data for fast LLM access.
  \item \textbf{Engines:} 
    \begin{itemize}
        \item Query Engines for Q\&A (e.g., RAG).
        \item Chat Engines for conversation.
    \end{itemize}
  \item \textbf{Agents:} Tool-augmented LLM knowledge workers.
  \item \textbf{Observability:} Tools to monitor, evaluate, and improve.
  \item \textbf{Workflows:} Combine all modules in flexible flows.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Agents}
\begin{itemize}
  \item LLM-powered automated decision-makers.
  \item Use tools to interact with the world and solve tasks.
  \item Take dynamic, multi-step actions — not hardcoded sequences.
  \item Well-suited for complex, adaptive tasks.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Workflows}
\begin{itemize}
  \item Event-driven abstractions to orchestrate LLM calls.
  \item Combine tools, agents, and logic in a structured flow.
  \item Power any agentic application in LlamaIndex.
  \item Core to building scalable LLM-based systems.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Structured Data Extraction}
\begin{itemize}
  \item Use Pydantic extractors to define precise data schemas.
  \item LLMs fill missing pieces from unstructured sources.
  \item Enables structured outputs from PDFs, websites, etc.
  \item Crucial for automating robust workflows.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Query Engines}
\begin{itemize}
  \item End-to-end interface for Q\&A over your data.
  \item Accepts natural language queries.
  \item Retrieves reference context and generates a response.
  \item Ideal for Retrieval-Augmented Generation (RAG).
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Chat Engines}
\begin{itemize}
  \item Built for interactive, conversational interfaces.
  \item Handle multiple back-and-forth queries with memory.
  \item Let users explore data via natural dialog.
  \item Great for copilots and customer support bots.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Who is LlamaIndex for?}
\begin{itemize}
  \item For all levels — from beginners to experts.
  \item High-level API for quick prototyping:
  \item Low-level APIs for advanced customization.
\end{itemize}

\begin{lstlisting}[language=Python]
from llama_index import VectorStoreIndex, SimpleDirectoryReader
documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Your question here")
\end{lstlisting}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{30 Second Quickstart}
\begin{itemize}
  \item By default, uses OpenAI gpt-3.5-turbo model for text generation and text-embedding-ada-002 for retrieval and embeddings. So, set \texttt{OPENAI\_API\_KEY} environment variable.
  \item Install the libraries
  \item Place documents in a folder named \texttt{data}.
  \item Use the 5-line starter to run a query:
  \item For advanced use, see full tutorials with remote/local models.
\end{itemize}

\begin{lstlisting}[language=Python]
pip install llama-index

---
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Some question about the data should go here")
print(response)
\end{lstlisting}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{For Open Source or Local models}
\begin{itemize}
  \item Use ollama or LM Studio for local models
  \item Use Groq (not Grok by X) API service to call Open source models
  \item Once set in Settings they are available to LlamaIndex (like global)
\end{itemize}

\begin{lstlisting}[language=Python]
from llama_index.llms.groq import Groq
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings

llm = Groq(model="llama3-8b-8192")
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
Settings.llm = llm
Settings.embed_model = embed_model
\end{lstlisting}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LlamaCloud}
\begin{itemize}
  \item End-to-end managed service for LLM data pipelines.
  \item Handles parsing, ingestion, indexing, and retrieval.
  \item Suitable for production-grade LLM applications.
  \item Available as both hosted and self-hosted solutions.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LlamaParse}
\begin{itemize}
  \item State-of-the-art document parser from LlamaIndex.
  \item Part of LlamaCloud or usable via self-serve API.
  \item Free plan allows parsing up to 1000 pages/day.
  \item Paid plan offers unlimited parsing with credit card.
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{So, LlamaIndex: A interface between your data and LLMs }


\begin{itemize}
\item Data management + query engine
\item Goal is to make this interface fast, cheap, efficient, and performant 
\item Components:
	\begin{itemize}
	\item Data Ingestion (LlamaHub): Connect your existing data sources and data formats (API’s, PDF’s, docs, SQL, etc.)
	\item Data Structure: Store and index your data in different data structures such as lists, trees, graphs, for different use cases. 
	\item Query Interface: Feed in an input prompt and obtain a knowledge-augmented output.
	\end{itemize}	

\end{itemize}	
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Overview}
    \begin{itemize}
        \item Two key phases: data processing and querying
        \item Indexes organize document chunks (nodes) into structures
        \item Supports list, tree, and keyword table index types
        \item Customizable chunking with built-in or custom splitters
    \end{itemize}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{llamaindex19}

{\tiny (Ref: What is LlamaIndex? - Leeway Hertz)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Index Composability}
    \begin{itemize}
        \item Indexes can be composed from other indexes, not just nodes
        \item Useful for querying across diverse data sources
        \item Tree and list indexes can combine multiple sub-indexes
        \item Summaries help guide queries to correct sub-indexes
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Querying: List \& Vector Indexes}
    \begin{itemize}
        \item List index: sequential traversal of all nodes
        \item Prompts refined step-by-step as nodes are processed
        \item Vector index: fetches only relevant nodes via embeddings
        \item Supports external vector DBs like PineCone
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Synthesis Methods}
    \begin{itemize}
        \item Create and refine: iterative response generation
        \item Tree summarize: builds a query-guided summary tree
        \item Compact: fits max nodes per prompt to reduce cost
        \item Prompts can be fully customized
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Storage Architecture}
    \begin{itemize}
        \item Stores: document, index, and vector data
        \item Options: in-memory or MongoDB for persistence
        \item Supports external vector DBs (e.g., PineCone, Chroma)
        \item `storage\_context` aggregates all storage configs
    \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{What it has?}


\begin{itemize}
\item 100+ data loaders
\item 13+ vector database providers
\item Integration with observability and experimentation frameworks (e.g. prompt tracking and system tracing)
\item Integration as a ChatGPT Retrieval Plugin or with Poe
\end{itemize}	

Vision: unlocking LLM capabilities on top of your data.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LangChain vs. LlamaIndex}

\begin{tabular}{|p{2cm}|p{4cm}|p{4cm}|}
\hline
\textbf{} & \textbf{LangChain} & \textbf{LlamaIndex} \\
\hline
\textbf{Primary Function} & 
Python library for building NLP apps with LLMs. & 
Connects LLMs to external data sources via custom indexes. \\
\hline
\textbf{Key Features} & 
Supports GPT-2/3, T5 – Tokenization, QA, summarization – Great for chatbots. & 
Links with data like Wikipedia, Stack Overflow – Topic extraction – Supports GPT-2/3/4, T5. \\
\hline
\textbf{Use Cases} & 
Chatbots, summarizing text, simple QA systems. & 
Index-based QA, topic extraction from unstructured data. \\
\hline
\end{tabular}


{\tiny (Ref: What is LlamaIndex? - Leeway Hertz)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Applications - Use Cases}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Semantic Search}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm29}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Summarization}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm30}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Unified Query Interface}

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{llamaindex15}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Document Comparison}

\begin{columns}
    \begin{column}[T]{0.45\linewidth}

		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llamaindex16}

		{\tiny (Ref: Getting started with LlamaIndex - AI Planet)}
		\end{center}
		
    \end{column}
    \begin{column}[T]{0.45\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llamaindex17}

		{\tiny (Ref: Getting started with LlamaIndex - AI Planet)}
		\end{center}
    \end{column}
  \end{columns}
  
  


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Text-to-SQL (Structured Data)}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm31}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Synthesis over Heterogeneous Data}

Composing an index over other indexes (a list index over vector indexes). 
The query will be routed to both simple vector indexes! 


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm32}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Composite Document: Text + Tables}


\begin{columns}
    \begin{column}[T]{0.4\linewidth}

		\begin{itemize}
		\item Recursive Retrieval expands exploration from directly relevant nodes to related retrievers/query engines
		\item Nodes representing table summaries, link to SQL or Pandas query engines for additional data
		\item If an IndexNode is fetched during a query, the associated query engine is queried
		\end{itemize}	
		
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llamaindex18}

		{\tiny (Ref: Getting started with LlamaIndex - AI Planet)}
		\end{center}
    \end{column}
  \end{columns}
  


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Compare/Contrast Queries}

https://github.com/jerryjliu/llama\_index/blob/main/examples/composable\_indices/city\_analysis/City\_Analysis-Decompose.ipynb

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{llm33}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Multi-Step Queries}

\begin{itemize}
\item Break a complex query into multiple simpler ones! 
\item Chain-of-thought prompting over an existing data source.
\item https://github.com/jerryjliu/llama\_index/blob/main/
examples/vector\_indices/SimpleIndexDemo-multistep.ipynb
\end{itemize}	

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Compare/Contrast Queries}


\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{llm34}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Exploiting Temporal Relationships}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
		\begin{itemize}
		\item Given a question, what if we would like to retrieve additional context in the past or the future?
		\item Example question: “What did the author do after his time at Y Combinator?” 
		\item Requires looking at context in the future! 
		\end{itemize}	
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llm35}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Use Case: Recency Filtering / Outdated nodes}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
		\begin{itemize}
		\item Imagine you have three timestamped versions of the same data.
		\item If you ask a question over this data, you want to make sure it’s over the latest document.
		\end{itemize}	
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm36}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
		\begin{itemize}
		\item What is the need for evaluation?
		\item Question Generator.
		\item Evaluators.
		\end{itemize}	
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm39}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Evaluator}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
Takes in response source information and output response to evaluate the correctness of the response.

    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm40}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Query Response Evaluator}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
Takes in Query, response source information and output response to evaluate the correctness of the response.

    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm41}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Source Context Evaluation}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
Takes in Query, each source information and output response to evaluate the correctness of the response.

    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm42}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Integration into Downstream Apps}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
		\begin{itemize}
		\item Build a Streamlit app! 
		\item https://huggingface.co/spaces/llamaindex/llama\_index\_sql\_sandbox
		\end{itemize}	
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llm37}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Integration into Downstream Apps}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
		\begin{itemize}
		\item Building a unified query interface
		\item https://colab.research.google.com/drive/
		1KH8XtRiO5spa8CT7UrXN54IWdZk3DDxl?usp=sharing
		\end{itemize}	
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.6\linewidth,keepaspectratio]{llm38}

		{\tiny (Ref: LlamaIndex: A Central Interface between LLM’s + your external data)}
		\end{center}
    \end{column}
  \end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Components}

{\tiny (Ref: Discover LlamaIndex - Youtube Playlist)}

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLMs: Completion}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex31}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LLMs: Chat}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex25}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Documents/Nodes}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex32}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Evaluation}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex33}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Embeddings}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex26}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Prompts}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex27}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Pydantic Programs}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex28}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrievers: Basic}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex29}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrievers: Custom}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex30}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Advanced Concepts}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Recap: Retrieval Augmented Generation (RAG)}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex1}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Recap: Naive RAG}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex2}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Challenges with Naive RAG}


\begin{itemize}
\item Easy to Prototype, Hard to Productionize
	\begin{itemize}
	\item Tend to work well for simple questions over a simple, small set of documents
	\item But productionizing RAG over more questions and a larger set of data is hard!
	\end{itemize}	
\item Failure Modes:
	\begin{itemize}
	\item Simple Questions over Complex Data
	\item Simple Questions over Multiple Documents
	\item Complex Questions 
	\end{itemize}	
\item In the naive setting, RAG is boring
	\begin{itemize}
	\item It's just a glorified search system
	\item There's many questions/tasks that naive RAG can’t give an answer to
	\item Can we build a general context-augmented research assistant?
	\end{itemize}		
\end{itemize}	

{\tiny (Ref: Data AI Summit - Databricks 2024)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Main Focus Areas}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex3}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Improving Data Quality}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RAG is only as Good as your Data}

Garbage in $=$ Garbage Out

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex4}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{RAG is only as Good as your Data}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex5}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{General Principles}


\begin{itemize}
\item Parsing:
	\begin{itemize}
	\item Bad parsers are a key cause of garbage in == garbage out.
	\item Badly formatted text/tables confuse even the best LLMs
	\end{itemize}	
\item Chunking: 
	\begin{itemize}
	\item Try to preserve semantically similar content, say, 5 Levels of Text Splitting
	\item Strong baseline: page-level chunking
	\end{itemize}	
\item Indexing:
	\begin{itemize}
	\item Raw text often times confuses the embedding model
	\item Don’t just embed the raw text, embed references
	\item Having multiple embeddings point to the same chunk is a good practice! 
	\end{itemize}		
\end{itemize}	

{\tiny (Ref: Data AI Summit - Databricks 2024)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Case Study: Complex Documents}

\begin{columns}
    \begin{column}[T]{0.4\linewidth}
	A lot of documents can be classified as complex: 

		\begin{itemize}
		\item Embedded Tables, Charts, Images
		\item Irregular Layouts
		\item Headers/Footers
		\end{itemize}	
Naive RAG indexing pipelines fail over these documents.
		
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth,keepaspectratio]{llamaindex6}

		{\tiny (Ref: Data AI Summit - Databricks 2024)}  
		\end{center}
    \end{column}
  \end{columns}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Most PDF Parsing is Inadequate}
Extracts into a messy format that is impossible to pass down into more advanced ingestion/retrieval algorithms.


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex7}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LlamaParse}
A special Document Parser designed to let you build RAG over Complex docs
https://github.com/run-llama/llama\_parse


\begin{columns}
    \begin{column}[T]{0.4\linewidth}
Capabilities:
		\begin{itemize}
		\item Extracts tables / charts
		\item Input natural language parsing instructions
		\item JSON mode
		\item Image Extraction
		\item Support for more than 10 document types (.pdf, .pptx, .docx, .xml)

		\end{itemize}	
		
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.8\linewidth,keepaspectratio]{llamaindex8}

		{\tiny (Ref: Data AI Summit - Databricks 2024)}
		\end{center}
    \end{column}
  \end{columns}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{LlamaParse $+$ Advanced Indexing}


\begin{columns}
    \begin{column}[T]{0.4\linewidth}

		\begin{itemize}
		\item Use LlamaParse to parse a document into a semi-structured markdown representation (text + tables)
		\item Use a markdown parser to extract out text and table chunks
		\item Use an LLM to extract a summary from each table → link to underlying table chunk.
		\item Index a graph of text and table chunks.
		\end{itemize}	
		
    \end{column}
    \begin{column}[T]{0.6\linewidth}
		\begin{center}
		\includegraphics[width=0.8\linewidth,keepaspectratio]{llamaindex9}

		{\tiny (Ref: Data AI Summit - Databricks 2024)}
		\end{center}
    \end{column}
  \end{columns}
  


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Improving Query Complexity}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Complex Questions}
There's certain questions we want to ask where naive RAG will fail.

Examples:
\begin{itemize}
\item Summarization Questions: ``Give me a summary of the entire annual report''
\item Comparison Questions: ``Compare the open-source contributions of candidate A and candidate B.''
\item Structured Analytics $+$ Semantic Search: ``Tell me about the risk factors of the highest-performing ride-share company in the US''
\item General Multi-part Questions: ``Tell me about the pro-X arguments in article A, and tell me about the pro-Y arguments in article B, make a table based on our internal style guide, then generate your own conclusion based on these facts.''
\end{itemize}	
		

 		{\tiny (Ref: Data AI Summit - Databricks 2024)}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Query Planning}

\begin{itemize}
\item Break down query into parallelizable sub-queries.
\item Each can be execute-query-ed against any set of RAG pipelines
\item Example: Compare revenue of Uber and Lyft in 2021
\end{itemize}	

\begin{center}
\includegraphics[width=0.5\linewidth,keepaspectratio]{llamaindex10}

{\tiny (Ref: Data AI Summit - Databricks 2024)}
\end{center}
\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Tool Use}

% \begin{columns}
    % \begin{column}[T]{0.4\linewidth}

		% \begin{itemize}
		% \item Use an LLM to call an API
		% \item Infer the parameters of that API
		% \item In normal RAG you just pass through the query.
		% \item But what if you used the LLM to infer all the parameters for the API interface?
		% \item A key capability in many QA use cases (auto-retrieval, text-to-SQL, and more)
		% \end{itemize}	
		
    % \end{column}
    % \begin{column}[T]{0.6\linewidth}
		% \begin{center}
		% \includegraphics[width=0.8\linewidth,keepaspectratio]{llamaindex11}

		% {\tiny (Ref: Data AI Summit - Databricks 2024)}
		% \end{center}
    % \end{column}
  % \end{columns}

% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Concepts}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Work-flow}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm18}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Connectors: powered by LlamaHub}


\begin{itemize}
\item Easily ingest any kind of data, from anywhere, into unified document containers 
\item Powered by community-driven hub, rapidly growing (90+ loaders and counting!)
\item Growing support for multimodal documents (e.g. with inline images)
\end{itemize}	

$<10$ lines of code to ingest from Notion

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{llm16}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Indices + Query Interface}


\begin{itemize}
\item Source documents are stored in a data collection, In-memory, MongoDB
\item Data indices help to provide a view of your raw data, Vectors, keyword lookups, summaries
\item A retriever helps to retrieve relevant documents for your query
\item A query engine manages retrieval and synthesis given the query. 
\end{itemize}	

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{llm17}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Ingestion: LlamaHub}


\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex12}

{\tiny (Ref: Getting started with LlamaIndex - AI Planet)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Data Indexing}

\begin{itemize}
\item Documents are split ie chunking and embedding for each chunk is created
\item Chunk $+$ Embedding is capsuled as 'Node'.
\item All Nodes are stored in 'in-memory index' or Vector Db (e.g. PineCone, Weaviate, Qdrant)
\end{itemize}	

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex13}

{\tiny (Ref: Getting started with LlamaIndex - AI Planet)}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Vector Store Index}

Nodes are stored in Vector Db, where each node has chunk $+$ embedding.

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{llm19}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Retrieval}

When User asks a query:
\begin{itemize}
\item It retrieves the relevant nodes, ranks them based on the embedding similarity between query and nodes.
\item Post processes the retrieved nodes if needed.
\item Directs the processed nodes to Response Synthesis module.
\end{itemize}	



\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{llm20}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Synthesis}

Collect answers from all, then collate and then send to final answer.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm21}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Synthesis}

Collect answers one after another and then send to final answer.

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm22}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}[fragile]\frametitle{Response Synthesis}

% Given a query, get answer from Node 1, generate intermediate answer, then with at plus Node2 info, generate step by step till final answer.


% \begin{center}
% \includegraphics[width=\linewidth,keepaspectratio]{llamaindex14}

% {\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
% \end{center}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Synthesis}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm23}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Response Synthesis}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{llm24}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Multi level Queries}

\begin{center}
\includegraphics[width=0.6\linewidth,keepaspectratio]{llm25}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Multi level Queries}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm26}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Multi level Queries}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llm27}

{\tiny (Ref: LlamaIndex: A Central Interface between LLM's + your external data)}
\end{center}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{}
\begin{center}
{\Large Workflows}

{\tiny (Ref: How I Streamline My Research and Presentation with LlamaIndex Workflows - Lingzhen Chen)}
\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Introducing LlamaIndex Workflows}
  \begin{itemize}
    \item New feature in LlamaIndex for building flexible, reliable AI pipelines
    \item Allows customized steps with control flow including loops and error handling
    \item Supports cyclical execution, unlike traditional DAG-based pipelines
    \item Ideal for agentic and complex AI workflows
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Why Use Workflows for Research Tasks}
  \begin{itemize}
    \item Simplifies research-to-presentation process
    \item Automates paper discovery, summarization, and slide creation
    \item Keeps user in control of intermediate steps and outputs
    \item Enables easy customization and scaling
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Motivation for a Custom Workflow}
  \begin{itemize}
    \item Maintain an existing research process while improving efficiency
    \item Centralized end-to-end solution for summarization and slide generation
    \item Retain control over intermediate outputs for fine-tuning
    \item Future-proof with extensibility and modularity
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Workflow Example: Research to Presentation}
  \begin{itemize}
    \item User provides a research topic (e.g., “GenAI for slides”)
    \item Pulls relevant papers from arXiv.org
    \item Summarizes each paper using LLMs
    \item Extracts key insights: approach, model, dataset, evaluation, results
    \item Outputs one slide per paper with structured summaries
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Core Concepts: Steps and Events}
  \begin{itemize}
    \item \textbf{Step}: Python function for discrete tasks in the workflow
    \item Steps can share context and communicate using events
    \item \textbf{Event}: Pydantic objects that control data flow and execution
    \item Special events include StartEvent and StopEvent
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Advanced Workflow Features}
  \begin{itemize}
    \item Async and parallel execution to boost performance
    \item Nested workflows for hierarchical structure
    \item Structured LLM output ensures clean data flow
    \item Dynamic session supports isolated code execution
    \item Use of specialized agents for different steps
    \item Varying LLM models (e.g., gpt-4o, gpt-4o-mini) for optimized tasks
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Getting Started and Resources}
  \begin{itemize}
    \item Requires API keys for Tavily, Semantic Scholar, and Azure OpenAI
    \item Full implementation available on GitHub
    \item Compatible with OpenAI or other models via LlamaIndex
    \item Additional support via notebooks and video tutorials
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Main Workflow Structure}
  \begin{itemize}
    \item Consists of two nested sub-workflows: \texttt{summary\_gen} and \texttt{slide\_gen}
    \item \textbf{summary\_gen}: Searches for research papers and uses LLMs to generate topic-specific summaries
    \item \textbf{slide\_gen}: Converts summaries into PowerPoint slides using a template and \texttt{python-pptx}
    \item Executes and formats slides via dynamic Python code generation
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Main workflow}

\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{llamaindex22}

{\tiny (Ref: How I Streamline My Research and Presentation with LlamaIndex Workflows - Lingzhen Chen)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Summary Generation Sub-workflow}
  \begin{itemize}
    \item \texttt{summary\_gen} is a linear, data-processing workflow
    \item Executes a series of steps to find and summarize research papers
    \item Some steps interact with an LLM to extract insights
    \item Designed for simplicity and clarity in processing academic content
    \item Starts with user-provided research topic
    \item \texttt{tavily\_query}: Uses Tavily API to fetch relevant academic papers
    \item \texttt{get\_paper\_with\_citations}: Retrieves metadata for each paper and its citations via SemanticScholar API
    \item \texttt{filter\_papers}: Uses LLM to evaluate and filter papers based on relevance
    \item \texttt{filter\_papers} step runs in parallel with \texttt{@step(num\_workers=4)} for efficiency	
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Summary Generation Sub-workflow}

\begin{center}
\includegraphics[width=0.7\linewidth,keepaspectratio]{llamaindex23}

{\tiny (Ref: How I Streamline My Research and Presentation with LlamaIndex Workflows - Lingzhen Chen)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Additional Steps in Summary Generation}
  \begin{itemize}
    \item \texttt{download\_papers}: Prioritizes and downloads the most relevant papers based on ArXiv availability and relevance score
    \item \texttt{paper2summary\_dispatcher}: Prepares papers for summarization, enabling parallel execution of \texttt{paper2summary}
    \item \texttt{paper2summary}: Converts PDFs to images, sends them to LLM for detailed summaries, saves in markdown for future reference
    \item \texttt{finish}: Verifies summary storage, logs completion, and triggers the next sub-workflow \texttt{slide\_gen}
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{The Slide Generation Sub-workflow}

\begin{center}
\includegraphics[width=0.8\linewidth,keepaspectratio]{llamaindex24}

{\tiny (Ref: How I Streamline My Research and Presentation with LlamaIndex Workflows - Lingzhen Chen)}
\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Slide Generation Sub-workflow}
  \begin{itemize}
    \item Starts after summary markdown files are ready from \texttt{summary\_gen}
    \item \texttt{get\_summaries}: Reads summary files and triggers concurrent execution via \texttt{SummaryEvent}
    \item \texttt{summary2outline}: Converts summaries into concise slide outlines using LLM
    \item \texttt{gather\_feedback\_outline}: Presents outline to user for feedback; loop continues until approval
    \item \texttt{outlines\_with\_layout}: Enhances slide outlines with layout details from PowerPoint template
    \item \texttt{slide\_gen}: Uses ReAct agent with \texttt{python-pptx} to generate slides
    \item \texttt{validate\_slides}: LLM visually inspects slides for content and style, sends \texttt{SlideValidationEvent} if errors are found
    \item \texttt{modify\_slides}: If validation fails, ReAct agent updates slides based on feedback and re-validates
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]\frametitle{Final Thoughts and Observations}
  \begin{itemize}
    \item \textbf{GPT-4o vs GPT-4o-mini}: GPT-4o-mini struggles with complex tasks but performs well in simpler ones (e.g., summarizing)
    \item \textbf{Intermediate files}: Useful for easing agent workload and ensuring content and style consistency
    \item \textbf{Handling edge cases}: Iterative prompts address style validation issues; human-in-the-loop could improve accuracy
    \item \textbf{python-pptx limitations}: Consider alternatives like VBA for more efficient slide generation
    \item \textbf{Agents and tools}: Agents with tool access can offer flexibility and adaptability for future changes
    \item \textbf{Enhance human interaction}: More user involvement in validation and refinement would improve results
    \item \textbf{Personalized query engines}: Allow users to query papers and modify summaries for tailored results
    \item \textbf{LlamaIndex flexibility}: A powerful tool for building customizable AI solutions with both control and flexibility
  \end{itemize}
\end{frame}

