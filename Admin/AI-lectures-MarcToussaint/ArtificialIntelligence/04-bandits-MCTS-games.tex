\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Bandits, MCTS, \& Games}
\renewcommand{\keywords}{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

The first lecture was about tree search (a form of sequential decision
making), the second about probabilities. If we combine this we get
Monte-Carlo Tree Search (MCTS), which is the focus of this lecture.

But before discussing MCTS we introduce an important conceptual
problem: Multi-armed bandits. This problem setting is THE prototype for
so-called exploration-exploitation problems. More precisely, for
problems where sequential decisions influence both, the state of
knowledge of the agent as well as the states/rewards the agent
gets. Therefore there is some tradeoff between choosing decisions for
the sake of learning (influencing the state of knowledge in a positive
way) versus for the sake of rewards---while clearly, learning might
also enable you to better collect rewards later. Bandits are a kind of
minimalistic problem setting of this kind, and the methods and
algorithms developed for Bandits translate to other
exploration-exploitation kind of problems within Reinforcement Learning,
Machine Learning, and optimization.

Interestingly, our first application of Bandit ideas and methods is
tree search: Performing tree search is also a sequential decision
problem, and initially the 'agent' (=tree search algorithm) has a lack
of knowledge of where the optimum in the tree is. This sequential
decision problem under uncertain knowledge is also an
exploitation-exploration problem. Applying the Bandit methods we get
state-of-the-art MCTS methods, which nowadays can solve problems like
computer Go.

We first introduce bandits, the MCTS, and mention MCTS for POMDPs. We
then introduce 2-player games and how to apply MCTS in this case.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Bandits}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Multi-armed Bandits}
\slide{Multi-armed Bandits}{

~

\show[.5]{bandits}

\item There are $n$ machines

\item Each machine $i$ returns a reward $y\sim P(y;\t_i)$

The machine's parameter $\t_i$ is unknown

\item Your goal is to maximize the reward, say, collected over the
  first $T$ trials


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Bandits -- applications}{

~

\item Online advertisement\anchor{30,-20}{\showh[.2]{google}}

~

~

\item Clinical trials, robotic scientist
\anchor{30,-90}{\showh[.3]{robotScientist}}

~

~

\item Efficient optimization

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Bandits}{

\item The bandit problem is an archetype for
\begin{items}
\item Sequential decision making

\item Decisions that influence knowledge as well as rewards/states

\item Exploration/exploitation
\end{items}

\item The same aspects are inherent also in global optimization,
active learning \& RL

~

\item The Bandit problem formulation is the basis of UCB -- which is
the core of serveral planning and decision making methods

\item Bandit problems are commercially very relevant

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Upper Confidence Bounds (UCB)}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Bandits: Formal Problem Definition}{

\item Let $a_t\in\{1,..,n\}$ be the choice of machine at time $t$

Let $y_t\in\RRR$ be the outcome

~

\item A policy or strategy maps all the history to a new choice:
$$ \pi:~ [ (a_1,y_1), (a_2,y_2), ..., (a_{t\1},y_{t\1}) ] \mapsto a_t$$

~

\item Problem: ~ Find a policy $\pi$ that
$$\max \< \Sum_{t=1}^T y_t \>$$
or
$$\max \< y_T \>$$

~

{\small or other objectives like discounted infinite horizon $\max \< \Sum_{t=1}^\infty \g^t y_t \>$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Exploration, Exploitation}
\slide{Exploration, Exploitation}{

\item ``Two effects'' of choosing a machine:
\begin{items}
\item You collect more data about the machine $\to$ knowledge
\item You collect reward
\end{items}

~

\item For example
\begin{items}
\item Exploration: ~ Choose the next action $a_t$ to
$\min \< H(b_t) \>$

\item Exploitation: ~ Choose the next action $a_t$ to
$\max \< y_t \>$
\end{items}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Upper Confidence Bound (UCB1)}
\slide{Upper Confidence Bound (UCB1)}{

\begin{algo}
\State Initialization: Play each machine once
\Repeat
\State Play the machine $i$ that maximizes $\hat y_i
+ \b \sqrt{\frac{2\ln n}{n_i}}$
\Until
\end{algo}

~

\item $\hat y_i$ is the average reward of machine $i$ so far

\item $n_i$ is how often machine $i$ has been played so far

\item $n = \Sum_i n_i$ is the number of rounds so far

\item $\b$ is often chosen as $\b=1$

~

\item The bound is derived from the Hoeffding inequality

~

\tiny

See \emph{Finite-time analysis of the multiarmed bandit problem},
Auer, Cesa-Bianchi \& Fischer, Machine learning, 2002.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{UCB algorithms}{

\item UCB algorithms determine a \textbf{confidence interval} such that 
$$\hat y_i - \s_i < \<y_i\> < \hat y_i + \s_i$$
with high probability.

UCB chooses the upper bound of this confidence interval

~

\item \emph{Optimism in the face of uncertainty}

~

\item Strong bounds on the regret (sub-optimality) of UCB1 (e.g.\ Auer
et al.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{UCB for Bernoulli**}{

\item If we have a single Bernoulli bandits, we can count
$$a=1+\text{\#wins} \comma b=1+\text{\#losses}$$

\item Our posterior over the Bernoulli parameter $\m$ is
$\Beta(\m\|a,b)$

\item The mean is $\<\m\>=\frac{a}{a+b}$

The mode (most likely) is $\m^*=\frac{a-1}{a+b-2}$ for $a,b>1$

The variance is $\Var{\m} = \frac{ab}{(a+b+1)(a+b)^2}$

One can numerically compute the \emph{inverse cumulative Beta
distribution} $\to$ get exact quantiles

~

\item Alternative strategies:
$$ \argmax_i \text{90\%-quantile}(\m_i) $$


$$ \argmax_i \<\m_i\> + \b \sqrt{\Var{\m_i}} $$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{UCB for Gauss**}{

\item If we have a single Gaussian bandits, we can compute

the mean estimator $\hat \m= \frac{1}{n}\sum_i y_i$

the empirical variance $\hat \s^2 = \frac{1}{n-1} \sum_i (y_i-\hat\m)^2$

and the estimated variance \emph{of the mean estimator} $\Var{\hat \m} = \hat \s^2/n$

~

\item $\hat\m$ and $\Var{\hat\m}$ describe our posterior Gaussian belief over the
true underlying $\m$

Using the err-function we can get exact quantiles

~

\item Alternative strategies:
$$ \text{90\%-quantile}(\m_i) $$

$$ \hat \m_i + \b \sqrt{\Var{\m_i}}  = \hat \m_i + \b \hat\s/\sqrt{n}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{UCB - Discussion}{

\item UCB over-estimates the reward-to-go (under-estimates cost-to-go),
just like $A^*$ -- but does so in the probabilistic setting of bandits

~

\item The fact that regret bounds exist is great!

~

\item UCB became a core method for algorithms (including planners) to
decide what to explore:

~

\emph{In tree search, the decision of which branches/actions to
explore (which node to expand) is itself a decision problem. An
``intelligent agent'' (like UBC) can be used within the planner to
make decisions about how to grow the tree.}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Monte Carlo Tree Search}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Monte Carlo Tree Search (MCTS)}
\slide{Monte Carlo Tree Search (MCTS)}{

\item MCTS is very successful on Computer Go and other games
\item MCTS is rather simple to implement
\item MCTS is very general: applicable on any discrete domain

~

\item Key paper:

Kocsis \& Szepesv{\'a}ri: \emph{Bandit based Monte-Carlo
Planning}, ECML 2006.

\item Survey paper:

Browne et al.: \emph{A Survey of Monte Carlo Tree Search Methods}, 2012.

\item Tutorial presentation:
\tiny \url{http://web.engr.oregonstate.edu/~afern/icaps10-MCP-tutorial.ppt}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Monte Carlo methods}{

\item General, the term \emph{Monte Carlo simulation} refers to methods
that generate many i.i.d.\ random samples $x_i\sim P(x)$ from a
distribution $P(x)$. Using the samples one can estimate expectations
of anything that depends on $x$, e.g.\ $f(x)$:
$$\<f\> ~=~ \int_x P(x)~ f(x)~ dx ~\approx~ \frac{1}{N} \sum_{i=1}^N
f(x_i)$$

(In this view, Monte Carlo approximates an integral.)

~

\item Example: What is the probability that a solitair would come out
successful? (Original story by Stan Ulam.) Instead of trying to
analytically compute this, generate many random solitairs and count.

~

\item The method developed in the 40ies, where computers became
faster. Fermi, Ulam and von Neumann initiated the idea. von Neumann
called it ``Monte Carlo'' as a code name.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Flat Monte Carlo}{

\item The goal of MCTS is to estimate the utility (e.g., expected payoff
$\D$) depending on the action $a$ chosen---the \textbf{Q-function}:
\begin{align*}
Q(s_0,a)
&= \Exp{\D | s_0, a}
\end{align*}
where expectation is taken with w.r.t.\ the whole future randomized
actions (including a potential opponent)

~

\item \emph{Flat Monte Carlo} does so by rolling out many random
simulations (using a \textsc{RolloutPolicy}) without growing a tree

The key difference/advantage of MCTS over flat MC is that the tree
growth focusses computational effort on promising actions

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Generic MCTS scheme}{

\show{MCTS}
{\hfill\tiny from Browne et al.}

\small
\begin{algo}
\State start tree $V=\{v_0\}$
\While{within computational budget}
\State $v_l \gets \textsc{TreePolicy}(V)$ chooses and creates a new leaf of $V$
\State append $v_l$ to $V$
\State $\D \gets \textsc{RolloutPolicy}(V)$ rolls out a full simulation, with return $\D$
\State $\textsc{Backup}(v_l,\D)$ updates the values of all parents of
$v_l$
\EndWhile 
\State return best child of $v_0$
\end{algo}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Generic MCTS scheme}{

\item Like FlatMC, MCTS typically computes full
roll outs to a terminal state. A heuristic (evaluation function) to
estimate the utility of a state is not needed, but can be
incorporated.

\item The tree grows unbalanced

\item The \textsc{TreePolicy} decides where the tree is expanded -- and
needs to trade off exploration vs.\ exploitation

\item The \textsc{RolloutPolicy} is necessary to simulate a roll out. It
typically is a random policy; at least a randomized policy.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Upper Confidence Tree (UCT)}
\slide{Upper Confidence Tree (UCT)}{%\label{lastpage}

\item UCT uses UCB to realize the {\sc
TreePolicy}, i.e.\ to decide where to expand the tree

~

\item \textsc{Backup} updates all parents of $v_l$ as

$n(v) \gets n(v)+1$ ~ (count how often has it been played)

$Q(v) \gets Q(v) + \D$ ~ (sum of rewards received)

~

\item \textsc{TreePolicy} chooses child nodes based on UCB:
$$\argmax_{v' \in \del(v)} \frac{Q(v')}{n(v')} + \b \sqrt{\frac{2\ln n(v)}{n(v')}}$$
or choose $v'$ if $n(v')=0$

%~

%% \item In games use a ``negamax'' backup: While iterating upward, flip
%% sign $\D \gets -\D$ in each iteration


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{MCTS for POMDPs}
\sublecture{MCTS applied to POMDPs**}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Recall POMDPs}{

\show[.5]{pomdp_agent}

\begin{items}
\item initial state distribution $P(s_0)$
\item transition probabilities $P(s' | s,a)$
\item observation probabilities $P(y' | s',a)$
\item reward probabilities $P(r | s,a)$
\end{items}

~

\item An optimal agent maps the history to an action, $(y_{0:t},a_{0:t\1}) \mapsto a_t$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Issues when applying MCTS ideas to POMDPs}{

\item key paper:

Silver \& Veness: \emph{Monte-Carlo Planning in Large POMDPs}, NIPS
2010

~

\item MCTS is based on generating rollouts using a simulator

-- Rollouts need to start at a specific \emph{state} $s_t$

$\to$ Nodes in our tree need to have states associated, to start
rollouts from

~

\item At any point in time, the agent has only the history $h_t = (y_{0:t},a_{0:t\1})$
to decide on an action

-- The agent wants to estimate the Q-funcion $Q(h_t,a_t)$

$\to$ Nodes in our tree need to have a history associated

~

$\to$ Nodes in the search tree will
\begin{items}
\item maintain $n(v)$ and $Q(v)$ as before
\item have a history $h(v)$ attached
\item have a \emph{set} of states $\SS(v)$ attached
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MCTS applied to POMDPs}{

\qquad\showh[.6]{POMCP}
\anchor{10,50}{\tiny from Silver \& Veness}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MCTS applied to POMDPs}{

\item For each rollout:
\begin{items}
\item Choose a \emph{random} world state $s_0 \sim \SS(v_0)$ from the
set of states associated to the root $v_0$; initialize the simulator
with this $s_0$
\item Use a \textsc{TreePolicy} to traverse the current tree; during
this, update the state sets $\SS(v)$ to contain the world state
simulated by the simulator
\item Use a \textsc{RolloutPolicy} to simulate a full rollout
\item Append a new leaf $v_l$ with novel history $h(v_l)$ and a single
state $\SS(v_l)$ associated
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Monte Carlo Tree Search}{

\item MCTS combines forward information (starting simulations from $s_0$) with backward
information (accumulating $Q(v)$ at tree nodes)

~

\item UCT uses an optimistic estimate of return to decide
on how to expand the tree -- this is the stochastic analogy to the
$A^*$ heuristic

~

{\tiny

\cen{
\begin{tabular}{|c|c|c||c|c|c||c||c|}
\hline
table & PDDL & NID & MDP & POMDP & DEC-POMDP & Games & control \\
\hline
y & y & y & y & y & ? & y & \\
\hline
\end{tabular}
}

}

~

\item \emph{Conclusion:} MCTS is a very generic and often powerful
planning method. For many many samples it converges to correct
estimates of the $Q$-function. However, the $Q$-function can be
estimated also using other methods.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Game Playing}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Outline}{

\item Minimax

\item $\alpha$--$\beta$ pruning

\item Evaluation functions

\item UCT for games

%% \item Games of chance

%% \item Games of imperfect information


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Games vs.~search problems}{

%% ``Unpredictable'' opponent $\Rightarrow$ solution is a \emph{strategy}\\
%% specifying a move for every possible opponent reply

%% Time limits $\Rightarrow$ unlikely to find goal, must approximate

%% Plan of attack:
%% \begin{itemize}
%% \item Computer considers possible lines of play (Babbage, 1846)
%% \item Algorithm for perfect play (Zermelo, 1912; Von Neumann, 1944)
%% \item Finite horizon, approximate evaluation (Zuse, 1945; Wiener, 1948; \\
%%       Shannon, 1950)
%% \item First chess program (Turing, 1951)
%% \item Machine learning to improve evaluation accuracy (Samuel, 1952--57)
%% \item Pruning to allow deeper search (McCarthy, 1956)
%% \end{itemize}


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Types of games}{

%% \vspace*{0.3in}

%% \show[1.05]
%% \show{russell/game-types.pdf}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Game tree (2-player, deterministic, turns)}{


\show[0.9]{russell/tictactoe.pdf}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Minimax}
\slide{Minimax}{

\item Perfect play for deterministic, perfect-information games

\item Choose move to position with highest \defn{minimax value}\\
= best achievable payoff against best play

~

\show[0.8]{russell/minimax.pdf}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Minimax algorithm}{

\item Computation by direct recursive function calls, which effectively does DFS

~

%\input{russell/minimax-algorithm}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Properties of minimax}{

\emph{Complete} \pause Yes, if tree is finite (chess has specific rules for this)

\emph{Optimal} \pause Yes, against an optimal opponent. Otherwise??

\emph{Time complexity} \pause $O(b^m)$

\emph{Space complexity} \pause $O(bm)$ (depth-first exploration)

For chess, $b\approx 35$, $m \approx 100$ for ``reasonable'' games\\
$\Rightarrow$ exact solution completely infeasible

But do we need to explore every path?

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{$\alpha$--$\beta$ pruning}{

\show[0.4]{russell/alpha-beta-general.pdf}

\item {$\alpha$} is the best value (to {\sc max}) found so far off the current path

\item If {$V$} is worse than {$\alpha$}, {\sc max} will avoid it
$\Rightarrow$ prune that branch

\item Define {$\beta$} similarly for {\sc min}

\item This is an instance of branch-and-bound

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Alpha-Beta Pruning}
\slide{$\alpha$--$\beta$ pruning example}{

~

\only<+>{ \showh[0.9]{russell/alpha-beta-progress1c}}
\only<+>{ \showh[0.9]{russell/alpha-beta-progress2c}}
\only<+>{ \showh[0.9]{russell/alpha-beta-progress3c}}
\only<+>{ \showh[0.9]{russell/alpha-beta-progress4c}}
\only<+>{ \showh[0.9]{russell/alpha-beta-progress5c}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The $\alpha$--$\beta$ algorithm}{

%\input{russell/alpha-beta-algorithm}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Properties of $\alpha$--$\beta$}{

\item Pruning \emph{does not} affect final result

\item Good move ordering improves effectiveness of pruning!

%% With ``perfect ordering,'' time complexity = {$O(b^{m/2})$}\\
%% $\Rightarrow$ \emph{doubles} solvable depth

\item A simple example of the value of reasoning about which 
computations are relevant (a form of \emph{metareasoning})

%Unfortunately, {$35^{50}$} is still impossible!



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Resource limits}{

Standard approach:
\begin{itemize}
\item Use {\sc Cutoff-Test} instead of {\sc Terminal-Test}\\
e.g., depth limit %(perhaps add \defn{quiescence search})
\item Use {\sc Eval} instead of {\sc Utility}\\
i.e., \defn{evaluation function} that estimates desirability of position
\end{itemize}

Suppose we have $100$ seconds, explore $10^4$ nodes/second\\
$\Rightarrow$ $10^6$ nodes per move $\approx$ $35^{8/2}$\\
$\Rightarrow$ $\alpha$--$\beta$ reaches depth 8 $\Rightarrow$ pretty good chess program


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Evaluation functions}
\slide{Evaluation functions}{

~

\show[0.95]{russell/chess-evaluation-bc.pdf}

For chess, typically \emph{linear} weighted sum of \defn{features}
{$$
\text{\sc Eval}(s) = w_1 f_1(s) + w_2 f_2(s) + \ldots + w_n f_n(s)
$$}
e.g., $w_1 = 9$ with \\
$f_1(s)$ = (number of white queens) -- (number of black queens),\ \ etc.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{UCT for games}
\slide{Upper Confidence Tree (UCT) for games}{

\item Standard backup updates all parents of $v_l$ as

$n(v) \gets n(v)+1$ ~ (count how often has it been played)

$Q(v) \gets Q(v) + \Delta$ ~ (sum of rewards received)

~

\item In games use a ``negamax'' backup: While iterating upward, flip
sign $\Delta \gets -\Delta$ in each iteration

~

\item Survey of MCTS applications:

Browne et al.: A Survey of Monte Carlo Tree Search Methods, 2012.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\hspace*{-10mm}\showh[.5]{browne1}
\showh[.5]{browne2}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Brief notes on game theory}{

\item Zero-sum games can be represented by a payoff matrix

\item $U_{ji}$ denotes the utility of player 1 if she chooses the \emph{pure}
(=deterministic) strategy $i$ and player 2 chooses the pure strategy
$j$.\\
Zero-sum games: $U_{ji} = - U_{ij} ~,\quad U^T = -U$

\item Fining a minimax optimal \emph{mixed strategy} $p$ is a Linear Program
$$\max_w w \text{\quad s.t.\quad} U p \ge w~,\quad \sum_i p_i = 1~,\quad p\ge 0$$
Note that $U p \ge w$ implies $\min_j (Up)_j \ge w$.

\item Gainable payoff of player 1: $\max_p \min_q q^T U p$\\
\emph{Minimax-Theorem}: $\max_p \min_q q^T U p = \min_q \max_p q^T U p$\\
Minimax-Theorem $\leftrightarrow$ optimal $p$ with $w\ge 0$ exists

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Beyond bandits**}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Perhaps have a look at the tutorial: \emph{Bandits, Global
Optimization, Active Learning, and Bayesian RL -- understanding the
common ground}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Global Optimization}
\slide{Global Optimization}{

\item Let $x\in\RRR^n$, $f:~ \RRR^n \to \RRR$, ~ find
\begin{align*}
\min_x~ & f(x)
\end{align*}

{\tiny

(I neglect constraints $g(x)\le 0$ and $h(x)=0$ here -- but could be
included.)
}

~

\item Blackbox optimization: find optimium by sampling values
$y_t = f(x_t)$

No access to $\na f$ or $\he f$

Observations may be noisy $y \sim \NN(y \| f(x_t),\s)$

%% ~

%% \item Example finite horizon problem definition:
%% $$\<\min f(x_T)\>$$

%% \item 
%% (AFAIK, this research started with (gold) mining.) [[TODO: kriging]]

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Global Optimization ~{\protect$=$}~ infinite bandits}{

\item In global optimization $f(x)$ defines a reward for every
$x\in\RRR^n$

-- Instead of a finite number of actions $a_t$ we now have $x_t$

~

\item The unknown ``world property'' is the function $\t=f$

\item Optimal Optimization could be defined as: ~ find $\pi:~
h_t \mapsto x_t$ that
$$\min \< \Sum_{t=1}^T f(x_t) \>$$
or
$$\min \< f(x_T) \>$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Processes as belief}{

\item If all the infinite bandits would be uncorrelated, there would
be no chance to solve the problem $\to$ \emph{No Free Lunch Theorem}

\item One typically assumes that nearby function values $f(x), f(x')$
are correlated as described by a covariance function $k(x,x')$ $\to$
Gaussian Processes


%% \item Given a Gaussian Process prior $GP(f|\mu,C)$ over $f$ and a history
%% $$ D_t ~=~ [ (x_1,y_1), (x_2,y_2), ..., (x_{t\1},y_{t\1}) ]$$
%% the belief is
%% \begin{align*}
%% b_t(f)
%%  &= P(f\|D_t) = \text{GP}(f|D_t,\mu,C) \\
%% \hspace*{-10mm}\text{Mean}(f(x))
%%  &= \hat f(x) = \vec\k(x) (\vec K + \s^2 \Id)^\1 \vec y
%% &&\text{\emph{response surface}}\\
%% \hspace*{-10mm}\text{Var}(f(x))
%%  &= \hat \s(x) = k(x,x)
%%   - \vec\k(x) (\vec K + \s^2 \Id_n)^\1 \vec\k(x)
%% &&\text{\emph{confidence interval}}
%% \end{align*}

%% ~

%% \small

%% \item Side notes:
%% \begin{items}
%% \item Don't forget that
%% $\text{Var}(y^*|x^*,D) = \s^2 + \text{Var}(f(x^*)|D)$

%% %% \item Gaussian Processes ~=~ Bayesian Kernel Ridge Regression

%% %% \item GP classification ~=~ Bayesian Kernel Logistic Regression

%% \item We can also handle discrete-valued functions $f$ using GP
%%   classification
%% \end{items}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Optimal optimization via belief planning}{

%% \item As for bandits it holds
%% \begin{align*}
%% V_{t\1}(b_{t\1})
%%  &= \max_\pi \< \Sum_{t=t}^T y_t \> \\
%%  &= \max_{x_t} \Int_{y_t} P(y_t|x_t,b_{t\1})~ \[y_t + V_t(b_{t\1}[x_t,y_t])\]
%% \end{align*}

%% $V_{t\1}(b_{t\1})$ is a function over the GP-belief!

%% If we could compute $V_{t\1}(b_{t\1})$ we ``optimally optimize''

%% ~

%% \item I don't know of a minimalistic case where this might be feasible

%% %% Approximately: discretize $x$ ($\to$ finite but dependent bandits),
%% %% small $T$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{GP-UCB}
\slide{Greedy 1-step heuristics}{

\show[.45]{jones01}

\item Maximize Probability of Improvement ~ (MPI)
\anchor{30,20}{\tiny from Jones (2001)}
$$x_t = \argmax_x \Int_{-\infty}^{y^*} \NN(y|\hat f(x),\hat\s(x))$$

\item Maximize Expected Improvement ~ (EI)
$$x_t = \argmax_x \Int_{-\infty}^{y^*} \NN(y|\hat f(x),\hat\s(x))~ (y^*-y)$$

\item Maximize UCB
$$x_t = \argmin_x \hat f(x) - \b_t \hat\s(x)$$

\tiny

(Often, $\b_t=1$ is chosen. UCB theory allows for
better choices. See Srinivas et al.\ citation below.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

From Srinivas et al., 2012:

~

%\show[1]{GP-UCB1}
\show[1]{GP-UCB2}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{
\show[1]{GP-UCB3}

~

\show[1]{GP-UCB4}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Further reading}{

\item Classically, such methods are known as \emph{Kriging}

~

\item \emph{Information-theoretic regret bounds for gaussian process
optimization in the bandit setting}
Srinivas, Krause, Kakade \& Seeger, Information Theory, 2012.

~

\item \emph{Efficient global optimization of expensive black-box functions.} Jones, Schonlau, \& Welch, Journal of Global Optimization, 1998.

\item \emph{A taxonomy of global optimization
methods based on response surfaces} Jones, Journal of Global
Optimization, 2001.

\item \emph{Explicit local models: Towards optimal optimization
algorithms}, Poland, Technical Report No. IDSIA-09-04, 2004.

%\show{GP-UCB}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Active Learning}
\sublecture{Active Learning**}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Example}{

%% \emph{Active learning with gaussian processes for object
%% categorization.} Kapoor, Grauman, Urtasun \& Darrell, ICCV 2007.

%% ~

%% \show[.5]{activeLearningClassifier}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Active Learning}{

\item In standard ML, a data set $D_t=\{ (x_s,y_s) \}_{s=1}^{t\1}$ is given.

In active learning, the learning agent sequentially decides on each
$x_t$ -- where to collect data

\item Generally, the aim of the learner should be to learn as fast as
  possible, e.g.\ minimize predictive error

~

\item Again, the unknown ``world property'' is the function $\t=f$

\item Finite horizon $T$ predictive error problem:

Given $P(x^*)$, find a policy $\pi:~ D_t \mapsto x_t$ that
$$\min \< -\log P(y^* | x^*, D_T) \>_{y^*,x^*,D_T;\pi}$$

This also can be expressed as \emph{predictive entropy}:
\begin{align*}
\< -\log P(y^* | x^*, D_T)\>_{y^*,x^*}
&= \< - \Int_{y^*} P(y^* | x^*, D_T)~ \log P(y^* | x^*, D_T) \>_{x^*}\\
&= \< H(y^* | x^*, D_T) \>_{x^*} =: H(f|D_T)
\end{align*}

\item Find a policy that $\min \Exp{D_T;\pi}{ H(f|D_T) }$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Gaussian Processes as belief}{

%% \item Again, the unknown ``world property'' is the function $\t=f$

%% \item We can use a Gaussian Process to represent the belief
%% $$b_t(f) = P(f\|D_t) = \text{GP}(f|D_t,\mu,C)$$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Optimal Active Learning via belief planning}{

%% \item The only difference to global optimization is the reward.

%% In active learning it is the predictive entropy: $- H(f|D_T)$

%% ~

%% \item Dynamic Programming:
%% \begin{align*}
%% V_T(b_T)
%%  &= - H(b_T) \comma H(b):= \<H(y^*|x^*,b)\>_{x^*}\\
%% V_{t\1}(b_{t\1})
%%  &= \max_{x_t} \Int_{y_t} P(y_t|x_t,b_{t\1})~ V_t(b_{t\1}[x_t,y_t])
%% \end{align*}

%% ~

%% \item Computationally intractable

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Greedy 1-step heuristic}{

\item The simplest greedy policy is 1-step Dynamic Programming:

Directly maximize immediate expected reward, i.e., minimizes $H(b_{t\po})$.

$$\pi:~  b_t(f) \mapsto \argmin_{x_t} \Int_{y_t} P(y_t|x_t,b_t)~ H(b_t[x_t,y_t])$$

~

\item For GPs, you reduce the entropy most if you choose
  $x_t$ where the current predictive variance is highest:
$$\text{Var}(f(x)) = k(x,x) - \vec\k(x) (\vec K + \s^2 \Id_n)^\1 \vec\k(x)$$
This is referred to as \emph{uncertainty sampling}

\item Note,  if we fix hyperparameters:
\begin{items}
\item This variance is \emph{independent} of the 
  observations $y_t$, only the set $D_t$ matters!

\item The order of data points also does not matter

\item You can pre-optimize a set of ``grid-points'' for the kernel --
and play them in any order
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Greedy Active Learning with hyperparameters}{

%% \cen{\showhs[1]{activeModelSelection}\qquad
%% \showhs[1]{activeModelSelection2}}

%% \item Change the reward function: Minimize expected entropy over
%% $\a$:
%% $$\argmin_x \Int_y P(y|x,D)~ H[p(\a|D,x,y)]$$

%% \item This can be rewritten in many ways (adding a constant $H[p(\a)]$)
%% {\tiny
%% \begin{align}
%% &- \Int_y p(y|x)~ H[p(\a|y,x)] - H[p(\a)] \\
%% %&= \Int_{y,\a} p(y|x)~ p(\a|y,x)~ \log p(\a|y,x) - H[p(\a)] \\
%% &= \Int_{y,\a} p(y,\a|x)~ \log p(\a|y,x) - H[p(\a)] \label{eq1}\\
%% %&= \Int_{y,\a} p(y|x)~ p(\a|y,x)~ \log\frac{p(\a|y,x)}{p(\a)} \\
%% &= \Int_y p(y|x)~ \kld{p(\a|y,x)}{p(\a)} \label{eq2}~,
%% \end{align}}
%% \item Eq.~(\ref{eq1}) maximizes the gain in \emph{Shannon Information}
%% %$\Int_{y,\a} p(y,\a|x)~ \log p(\a|y,x)$

%% \item Eq.~(\ref{eq2}) maximizes the expected KLD

%% {\tiny (see Chaloner et al.)}

%% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Further reading}{\label{lastpage}

\item \emph{Active learning literature survey.} Settles, Computer Sciences Technical Report 1648, University of Wisconsin-Madison, 2009.

\item \emph{Bayesian experimental design: A review.} Chaloner \& Verdinelli, Statistical Science, 1995.

\item \emph{Active learning with statistical models.} Cohn, Ghahramani
\& Jordan, JAIR 1996.

\item ICML 2009 Tutorial on \emph{Active Learning},
Sanjoy Dasgupta and John Langford
{\small\url{http://hunch.net/~active_learning/}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Digression: Exact values don't matter}{

%% \vspace*{0.3in}

%% \show[1.05]
%% \show{russell/ordinal-utility.pdf}

%% Behaviour is preserved under any \emph{monotonic} transformation of
%% {\sc Eval}

%% Only the order matters:\\
%% payoff in deterministic games acts as an \defn{ordinal utility} function

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Deterministic games in practice}{

%% Checkers: Chinook ended 40-year-reign of human world champion Marion
%% Tinsley in 1994. Used an endgame database defining perfect play for
%% all positions involving 8 or fewer pieces on the board, a total of
%% 443,748,401,247 positions.

%% Chess: Deep Blue defeated human world champion Gary Kasparov
%% in a six-game match in 1997. Deep Blue searches 200 million positions
%% per second, uses very sophisticated evaluation, and undisclosed methods for
%% extending some lines of search up to 40 ply.

%% Othello: human champions refuse to compete against computers, who are
%% too good.

%% Go: human champions refuse to compete against computers, who are too
%% bad. In go, $b > 300$, so most programs use pattern knowledge bases to
%% suggest plausible moves.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Nondeterministic games: backgammon}{

%% \vspace*{0.3in}

%% \show[0.65]
%% \show{russell/backgammon-position.pdf}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Nondeterministic games in general}{

%% In nondeterministic games, chance introduced by dice, card-shuffling

%% Simplified example with coin-flipping:

%% \vspace*{0.3in}

%% \show[0.65]
%% \show{russell/expectiminimax-simple.pdf}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Algorithm for nondeterministic games}{

%% {\sc Expectiminimax} gives perfect play

%% Just like {\sc Minimax}, except we must also handle chance nodes:

%% $\ldots$\\
%% \keyword{if} \var{state} is a {\sc Max} node \keyword{then}\\
%%    \keyword{return} the highest {\sc ExpectiMinimax-Value} of {\sc Successors}(\var{state})\\
%% \keyword{if} \var{state} is a {\sc Min} node \keyword{then}\\
%%    \keyword{return} the lowest {\sc ExpectiMinimax-Value} of {\sc Successors}(\var{state})\\
%% \keyword{if} \var{state} is a chance node \keyword{then}\\
%%    \keyword{return} average of {\sc ExpectiMinimax-Value} of {\sc Successors}(\var{state})\\
%% $\ldots$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Nondeterministic games in practice}{

%% Dice rolls increase $b$: 21 possible rolls with 2 dice\\
%% Backgammon $\approx$ 20 legal moves (can be 6,000 with 1-1 roll)
%% $$
%% {\rm depth}\ 4 = 20 \times (21 \times 20)^3 \approx 1.2\times 10^9
%% $$

%% As depth increases, probability of reaching a given node shrinks\\
%% $\Rightarrow$ value of lookahead is diminished

%% $\alpha$--$\beta$ pruning is much less effective

%% {\sc TDGammon} uses depth-2 search + very good {\sc Eval}\\
%% $\approx$ world-champion level

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Digression: Exact values DO matter}{

%% \vspace*{0.3in}

%% \show[1.05]
%% \show{russell/chance-evaluation.pdf}


%% Behaviour is preserved only by \emph{positive linear} transformation of
%% {\sc Eval}

%% Hence {\sc Eval} should be proportional to the expected payoff

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Games of imperfect information}{

%% E.g., card games, where opponent's initial cards are unknown

%% Typically we can calculate a probability for each possible deal

%% Seems just like having one big dice roll at the beginning of the game$^*$

%% \emph{Idea}: compute the minimax value of each action in each deal,\\
%%    then choose the action with highest expected value over all deals$^*$

%% Special case: if an action is optimal for all deals, it's optimal.$^*$

%% GIB, current best bridge program, approximates this idea by\al
%% 1) generating 100 deals consistent with bidding information\al
%% 2) picking the action that wins most tricks on average 


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Example}{

%% Four-card bridge/whist/hearts hand, {\sc Max} to play first

%% \vspace*{0.1in}

%% \show[1.0]
%% \show{\sfile{figures}{card-tree1}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Example}{

%% Four-card bridge/whist/hearts hand, {\sc Max} to play first

%% \vspace*{0.1in}

%% \show[1.0]
%% \show{\sfile{figures}{card-tree2}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Example}{

%% Four-card bridge/whist/hearts hand, {\sc Max} to play first

%% \vspace*{0.1in}

%% \show[1.0]
%% \show{\sfile{figures}{card-tree3}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Commonsense example}{

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    take the left fork and you'll find a mound of jewels;\\
%%    take the right fork and you'll be run over by a bus.


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Commonsense example}{

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    take the left fork and you'll find a mound of jewels;\\
%%    take the right fork and you'll be run over by a bus.

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    take the left fork and you'll  be run over by a bus;\\
%%    take the right fork and you'll find a mound of jewels.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Commonsense example}{

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    take the left fork and you'll find a mound of jewels;\\
%%    take the right fork and you'll be run over by a bus.

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    take the left fork and you'll  be run over by a bus;\\
%%    take the right fork and you'll find a mound of jewels.

%% Road A leads to a small heap of gold pieces\\
%% Road B leads to a fork:\\
%%    guess correctly and you'll find a mound of jewels;\\
%%    guess incorrectly and you'll be run over by a bus.


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Proper analysis}{

%% {*} Intuition that the value of an action is the average of its values\\
%% in all actual states is \emph{WRONG}

%% With partial observability, value of an action depends on the\\
%% \defn{information state} or \defn{belief state} the agent is in

%% Can generate and search a tree of information states

%% Leads to rational behaviors such as\al
%% \item Acting to obtain information\al
%% \item Signalling to one's partner\al
%% \item Acting randomly to minimize information disclosure



%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Summary}{

%% Games are fun to work on! (and dangerous)

%% They illustrate several important points about AI

%% \item perfection is unattainable $\Rightarrow$ must approximate

%% \item good idea to think about what to think about

%% \item uncertainty constrains the assignment of values to states

%% \item optimal decisions depend on information state, not real state

%% Games are to AI as grand prix racing is to automobile design







%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning1}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning2}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning3}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning4}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning5}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning6}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning7}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning in nondeterministic game trees}{

%% A version of $\alpha$-$\beta$ pruning is possible:

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-pruning8}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded1}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded2}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded3}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded4}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded5}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Pruning contd.}{

%% More pruning occurs if we can bound the leaf values

%% \vspace*{0.3in}

%% \show[0.9]
%% \show{\sfile{figures}{expectiminimax-bounded6}}





\slidesfoot
