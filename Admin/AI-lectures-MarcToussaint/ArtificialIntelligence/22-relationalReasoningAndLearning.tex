\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Relational Probabilistic Modelling and Learning**}
\renewcommand{\keywords}{}

\slides

\newcommand{\pr}[1]{{\ensuremath{\texttt{#1}}}}

\story{We've learned about FOL and the standard logic inference
methods. As I mentioned earlier, I think that the motivation to learn
about FOL is less the logic inference methods, but that the FOL
formalism can be used to generalize AI methods (Markov-Decision
Processes, Reinforcement Learing, Graphical Models, Machine Learning)
to relational domains, that is, domains where the state or input is
described in terms of properties and relations of objects.

As a side note: in the mentioned areas researchers often use the
word \emph{relational} to indicate that a model uses FOL
representations.

These generalizations are the topic of this lecture. We first consider
MDPs and describe STRIPS rules as a relational way to model state
transitions for deterministic worlds; then their probabilistic
extension called NDRs and how to learn them from data. A core message
here is that allowing for probabilities in transition is a crucial
pre-requisite to make them learnable---because anything that is learnt
from limited data is necessarily also uncertain.

We then decribe briefly relational extensions of graphical models,
namely Markov Logic Networks (=relational factor graphs), which allow
us to formulate probabilistic models over relational domains, e.g.,
over data bases, and use probabilistic inference methods to draw
conclusions.

If time permits, we also mention relational regression trees as
a relational extension of standard Machine Learning regression.

For brevity we skip the classical AI discussion of the situation calculus and
frame problem---please see the AIMA book if you're interested.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{STRIPS-like rules to model MDP transitions}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Markov Decision Process (MDP)}
\slide{Markov Decision Process}{

\item Let's recall standard MDPs

\shows[.7]{mdp1}

~

\item Assume the state $s$ is a sentence (or KB) in a FOL. How
could we represent transition probabilities $P(s' \| s,a)$, rewards
$R(s,a)$, and a policy $\pi(s)$?? In general that would be very hard!

~

\item We make the simpler assumption that the state $s$ is a
conjuction of \emph{grounded literals}, that is, facts without logic
variables, for instance:

\begin{items}
\item Constants: $C_1, C_2, P_1, P_2, SFO, JFK$
\item Predicates: $At(.,.), Cargo(.), Plane(.), Airport(.)$
\item A state description:

$At(C_1, SFO) \wedge At(C_2, JFK) \wedge  At(P_1, SFO) \wedge  At(P_2, JFK)
\wedge Cargo(C_1) \wedge Cargo(C_2) \wedge Plane(P_1) \wedge  Plane(P_2)
\wedge Airport (JFK) \wedge Airport (SFO)$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{STRIPS rules}
\key{Planning Domain Definition Language (PDDL)}
\slide{STRIPS rules and PDDL}{

\item \defn{STRIPS} rules (Stanford Research Institute Problem Solver)
are a simple way to describe \emph{deterministic} transition
models. The \defn{Planning Domain Definition Language} (PDDL)
standardizes STRIPS

\show[.9]{rn-PDDL1}
%{\tiny\hfill (from Russel \& Norvig)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PDDL (or STRIPS)}{

\item The \defn{precondition} specifies if an action predicate is
  applicable in a given situation

\item The \defn{effect} determines the \emph{changed facts}

~

\item \defn{Frame assumption:} All facts not mentioned in the effect remain
unchanged.

~

\item The majority of state-of-the-art AI planners use this
format. E.g., \emph{FFplan}: (B.\ Nebel, Freiburg) a forward chaining heuristic state space planner\\

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Another PDDL example}{

%\show{rn-PDDL2}
\show{rn-PDDL3}
\show{rn-PDDL4}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decision Making with STRIPS}{

\item A general approach to planning is to query the KB for a plan that
fulfills a goal condition; whether this is efficient is debated.

\item The standard approach is fwd search:
\begin{items}
\item We build a standard decision tree; every node corresponds to a
situation
\item When expanding a node we need to compute all
feasible actions. This implies to compute all feasible substitutions
of all action preconditions $\to$ \textbf{matching problem}.
\item This can in principle allow also for rewards and costs; if we
have a heuristic we could use $A^*$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item STRIPS are nice, intuitive, concise, easy to plan with, and work very well
in deterministic domains. But they can't really be learned. Even in a
deterministc world it is very awkward and hard to try to extract
deterministic rules from only limited data.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Consider data collected by an agent...}{

\hspace*{-10mm}$D=\{$

{\tiny\tt
\hspace*{-12mm}\begin{tabular}{r@{~}p{.9\textwidth}}
grab(c)  :~~& box(a) box(b) ball(c) table(d) on(a,b) on(b,d) on(c,d) inhand(nil) ...\\
      $\to$ & box(a) box(b) ball(c) table(d) on(a,b) on(b,d) $\neg$on(c,d) inhand(c) ...\\[3ex]
puton(a) :~~& box(a) box(b) ball(c) table(d) on(a,b) on(b,d) $\neg$on(c,d) inhand(c) ...\\
      $\to$ & box(a) box(b) ball(c) table(d) on(a,b) on(b,d) on(c,a) inhand(nil) ...\\[3ex]
puton(b) :~~& box(a) box(b) ball(c) table(d) on(a,b) on(b,d) on(c,a) inhand(nil) ...\\
      $\to$ & box(a) box(b) ball(c) table(d) on(a,b) on(b,d) on(c,a) inhand(nil) ...\\[3ex]
grab(b)  :~~& box(a) box(b) ball(c) table(d) on(a,b) on(b,d) on(c,a) inhand(nil) ...\\
      $\to$ & box(a) box(b) ball(c) table(d) on(a,d) $\neg$on(b,d) on(c,d) inhand(b) ...\\[3ex]
\vdots
\end{tabular}
}

\}

\item How can we learn a predictive model $P(s' \| a, s)$ for this
data?

{\tiny With $n=20$ objects, state space is $>2^{n^2} \approx 10^{120}$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Learning probabilistic rules}
\slide{Learning probabilistic rules}{

{\tiny Pasula, Zettlemoyer \& Kaelbling: Learning probabilistic relational
planning rules (ICAPS 2004)

}

\item \textbf{Compress} this data into probabilistic relational rules:
%\show[.4]{rules}
{\tiny
\begin{align*}
  grab(X) : & \quad on(X,Y),~ ball(X),~ cube(Y),~ table(Z)
  \\
  &
  \quad
  \rightarrow
  \quad
  \left\{
  \begin{array}{ l l l }
      0.7 & : & inhand(X),~ \neg on(X,Y) \\
      0.2 & : & on(X,Z),~ \neg on(X,Y) \\
      0.1 & : & \text{noise}
  \end{array}
  \right.
 \end{align*}}
Find a rule set that maximizes \textbf{(likelihood - description
length)}

\item These rules define a \emph{probabilistic transition probability}
$P(s'|s,a)$

\tiny
Namely, if $(s,a)$ has a unique covering rule $r$, then
$$P(s'|s, a) = P(s'|s, r) = \sum_{i=0}^{m_r} p_{r,i}~ P(s'|\O_{r,i}, s)$$
where $P(s'|\O_{r,i}, s)$ describes the deterministic state transition
of the $i$th outcome.% (see Lang \& Toussaint, JAIR 2010).

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Role of uncertainty in learning these rules}{

~

\centerline{\input{figs/relationalStatLearn}}

~%\mypause

\item[$\To$]

uncertainty $\oto$ regularization $\oto$ compression \& abstraction

~

\item Introducing uncertainty in the rules not only allows us to
model stochastic worlds, it \emph{enables to compress/regularize and thereby
learn strongly generalizing models!}

~

\cen{uncertainty enables learning!}

%% ~\mypause

%% \item Perhaps: a core problem with deterministic AI is
%%   \emph{learning} deterministic models?

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Planning with probabilistic rules}
\slide{Planning with learned probabilistic rules*}{

\item Tree search (SST \& UCT) does not scale with \# objects

~

\item We can \textbf{propositionalize} the learned knowledge into
a \textbf{Dynamic Bayesian Network (DBN)}: For every domain $\DD$ they
define a grounded DBN

\show[.25]{dbn_nid}

{\tiny\hfill (Lang \& Toussaint, JAIR 2010)}

\item Planning (estimating the likelihood of action sequences) can
efficiently be done using probabilitic inference methods in this DBN

}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Robot Applications}{

%% \hspace*{-10mm}\twocol{.5}{.5}{
%% \center
%% Random exploration:\\
%% \moviex{\show[.6]{09-lang-NIDDBN/robot}}{movies/09-tobias/film_randomActions.avi}

%% ~

%% Planning:\\
%% \moviex{\show[.6]{09-lang-ecml-rg/img/robot_clearance}}{movies/09-tobias/film_exp3.avi}

%% }{\center

%% Real-world:\\
%% \moviex{\show[.6]{robi}}{movies/09-robi/ICRAmovie.avi}

%%  ~

%% Online explore-exploit:\\
%% \moviex{\show[.6]{10-lang-ecml/img/grobi}}{movies/10-relationalExploration/movie_clearance_big.avi}

%% }

%% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{}{
\tiny switch slides: 12/talk-Stanford ./talk-MIT
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Relational Graphical Models}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Intro}{

\item Probabilistic relational modelling has been an
important development in modern AI. It fuses:

~

{\small
\cen{Structured first-order (logic) representations ($\oto$ strong generalization)}

\cen{\Large\textbf{+}}

\cen{Probabilistic/statistical modelling, inference \& learning}
}

~

~\tiny

\item I use the term ``Probabilistic relational modelling'' for all
formalisms of that kind, including \emph{Markov Logic Networks,
Bayesian Logic Programs, Probabilistic Relational Models, Relational
Markov Networks, Relational Probability Trees, Stochastic Logic
Programming, ... BLOG}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\show[.5]{srl1}
\tiny\hfill (from De Readt \& Kersting)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Intro}{

\item A popular science article:

\cen{\emph{I, algorithm: A new dawn for artificial intelligence}}

\hfill{(Anil Ananthaswamy, NewScientist, January 2011)}

~

{\tiny Talks of ``probabilistic programming, which combines the logical
underpinnings of the old AI with the power of statistics and
probability.'' Cites Stuart Russel as ``It's a natural unification of
two of the most powerful theories that have been developed to
understand the world and reason about it.'' and Josh Tenenbaum as
{\color{blue}``It's definitely spring''}.\\}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Intro}{

\item I think: probabilistic relational modelling does not suddenly
solve all problems, but is important because:
\begin{items}
\item One of the great deficits of classical AI is the inefficiency of
learning (constructing deterministic knowledge bases from data) --
statistical relational approaches do this the right way

\item The world is structured in terms of objects and their properties
and relations -- first-order representations offer a formalization of
this structure; we need to such formalizations for strong
generalization

\item In my view: currently the only way to express \& learn
uncertain \& generalizing knowledge about environments with objects,
properties \& relations

\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{References}{

\item Pedro Domingos: CIKM-2013 tutorial on Statistical Relational Learning 

{\tiny\url{http://homes.cs.washington.edu/~pedrod/cikm13.html}}

~

\item Lise Getoor: ECML/PKDD 2007 tutorial on SRL

{\tiny\url{http://www.ecmlpkdd2007.org/CD/tutorials/T3_Getoor/Getoor_CD.pdf}

(or \tiny\url{http://www.cs.purdue.edu/probdb/updb06/UPDB-PRM-09-22-06.ppt}

}

\item Survey paper by Luc De Raedt and Kristian Kersting:

{\tiny\url{https://lirias.kuleuven.be/bitstream/123456789/301404/1/pilp.pdf}}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Background}{

%% \item \textbf{Constants} (objects), \textbf{logical variables}, 
%% \textbf{predicates} \quad (\& functions)

%% \cen{$ \text{Anna},~ x,~ \pr{Friends}(x, y), \quad \pr{MotherOf}(x)$}

%% \item \textbf{Literal:} Predicate or its negation

%% \item \textbf{Clause:} Disjunction of literals

%% \item \textbf{Horn clause:} A clause written as rule

%% \item Current \textbf{Domain:} The set $\DD=\{A,B,..\}$ of constants

%% \item \textbf{Grounding:} Replace all logical variables by constants

%% \item \textbf{State $X$} (``World, model, instantiation, interpretation''):
%% Assignment of truth values to all ground predicates (atoms)

%% \item \textbf{Random variables:} Each ground predicate corresponds to
%% a binary RV, such that $X=(X_1,..,X_N)$

%% \item \textbf{Grouded domain $\dom(X;\DD)$:} The space of $X\in \dom(X;\DD)$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Probabilistic Relational Modelling}{

\item In general, probabilistic relational approaches
\begin{items}
\item make predictions based only on the properties/relations of
   objects, \emph{not their identity}

\item generalize data seen in one world (with objects $A,B,C,...$)

 to another world (with objects $D,E,..$)

\item thereby imply a very strong type of generalization/prior

 which allows to efficiently learn in the exponentially large space
\end{items}

~

\item Formally, they are frameworks that define a

\begin{center}
probability distribution $P(X;\DD)$ or discriminative\\ function
$F(X;\DD)$ over $\dom(X;\DD)$, for any domain $\DD$
\end{center}
where $X$ are the random variables that exist for a given domain $\DD$
(a given set of objects/constants) [[Inconsistent with previous use of
word 'domain']]

{\small (Note, this is a ``transdimensional'' distribution/discriminative function)}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Probabilistic Relational Models (PRMs)}
\slide{Probabilistic Relational Models (PRMs)}{

\item (brief informal intro, from Lise Getoor's tutorial)

~

\item Consider a relational data base

~

\show{prm1}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PRM}{

\item We think of the table attributes as random variables that depend
on each other

~

\show[.7]{prm2}

~

$P(A|Q,M)$ is a conditional probability table, which should
be \emph{independent of the particular identity (primary key) of the
paper and reviewer}---$A$ should only depend on the values of $Q$ and
$M$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PRM}{

\item In a particular domain $\DD=\{A1, A2, P1, P2, P3, R1, R2, R3\}$,
the PRM defines a probability distribution over the instantiations of
all attributes (grounded predicates) 

~

\show[.6]{prm3}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PRM}{

\item \emph{Learning} PRMs amounts to learning all conditional
probability tables from a relational data base

\item \emph{Inference} with PRMs means to construct the big grounded
Bayesian Network
\begin{items}
\item Each grounded predicate $\to$ random variable
\end{items}

~

\item PRMs are nice because they draw clear connections to relational
databases. But there is a easier/cleaner formulation of such types of
models: Markov Logic Networks (MLN).

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Markov Logic Networks (MLNs)}
\slide{Markov Logic Networks (MLN)}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MLN example: Friends \& Smokers}{

\item Consider three \textbf{weighted} Horn clauses

$w_1=1.5, F_1: \pr{cancer}(x) \gets \pr{smoking}(x)$

$w_2=1.1, F_2: \pr{smoking}(x) \gets \pr{friends}(x,y) \wedge \pr{smoking}(y)$

$w_3=1.1, F_3: \pr{smoking}(y) \gets \pr{friends}(X,Y) \wedge \pr{smoking}(x)$


\item Consider the domain $\DD=\{Anna, Bob\}$

\item Set of random variables (grounded predicates) becomes:
\begin{center}\tiny
$\{\pr{cancer}(A), \pr{cancer}(B), \pr{smoking}(A), \pr{smoking}(B),$\\$ \pr{friends}(A,A), \pr{friends}(A,B), \pr{friends}(B,A), \pr{friends}(B,B) \}$
\end{center}

\show[.5]{srl2}

%% $$P(X) = \frac{1}{Z} \prod_c \exp{n_c(X)~ w_c}$$

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MLN}{

\item The MLN is defined by a set $\{ (F_i,w_i) \}$ of pairs where
\begin{items}
\item $F_i$ is a formula in first-order logic
\item $w_i \in \RRR$ is a weight
\end{items}

~

\item For a domain $\DD$ this generates a factor graph with
\begin{items}
\item one random variable for each grounded predicate
\item one factor for each grounding of each formula
\end{items}
$$F(X,\DD) \propto \exp\{\sum_i \sum_{\text{true groudings of $F_i$ in $X$}} w_i\}$$

~

\item MLNs can be viewed as a \textbf{factor graph template}
\begin{items}
\item For every domain $\DD$ a grounded factor graph $F(X;\DD)$ is defined
\item The ground factor graph has many \emph{shared
  parameters}
$\to$ learning the weights implies strong generalization across objects
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Generality of MLNs}{

\item Special (non-relational) cases: ~ (Limit of all predicates zero-arity)
\begin{items}
\item Markov networks
\item Markov random fields
\item Bayesian networks
\item Log-linear models
\item Exponential models
\item Max. entropy models
\item Gibbs distributions
\item Boltzmann machines
\item Logistic regression
\item Hidden Markov models
\item Conditional random fields
\end{items}

\item Limit infinite weights $\to$ first-order logic

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MLN}{

\item \emph{Inference} in MLN: Create the grounded factor graph

\item \emph{Learning}: Gradient descent on the likelihood (often hard,
even with full data)

~

\item The learned factor graph $F(X;\DD)$ can also define a
\emph{discriminative function}:
\begin{items}
\item Relational logistic regression
\item Relational Conditional Random Fields
\end{items}
\tiny (See also \emph{Discriminative probabilistic models for
relational data}, Taskar, Abbeel \& Koller; UAI 2002.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Bayesian Logic Programs}{
%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Bayesian Logic Programs}{

%% \item In logic programming,

%% \cen{$\pr{stable}(A) \pr{~:-~} \pr{on}(A,B), \pr{big}(B)$}
%% means
%% {\tiny ``\textbf{For all} $A,B$ if $B$ is big and $A$ on $B$ then $A$ is
%% stable''}

%% ~

%% \item In Bayesian Logic Programs [Kersting and de Raedt]

%% \cen{$\pr{stable}(A) \mid \pr{on}(A,B), \pr{big}(B)$}
%% means
%% {\tiny ``\textbf{For all} $A,B$, if $\pr{big}(B)$ and $\pr{on}(A,B)$ are random
%% variables, then $\pr{stable}(A)$ is a random variable''}

%% Associated with this rule is a conditional probability table (CPT)
%% that specifies the probability distribution over $\pr{stable}(A)$ for any
%% possible values of $\pr{on}(A,B)$ and $\pr{big}(B)$

%% ~

%% \item BLP can be viewed as a \emph{syntax} to specify
%% a \textbf{Bayesian Network template}, that is
%% \begin{items}
%% \item For every domain $\DD$ a grounded Bayesian Network $P(X;\DD)$ is
%% defined
%% \item The ground Bayes Net will have shared parameters from the CPTs
%% $\to$ learning the weights implies strong generalization across objects
%% \end{items}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{BLP example}{

%% \item Consider the dependency of the blood type \pr{bt} of Dorothy
%% depending on the maternal \pr{mc} and paternal chromosomes \pr{pc},
%% from her mother Ann and father Brian.

%% ~

%% \show{blp1}
%% \tiny\hfill (from Kersting \& De Readt)

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{BLP example}{

%% \item The dependency structure re-written as a Logic Program

%% ~

%% \show[.45]{blp2}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{BLP example}{

%% \item Instead of having clauses, we now have conditional dependencies
%% and probability tables

%% ~

%% \show{blp3}

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{BLPs}{

%% \item The benefit of BLPs is: learning CPTs is easier than weights of
%% a MLN

%% \item Reference:
%% \emph{Bayesian Logic Programming: Theory and Tool}, K Kersting \& L De Raedt. In \emph{Statistical Relational Learning},
%% 2007.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{}{

\tiny slides: ~/git/3rdHand/documents/USTT/17-reviewMeeting3/slides.pdf

}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conclusions}{

\item What all approaches have in common:
\begin{items}
\item A ``syntax'' for a \textbf{template} that, for every domain
$\DD$, defines a grounded factor graph, Bayes Net, or DBN
\item The grounding implies parameter sharing and strong
generalization, e.g.\ over object identities
\item Inference, learning \& planning often operate on the grounded
model
\end{items}

~

\item Using probabilistic modelling, inference and learning \emph{on top
  of first-order representations}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Side note on inference in first-order logic}{

%% \item Traditionally done by theorem proving (e.g.: Prolog)

%% \item Propositionalization followed by model checking turns out to be faster (often a lot)

%% \item Propositionalization: Create all ground atoms and clauses

%% \item Model checking: Satisfiability testing (e.g.: WalkSAT, or Linear
%% Programming reductions...)

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{The role of uncertainty in AI}
\slide{The role of uncertainty in AI}{\label{lastpage}

\item What is the benefit of the probabilities in these approaches?
\begin{items}
\item Obviously: If the world is stochastic, we'd like to represent this
\item But, at least as important:
\end{items}

\begin{quote}\emph{Uncertainty \textbf{enables to compress/regularize and thereby
learn strongly \emph{generalizing} models}}
\end{quote}

~

\show[.6]{relationalStatLearn}

~

\cen{uncertainty $\oto$ regularization $\oto$ compression \& abstraction}

~

%% \cen{\textbf{uncertainty enables learning!}}

%% ~

\item The core problem with deterministic AI is \emph{learning}
 deterministic models

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{More structure than objects-properties-relations}{\label{lastpage}

%% ~

%% \begin{tabular}{c@{\qquad\qquad}c}
%% \showh[.25]{robot-bubble}
%% &
%% \showh[.3]{messy-kitchen} \\[2ex]
%% %\mypause
%% vector space & hybrid, combinatorial space \\
%% great advances & great challenges \\
%% %\mypause
%% mature ML/RL methods & relational learning \& inference
%% \end{tabular}

%% ~

%% \item Statistical Relational Learning addresses only one aspect
%%  of such worlds: objects. There is much more:

%% \tiny

%% ~

%% \hspace*{-5mm}\threecoltop[.05]{.3}{.3}{.3}{
%% static:

%% -- 3D Geometry

%% -- Kinematics (rigid b., DoFs)

%% -- Controllability

%% }{

%% dynamic:

%% -- things don't float in the air

%% -- small things on top of large

%% -- implausible forces

%% }{

%% semantic:

%% -- a car in a kitchen is unlikely

%% %-- 20 Australians drinking good beer is unlikely

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
