\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\exnum}{3}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Votieraufgabe: ~ Value Iteration}

(Teilaufgaben werden separat votiert.)

\show{../pics-AI/Maze}

Consider the circle of states above, which depicts the 8 states of an
MDP. The green state (\#1) receives a reward of $r=4096$ and is a 'tunnel'
state (see below), the red state (\#2) is punished with
$r=-512$. Consider a discounting of $\g=1/2$.

Description of $P(s'|s,a)$:\NewParNoBreak
\begin{itemize}
\item The agent can choose between two actions: going one step clock-wise or one
  step counter-clock-wise.
\item With probability $3/4$ the agent will transition to the desired state,
  with probability $1/4$ to the state in opposite direction.
\item Exception: When $s=1$ (the green state) the next state will be
$s'=4$, independent of $a$. The Markov Decision Process never ends.
\end{itemize}

Description of $R(s,a)$:\NewParNoBreak
\begin{itemize}
\item The agent receives a reward of $r=4096$ when $s=1$ (the green state).
\item The agent receives a reward of $r=-512$ when $s=2$ (the red
state).
\item The agent receives zero reward otherwise.
\end{itemize}

\begin{enumerate}
\item Perform three steps of Value Iteration: Initialize $V_{k=0}(s)=0$, what is
  $V_{k=1}(s)$, $V_{k=2}(s)$, $V_{k=3}(s)$?
\item How can you compute the value function $V^\pi(s)$ for
a GIVEN policy (e.g., always walk clock-wise) in closed form? Provide
an explicit matrix equation.
\item Assume you are given $V^*(s)$. How can you compute the optimal $Q^*(s,a)$
  form this? And assume $Q^*(s,a)$ is given, how can you compute the optimal
  $V^*(s)$ from this? Provide general equations.
\item What is $Q_{k=3}(s,a)$ for the example above? What is the
  ``optimal'' policy given $Q_{k=3}$?
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Programmieraufgabe: ~ Value Iteration}

In the repository you find python code to load the probability table
$P(s'|a,s)$ and the reward function $R(a,s)$ for the maze of Exercise
1. In addition, the MDP is defined by $\g=0.5$.

Implement Value Iteration to reproduce the results of Exercise
1(a). Tip: An easy way to implement this is to iterate the two
equations:
\begin{align}
Q(s,a) \gets R(s,a) + \g \sum_{s'} P(s'|s,a)~ V(s') \\
V(s) \gets \max_a Q(s,a)
\end{align}
Compare with the value functions $V_{k=1}(s)$,
$V_{k=2}(s)$, $V_{k=3}(s)$ computed by hand. Also compute the
$V_{k=100} \approx V^*$.

The code test requires you to return both, the value function and the Q-function.
%% Also return the Q-function
%% (b) Implement Q-Iteration for exactly the same setting. (Yes, this is a simple modification of (a).)
% Check that $V(s) = \max_a Q(s,a)$ converges to the same optimal value.

WARNING: The test you have in your repository only tests for the specific world of Exercise 1. However, our evaluation will test your method also for other MDPs with other states, actions, rewards, and transitions! Implement general methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Präsenzaufgabe: ~ The Tiger Problem}

Assume that the tiger is truly behind the left door. Consider an agent that always chooses to listen.

a) Compute the belief state after each iteration.

b) In each iteration, estimate the expected reward of
open-left/open-right based only on the current belief state.

c) When should the agent stop listening and open a door for a discount
factor of $\g=1$? (How would this change if there were zero costs for listening?)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \exsection{Präsenzaufgabe: ~ Eligibilities in TD-learning}

%% Consider TD-learning in the same maze. Describe at what events plain
%% TD-learning will update the value function, how it will update
%% it. Guess roughly how many steps the agent will have taken when for
%% the first time $V(s_4)$ becomes non-zero. How would this be
%% different for eligibility traces?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exerfoot
