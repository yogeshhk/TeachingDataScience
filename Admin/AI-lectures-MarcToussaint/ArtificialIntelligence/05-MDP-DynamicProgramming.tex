\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Dynamic Programming}
\renewcommand{\keywords}{Dynamic Programming, Belief Planning}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

So far we focussed on tree search-like solvers for decision
problems. There is a second important family of methods based on
dynamic programming approaches, including Value Iteration. The Bellman
optimality equation is at the heart of these methods.

Such dynamic programming methods are important also because standard
Reinforcement Learning methods (learning to make decisions when the
environment model is initially unknown) are directly derived from
them.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Markov Decision Process}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Tree Search vs.\ Dynamic Programming}{

\item \emph{Tree search} and sampling (MCTS) is one way to collect information about possible futures

\item \emph{Dynamic Programming} is the core alternative, which computes a  function over states that accumulates all information (value function, cost-to-go function)

~\pause

\item To enable Dynamic Programming, we need a (probabilistic) model of possible transitions. Previously this was the \texttt{succ} function $s\mapsto \{s',..\}$. Now we formulate transitions probabilistically $P(s'|s,a)$ as \emph{Markov Decision Processes} (MDPs).

~\pause

\item MDPs are the basis of Reinforcement Learning, where $P(s'|s,a)$
is not know by the agent. A core reason for formalizing $P(s'|s,a)$ probabilistically is that the agent only has partial knowledge about transitions, due to limited experience.

%% ~

%% \twocol{.5}{.4}{\center
%%   \mov{\show[.6]{mov-balance}}{05-sethu-movies/DA_PoleLearn.avi}
%% }{\center
%%   \mov{\show[.6]{mov-juggle}}{05-sethu-movies/DB_juggle.avi}
%% }

%% {\hfill\tiny (around 2000, by Schaal, Atkeson, Vijayakumar)}

%% ~

%% \mov{\show[.25]{helicopter}}{07-andrew-ng/Aerobatic_rolls.wmv}

%% {\hfill\tiny (2007, Andrew Ng et al.)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Markov Decision Process}
\slide{Markov Decision Process}{

~

\shows[.7]{mdp1}

\cen{$P(s_{0:T\po},a_{0:T},r_{0:T};\pi) = P(s_0)~ \prod_{t=0}^T
P(a_t|s_t;\pi)~ P(r_t|s_t,a_t)~ P(s_{t\po}|s_t,a_t)$}

~

\begin{items}
\item world's initial state distribution $P(s_0)$
\item world's transition probabilities $P(s_{t\po} \| s_t,a_t)$
\item world's reward probabilities $P(r_t \| s_t,a_t)$
\item agent's \emph{policy} $\pi(a_t \| s_t) = P(a_0|s_0;\pi)$ ~ (or
   deterministic $a_t = \pi(s_t)$)
\end{items}

~

\item \textbf{Stationary MDP:}

-- We assume $P(s' \| s,a)$ and $P(r|s,a)$ independent of time

-- We also define $R(s,a) := \Exp{r|s,a} = \int r~ P(r|s,a)~ dr$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{MDP}{

\item In basic discrete MDPs, the transition probability
$$P(s'|s,a)$$
is just a table of probabilities

~

\item The \emph{Markov} property refers to how we defined state:

History and future are conditionally independent given $s_t$
$$I(s_{t^+}, s_{t^-} | s_t) \comma \forall t^+>t, t^-<t$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Dynamic Programming}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Value Function}
\slide{State value function}{

\item We consider a stationary MDP described by
$$P(s_0)\comma P(s' \| s,a)\comma P(r \| s,a)\comma \pi(a_t \| s_t)$$

~

\item The \defn{value} (\emph{expected} discounted return) of policy $\pi$ when started in state $s$ is:
\begin{align*}
V^\pi(s) = \Exp[\pi]{ r_0 + \g r_1 + \g^2 r_2 + \cdots \| s_0\=s }~,
\end{align*}
with \emph{discounting factor $\g\in[0,1]$}

~\mypause

\item A policy $\pi^*$ is \defn{optimal} iff
\begin{align*}
\forall s:~ V^{\pi^*}(s) = V^*(s)\comma
\text{ where } ~ V^*(s) = \max_\pi V^\pi(s) ~,
\end{align*}
i.e., simultaneously maximises the value in all states

{\small (In MDPs there always exists (at least one) optimal deterministic
policy.)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\twocol{.3}{.3}{
An example for a \\
value function...
}{\center

\showhs[5]{15x20holes}

}

~

{\hfill\tiny\texttt{demo: test/mdp runVI}}

~

~

\cen{\textbf{Values provide a gradient towards desirable states}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Value function}{

~

\item The value function $V$ is a central concept in all of RL!

{\small\medskip

Many algorithms can directly be derived from properties of the value
function.

}

~

\item In other domains (stochastic optimal control) it is also
called \emph{cost-to-go} function ($\text{cost}=-\text{reward}$)

%% ~

%% \item There also exists a $Q$-function (state-action value function)
%% very similar to the $V$ function (see below)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Recursive property of the value function}{

\begin{align*}
\hspace*{-10mm}{\color{red}V^\pi(s)}
 &= \Exp{ r_0 + \g r_1 + \g^2 r_2 + \cdots \| s_0\=s ;\pi} \\
 &= \Exp{ r_0 \| s_0\=s ;\pi} + \g \Exp{ r_1 + \g r_2 + \cdots \| s_0\=s ;\pi} \\
 &= R(s,\pi(s))  +  \g \tsum_{s'} P(s'\|s,\pi(s))~ \Exp{ r_1 + \g r_2 + \cdots \| s_1\=s' ;\pi} \\
 &= R(s,\pi(s)) + \g \tsum_{s'} P(s'\|s,\pi(s))~ {\color{red}V^\pi(s')}
\end{align*}

~\mypause

\item We can write this in vector notation \quad $\vec
V^\pi = \vec R^\pi + \g \vec P^\pi \vec V^\pi$ \\ with vectors $\vec
V^\pi_s = V^\pi(s)$, $\vec R^\pi_s =R(s,\pi(s))$ and matrix $\vec
P^\pi_{ss'} = P(s'\|s,\pi(s))$

~

\item For stochastic  $\pi(a|s)$:

\cen{$V^\pi(s)= \sum_a \pi(a|s)R(s,a) + \g \tsum_{s',a}  \pi(a|s)P(s'\|s,a)~ V^\pi(s')$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bellman optimality equation}
\slide{Bellman optimality equation}{

\item Recall the recursive property of the value function
$${\color{red}V^\pi(s)}
 = R(s,\pi(s)) + \g \tsum_{s'} P(s'\|s,\pi(s))~ {\color{red}V^\pi(s')}$$

~

\item Bellman optimality equation

\eqbox{$
V^*(s)
 = \max_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \]
$}
with $
\pi^*(s)
 = \argmax_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \]
$

~

{\tiny (Sketch of proof: If $\pi$ would select another action than
$\argmax_a[\cdot]$, then $\pi'$ which $=\pi$
everywhere except $\pi'(s)=\argmax_a[\cdot]$ would be better.)

}

~

\item This is the \textbf{principle of optimality} in the stochastic
  case

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\hspace*{-5mm}\twocol{.4}{.65}{\center
\small Richard E. Bellman (1920â€“-1984)
\show{bellman}

}{\center

~

\textbf{Bellman's principle of optimality}

\medskip

\showh[.4]{bellmanOpt}

}

~

~

\begin{align*}
V^*(s)
 &= \max_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \]  \\
\pi^*(s)
 &= \argmax_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \]
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Value Iteration}
\slide{Value Iteration}{

\item \emph{How can we use this to compute $V^*$?}

\item Recall the Bellman optimality equation:

~

\cen{$
V^*(s)
 = \max_a \[R(s,a) + \g \sum_{s'} P(s'\|s,a)~ V^*(s') \]
$}
\item \textbf{Value Iteration:} \quad (initialize $V_{k\=0}(s)=0$)
$$
\forall s:~
  V_{k+1}(s)
  = \max_a \[ R(s,a) + \g \sum_{s'} P(s'|s,a) ~V_k(s') \]
$$
stopping criterion: \quad $\max_s |V_{k+1}(s) - V_k(s)| \le \e$

~

\item Note that $V^*$ is a \textbf{fixed point} of value iteration!

\item Value Iteration converges to the optimal value function $V^*$
(proof below)

{\hfill\tiny\texttt{demo: test/mdp runVI}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Q-Function}
\slide{State-action value function (\protect{$Q$}-function)}{

\item \emph{We repeat the last couple of slides for the $Q$-function...}

\item The \emph{state-action value function} (or \textbf{$Q$-function}) 
is the expected discounted return when starting in state $s$
and taking first action $a$:
\begin{align*}
{\color{red}Q^\pi(s,a)}
 &= \Exp[\pi]{ r_0 + \g r_1 + \g^2 r_2 + \cdots \| s_0\=s, a_0\=a } \\
 &= R(s,a) + \g \sum_{s'} P(s'\|s,a)~ {\color{red}Q^\pi(s',\pi(s'))}
\end{align*}
(Note: $V^\pi(s) = Q^\pi(s,\pi(s))$.)

~

\item Bellman optimality equation for the $Q$-function

\eqbox{$
Q^*(s,a)
 = R(s,a) + \g \sum_{s'} P(s'\|s,a) \max_{a'} Q^*(s',a') 
$}
with $\pi^*(s)
= \argmax_a Q^*(s,a)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Q-Iteration}
\slide{Q-Iteration}{

\item Recall the Bellman equation:

~

\cen{$
Q^*(s,a)
 = R(s,a) + \g \sum_{s'} P(s'\|s,a)~ \max_{a'} Q^*(s',a')
$}
\item \textbf{Q-Iteration:} \quad (initialize $Q_{k\=0}(s,a)=0$)
$$
\forall_{s,a}:~
  Q_{k+1}(s,a)
  = R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'} Q_k(s',a')
$$
stopping criterion: \quad $\max_{s,a} |Q_{k+1}(s,a) - Q_k(s,a)| \le \e$

~

\item Note that $Q^*$ is a \textbf{fixed point} of Q-Iteration!

\item Q-Iteration converges to the optimal state-action value function $Q^*$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Proof of convergence of Q-Iteration}
\slide{Proof of convergence}{

\item Let $\D_k = ||Q^* - Q_k||_\infty = \max_{s,a} | Q^*(s,a) - Q_k(s,a)|$
\begin{align*}
\hspace*{-10mm}Q_{k+1}(s,a)
 &= R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'} Q_k(s',a') \\
 &\le R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'}\[ Q^*(s',a') + \D_k \]\\
 &= \[ R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'} Q^*(s',a')\] + \g \D_k \\
 &= Q^*(s,a) + \g \D_k
\end{align*}

similarly: $Q_k \ge Q^*-\D_k ~ \To ~ Q_{k+1} \ge Q^* - \g \D_k$

~

\item The proof translates directly also to value iteration

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{For completeness**}{

~

\item \textbf{Policy Evaluation} computes $V^\pi$ instead of $V^*$:
Iterate:
$$
\forall s:~
 V_{k+1}^\pi(s)
 = R(s,\pi(s)) + \g \Sum_{s'} P(s'|s,\pi(s))~ V_k^\pi(s')
$$
%% \forall_{s,a}:~
%%  &Q_{k+1}(s,a)
%%  = R(s,a) + \g \sum_{s'} P(s'|s,a)~ Q_k(\pi(s'),s')
 Or use matrix inversion
%% \vec V^\pi &= \vec R^\pi + \g \vec P^\pi \vec V^\pi \\
%% \vec V^\pi + \g \vec P^\pi \vec V^\pi &= \vec R^\pi \\
%% (\vec I - \g \vec P^\pi) \vec V^\pi &= \vec R^\pi \\
$\vec V^\pi = (\vec I - \g \vec P^\pi)^{-1} \vec R^\pi$,
which is $O(|S|^3)$.

~

~

\item \textbf{Policy Iteration} uses $V^\pi$ to incrementally improve
the policy:
\begin{enumerate}
\item Initialise $\pi_0$ somehow (e.g.\ randomly)

\item Iterate:

\begin{items}
\item \textbf{Policy Evaluation:} compute $V^{\pi_k}$ or $Q^{\pi_k}$

\item \textbf{Policy Update:} $\pi_{k+1}(s) \gets \argmax_a Q^{\pi_k}(s,a)$
\end{items}
\end{enumerate}

~

{\hfill\tiny\texttt{demo: test/mdp runPI}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Summary: Bellman equations}{

\item Discounted infinite horizon:
\begin{align*}
V^*(s)
&= \max_a Q^*(s,a) 
 = \max_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \] \\
Q^*(s,a)
&= R(s,a) + \g \sum_{s'} P(s'\|s,a)~ \max_{a'} Q^*(s',a')
\end{align*}

~

\item With finite horizon $T$ (non stationary MDP), initializing $V_{T\po}(s)=0$
\begin{align*}
V^*_t(s)
&= \max_a Q^*_t(s,a) 
 = \max_a \[R_t(s,a) + \g \tsum_{s'} P_t(s'\|s,a)~ V^*_{t\po}(s') \] \\
Q^*_t(s,a)
&= R_t(s,a) + \g \sum_{s'} P_t(s'\|s,a)~ \max_{a'} Q^*_{t\po}(s',a')
\end{align*}

~

\item This recursive computation of the value functions is a form
of \textbf{Dynamic Programming}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Comments \& relations}{

\item Tree search is a form of \textbf{forward} search, where heuristics
($A^*$ or UCB) may optimistically estimate the value-to-go

\item Dynamic Programming is a form of \textbf{backward} inference,
which exactly computes the value-to-go backward from a horizon

\item UCT also estimates $Q(s,a)$, but based on Monte-Carlo rollouts
instead of exact Dynamic Programming

~

\item In deterministic worlds, Value Iteration is the same as
\emph{Dijkstra backward}; it labels all nodes with the value-to-go
($\oto$ cost-to-go).

~

\item In \emph{control theory}, the Bellman equation is formulated for
continuous state $x$ and continuous time $t$ and ends-up:
$$
-\frac{\del}{\del t} V(x,t)
 = \min_u \[c(x, u) + \frac{\del V}{\del x} f(x,u) \]
$$
which is called \emph{Hamilton-Jacobi-Bellman} equation.

For linear quadratic systems, this becomes the \emph{Riccati equation}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Comments \& relations}{

\item The Dynamic Programming principle is applicable in any
domain -- but inefficient if the state space is large (e.g.\ relational or
high-dimensional continuous)

\item It requires iteratively computing a value function over the whole
state space

%% \item If this is feasible, we get optimal policies

%% ~

%% ~

%% \tiny
%% \cen{
%% \begin{tabular}{|c|c|c||c|c|c||c||c|}
%% \hline
%% table & PDDL & NID & MDP & POMDP & DEC-POMDP & Games & control \\
%% \hline
%% y & y & y & y & y &  &  & y \\
%% \hline
%% \end{tabular}
%% }

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conclusions}{\label{lastpage}

\item We covered two basic types of planning methods
\begin{items}
\item Tree Search: forward, but with backward heuristics
\item Dynamic Programming: backward
\end{items}

~

\item Dynamic Programming explicitly describes optimal policies. Exact
DP is computationally heavy in large domains $\to$ approximate DP

Tree Search became very popular in large domains, esp.\ MCTS using UCB
as heuristic

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot


%% However, prior Monte-Carlo planners have been limited to fixed horizon, depth-first search
%% [7] (also known as sparse sampling),


%% Decision theory = probability theory + utility theory .

%% The fundamental idea of decision theory is that an agent is rational
%% if and only if it chooses the action that yields the highest expected
%% utility, averaged over all the possible outcomes of the action.

%% Maximization of expected utility
