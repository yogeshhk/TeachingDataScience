\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\exnum}{4}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Programmieraufgabe: ~ Sarsa \& Q-Learning}

Consider the following \textit{Cliff Walking problem}.

\begin{figure}[ht]
	\centering
        \includegraphics[width=0.7\textwidth]{cliffwalk_graph.png}
        \caption{Cliffwalk environment and reference plot (smoothed)}
\end{figure}

\noindent
In your git repo you find an implementation of the \emph{Cliff
  Walking} environment. This is a standard undiscounted ($\gamma =
1$), episodic task, with start (S) and goal (G) states, and 
actions \texttt{up, down, left, right} that cause deterministic movement.
The reward is -1 for states except those in the region marked
\textit{The Cliff}. States in this region have a reward of -100; additionally the agent is instantly set back to the start state. An episode ends when
reaching the goal state and NOT when falling down the cliff.

Recall: See slide 06:12 for pseudo code of Q-Learning; it updates the Q-function using
$$
Q_\new(s,a) \gets Q_\old(s,a)
 + \a~ [r + \g \max_{a'} Q_\old(s',a') - Q_\old(s,a)] ~.$$
SARSA is exactly the same algorithm, except that it updates the Q-function with
$$Q_\new(s,a) \gets Q_\old(s,a)
 + \a~ [r + \g Q_\old(s',a') - Q_\old(s,a)] ~,$$
where $a'$ is the action actually taken by the current policy. To implement this SARSA update, you need to sample the next action $a'$ before updating the Q-function.

\begin{enumerate}
\item Implement the SARSA and Q-learning methods using the
  $\epsilon$-greedy action selection strategy with a fixed $\epsilon=
  0.1$. Choose a small learning rate $\alpha = 0.1$. Compare the
  resulting policies when greedily selecting actions based on the
  learned Q-tables.
        
  To compare the agents' online performance run them for at
  least 500 episodes and log the \textit{reward per episode} in a
  numpy array. Plot your logged data with
  \textit{episodes} as x-axis and \textit{reward per episode} as
  y-axis. Export the logged reward array for Q-Leaning and Sarsa to
  \textit{R\_ql.csv} and \textit{R\_sa.csv respectively}. See below on
  how to plot using python.
    
\item Propose a schedule that gradually reduces $\epsilon$, starting
  from $\epsilon = 1$ (e.g., multiplicative).  Then redo question 1
  using your schedule for $\epsilon$ instead of the fixed value. Again
  plot the learning graph and export your logged rewards per episode
  to \textit{R\_ql\_sched.csv} and \textit{R\_sa\_sched.csv}
  respectively.

\item {} [Note: this doesn't make much sense: model-free RL has to fully relearn the Q-function when the reward function changes. Model-based RL would make sense. Or perhaps using a replay buffer.] In the lectures we introduced Rmax, which is a
  \emph{model-based} approach. Here we consider a simplified
  model-free Rmax-variant working with Q-Learning: Implement standard
  Q-learning using greedy action selection but a modified reward
  function. The modified reward function assigns $r_{max} = 0$ to
  unknown states ($\#(s,a)<100$) and the original reward for known
  states. Plot and export your rewards per episode
  to \textit{R\_rmax.csv}.
\end{enumerate}

{\bf
Be sure to upload your code and all your csv files containing the
rewards per episode. The evaluation is based on these files.

For each exercise get an understanding for the agent's online
behavior and their learned policies. Be ready to explain in class!

}

%\subsection{How to plot and export}

The following example shows how to plot and export data contained in a
numpy array. This is all you need to
create the plots and generate the csv files for the above exercises.

\begin{shaded}
\begin{verbatim}
import numpy as np
import matplotlib.pyplot as plt

Y = np.array([5, 8, 1, 4])

# Export to CSV
np.savetxt(Y, 'Y.csv')

# Plot and display
plt.plot(Y)
plt.show()
\end{verbatim}
\end{shaded}

Note: The graph may be very spiky so it might be a good idea to smooth your plot before comparing it with the graph given above, e.g. by using the simple box filter provided in the code. However you have to export and hand in the un-smoothed version.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Votieraufgabe: ~ Eligibilities in TD-learning}

Consider TD-learning in the same maze as in the previous exercise 1
(Value Iteration), where the agent starts in state $4$, and action
outcomes are probabilistic.
\begin{itemize}
\item Describe at what events plain TD-learning
will update the value function, how it will update it.
\item Assuming the
agent always decides for the clock-wise action (but outcomes are probabilistic), guess roughly how many steps
the agent will have taken when for the first time $V(s_4)$ becomes
non-zero.
\item How would this be different for eligibility traces?
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exerfoot
