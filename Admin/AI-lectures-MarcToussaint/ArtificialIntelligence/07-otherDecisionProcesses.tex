\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Beyond Plain MDPs}
\renewcommand{\keywords}{Notion of state, taxonomy of domains, PDDL,
Noisy Deictic Rules, MDPs, POMDPs, basic agent models, DEC-POMDPs,
Multi-armed bandits}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

We discussed dynamic programming and reinforcement learning in
standard MDPs. There are some extensions which address relational
state representations, multi-agent situations, partial observability
and continuous time. We address this briefly.

Especially partial observability is an very important aspect, as any
real-world decision process is necessarily partially observable, and
it provides a potential bridge to think of perception as being
fundamentally part of planning.  The respective model is called POMDP
(partially observable MDP). We'll discuss the basic approach to
address such problems, namely belief space planning, and emphasize the
hardness of such problems.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Overview of common domain models}{

~

\tiny
\begin{tabular}{|p{40mm}||c|c|c|c|c|}
\hline
model / state representation & prob.\ & rel.\ & multi & PO & cont.time \\
\hline
plain deterministic & - & - & - & - & - \\
plain MDP & + & - & - & - & - \\
PDDL (STRIPS rules) & - (+) & + & - (+) & - & - \\
NDRs & + & + & - (+) & - & - \\
relational MDP & + & + & - & - & - \\
\hline
POMDP & + & - & - & + & - \\
DEC-POMDP & + & - & + & + & - \\
\hline
Games & - & + & + & - & - \\
\hline
stochastic diff.~eqns.\ (SOC) & + & - &   & + & + \\
\hline
\end{tabular}

~

\ttiny

~

[probabilistic, relational, multi-agent, partially observable, continuous time]

~

PDDL: Planning Domain Definition Language, STRIPS: STanford Research
Institute Problem Solver, NDRs: Noisy Deictic Rules, MDP:
Markov Decision Process, POMDP: Partially Observable MDP, DEC-POMDP:
Decentralized POMDP, SOC: Stochastic Optimal Control

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Relational State Representations}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Types of state representation:
\begin{items}
\item Discrete, continuous, hybrid
\item Factored
\item Structured/relational
\end{items}

~

\show[.6]{rn-factoredStateRepresentation}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Relational representations of state}{

\item The world is composed of objects; its state described in terms
of properties and relations of objects. Formally
\begin{items}
\item A set of constants (referring to objects)
\item A set of predicates (referring to object properties or
relations)
\item A set of functions (mapping to constants)
\end{items}

~

\item A (grounded) state can then described by a conjunction of predicates
(and functions). For example:
\begin{items}
\item Constants: $C_1, C_2, P_1, P_2, SFO, JFK$
\item Predicates: $At(.,.), Cargo(.), Plane(.), Airport(.)$
\item A state description:

$At(C_1, SFO) \wedge At(C_2, JFK) \wedge  At(P_1, SFO) \wedge  At(P_2, JFK)
\wedge Cargo(C_1) \wedge Cargo(C_2) \wedge Plane(P_1) \wedge  Plane(P_2)
\wedge Airport (JFK) \wedge Airport (SFO)$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Taxonomy of domains II}{

%% \item Domains (or models of domains) can additionally be
%% distinguished based on:

%% ~

%% \item Categories of Russel \& Norvig:
%% \begin{items}
%% \item Fully observable vs.\ partially observable
%% \item Single agent vs.\ multiagent
%% \item Deterministic vs.\ stochastic
%% \item Known vs.\ unknown
%% {\color{gray}
%% \item Episodic vs.\ sequential
%% \item Static vs.\ dynamic
%% \item Discrete vs.\ continuous
%% }
%% \end{items}

%% ~

%% \item We add:
%% \begin{items}
%% \item Time discrete vs.\ time continuous
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \show{rn-environments}
%% \tiny\hfill (from Russel \& Norvig)

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{PDDL}
\slide{PDDL}{

\item Planning Domain Definition Language

Developed for the 1998/2000 International Planning Competition (IPC)

\show[.7]{rn-PDDL1}
{\tiny\hfill (from Russel \& Norvig)}


\item PDDL describes a deterministic mapping $(s,a) \mapsto s'$,
but
\begin{items}
\item using a set of action schema (rules) of the form

\cen{ActionName$(...): ~$ PRECONDITION $\to$ EFFECT}

\item where action arguments are variables and the preconditions and
effects are conjunctions of predicates
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PDDL}{

%\show{rn-PDDL2}
\show{rn-PDDL3}
\show{rn-PDDL4}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PDDL}{

\item The state-of-the-art solvers are actually \Astar methods. But the heuristics!!

\item Scale to huge domains

\item Fast-Downward is great

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Noisy Deictic Rules}
\slide{Noisy Deictic Rules ~ (NDRs)}{

\item Noisy Deictic Rules (Pasula, Zettlemoyer, \&
Kaelbling, 2007)

\item A probabilistic extension of ``PDDL rules'':

\show[.5]{logo2}

~

\item These rules define a \emph{probabilistic transition probability}
$$P(s'|s,a)$$

\tiny
Namely, if $(s,a)$ has a unique covering rule $r$, then
$$P(s'|s, a) = P(s'|s, r) = \sum_{i=0}^{m_r} p_{r,i}~ P(s'|\O_{r,i}, s)$$
where $P(s'|\O_{r,i}, s)$ describes the deterministic state transition
of the $i$th outcome (see Lang \& Toussaint, JAIR 2010).

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Examples}{

%% \hspace*{-10mm}\twocol{.5}{.5}{
%% \center
%% Random exploration:\\
%% \moviex{\show[.6]{09-lang-NIDDBN/robot}}{movies/09-tobias/film_randomActions.avi}

%% ~

%% Planning:\\
%% \moviex{\show[.6]{09-lang-ecml-rg/img/robot_clearance}}{movies/09-tobias/film_exp3.avi}

%% }{\center

%% Real-world:\\
%% \moviex{\show[.6]{robi}}{movies/09-robi/ICRAmovie.avi}

%%  ~

%% Online explore-exploit:\\
%% \moviex{\show[.6]{10-lang-ecml/img/grobi}}{movies/10-relationalExploration/movie_clearance_big.avi}

%% }

%% \tiny

%% Lang \& Toussaint: Planning with Noisy Probabilistic Relational Rules
%% (JAIR 2010)

%% Toussaint et al: Integrated motor control, planning, grasping and
%% high-level reasoning in a blocks world using probabilistic inference
%% (ICRA 2010)

%% Lang, Toussaint \& Kersting: Exploration in Relational Worlds (ECML
%% 2010)

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item While such rule based domain models originated from classical AI
research, the following were strongly influenced also from stochastics,
decision theory, Machine Learning, etc..

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{POMDP}
\sublecture{Partially Observable MDPs}{
}
%% \show[.5]{pomdp_agent}

%% \begin{items}
%% \item initial state distribution $P(s_0)$
%% \item transition probabilities $P(s' | s,a)$
%% \item observation probabilities $P(y' | s',a)$
%% \item reward probabilities $P(r | s,a)$
%% \end{items}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{``Systems that take decisions based on available information''}{

~\anchor{220,-90}{\showh[.3]{RLagenAndEnvironment}}

\item We assume the agent is in \emph{interaction} with a domain.
\begin{items}
\item The world is in a state $s_t\in\SS$
\item The agent senses observations $y_t\in\OO$
\item The agent decides on an action $a_t\in\AA$
\item The world transitions in a new state $s_{t\po}$
\end{items}

\show[.6]{markov}

~

\item Generally, an agent maps the history to an action, $h_t =
(y_{0:t},a_{0:t\1}) \mapsto a_t$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{POMDPs}{

\item Partial observability adds a totally new level of complexity!

\item Basic alternative agent models:
\begin{items}
\item The agent maps $y_t \mapsto a_t$ \\ (\textbf{stimulus-response} mapping.. non-optimal)
\item The agent stores all previous observations and maps
$$f:~ y_{0:t},a_{0:t\1} \mapsto a_t$$
$f$ is called \textbf{agent function}. This is the most general model,
including the others as special cases.
\item The agent stores only the recent history and maps\\
$y_{t-k:t},a_{t-k:t\1} \mapsto a_t$ (crude, but may be a good heuristic)
\item The agent is some machine with its own \textbf{internal state} $n_t$,
e.g., a computer, a finite state machine, a brain...
The agent maps $(n_{t\1},y_t) \mapsto n_t$ (internal state update) and
$n_t \mapsto a_t$
\item The agent maintains a full probability distribution
(\textbf{belief}) $b_t(s_t)$ over the state, maps
$(b_{t\1},y_t) \mapsto b_t$ (Bayesian belief update), and
$b_t \mapsto a_t$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{POMDP coupled to a state machine agent}{

~

\show[.5]{pomdp_fsm}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\movc{1}{10-RoboticsLecture/2007-The_Robot_Champion_of_DARPA_s_Urban_Challenge}


{\hfill\tiny\url{http://www.darpa.mil/grandchallenge/index.asp}}

%% \mov{DARPA Grand Urban Challenge
%% 2007}{}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item The tiger problem: a typical POMDP example:

~

\show[1]{tiger-problem}
{\hfill\tiny (from the a ``POMDP tutorial'')}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Solution via Dynamic Programming in Belief Space}{

\cen{\showh[.4]{figs/pomdp1}\quad
\showh[.4]{figs/pomdp3b}}

\item \textbf{Consider the \emph{belief} as state of the Markov(!) decision process!}

The value function is a function over the belief ~ ($\d \sim$ belief update)

$$V(b)
 = \max_a \[ R(b,a) + \g \Sum_{y} \int_{b'} P(y'|a,b)~ \d(b'|y',a,b)~ V(b')\]
$$

~

\item Sondik 1971: $V$ is piece-wise linear and convex: Can be described by $m$ vectors
$(\a_1,..,\a_m)$, each $\a_i=\a_i(s)$ is a function over discrete $s$

$$V(b) = \max_i \Sum_s \a_i(s) b(s)$$

Exact dynamic programming possible, see Pineau et al., 2003

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Truely Optimal Policies}{

\item The value function assigns a value (maximal achievable expected return)
  to a state of knowledge

%% \item The optimal policy is greedy w.r.t.\ the value function (in the sense of the $\max_{a_t}$ above)

\item Optimal policies optimally ``navigate through belief space''
\begin{items}
\item This automatically implies/combines ``exploration'' and ``exploitation''
\item There is no need to explicitly address ``exploration vs.\
exploitation'' or decide for one against the other. Optimal policies
will automatically do this.
\end{items}

\item Computationally heavy: $b_t$ is a probability distribution, $V_t$ a
  function over probability distributions

~

\item See appendix on bandits to make equations more explicit

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Approximations \& Heuristics}{

\item Point-based Value Iteration (Pineau et al., 2003)

-- Compute $V(b)$ only for a finite set of belief points

~

\item Discard the idea of using belief to ``aggregate'' history

-- Policy directly maps history (window) to actions

-- Optimize finite state controllers (Meuleau et al. 1999, Toussaint et al.\ 2008)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Further reading}{

\item \emph{Point-based value iteration: An anytime algorithm for
POMDPs.} Pineau, Gordon \& Thrun,
IJCAI 2003.

\item The standard references on the ``POMDP page'' {\small\url{http://www.cassandra.org/pomdp/}}

\item \emph{Bounded finite state
controllers.} Poupart \& Boutilier, NIPS 2003.

\item \emph{Hierarchical POMDP Controller Optimization by Likelihood
Maximization.} Toussaint, Charlin \& Poupart, UAI 2008.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \item Can you think of a (single agent) problem that is not a POMDP?

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Dec-POMDP}
\slide{Decentralized POMDPs}{

\item Finally going multi agent!

\show[.4]{DECpomdp}

{\tiny\hfill (from Kumar et al., IJCAI 2011)}

~

\item This is a special type (simplification) of a general DEC-POMDP

~

\item Generally, this level of description is very general, but
NEXP-hard

Approximate methods can yield very good results, though

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Belief Space Planning for Bandits**}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Back to the Bandits}{

\item Can Dynamic Programming also be applied to the Bandit problem?

We learnt UCB as the standard approach to address Bandits -- but what
would be the optimal policy?

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Bandits recap}{

\item Let $a_t\in\{1,..,n\}$ be the choice of machine at time $t$

Let $y_t\in\RRR$ be the outcome with mean $\< y_{a_t} \>$

A policy or strategy maps all the history to a new choice:
$$ \pi:~ [ (a_1,y_1), (a_2,y_2), ..., (a_{t\1},y_{t\1}) ] \mapsto a_t$$

~

\item Problem: ~ Find a policy $\pi$ that
$$\max \< \Sum_{t=1}^T y_t \>$$
or
$$\max \< y_T \>$$

~

\item ``Two effects'' of choosing a machine:

-- You collect more data about the machine $\to$ knowledge

-- You collect reward

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The Belief State}{

\item ``Knowledge'' can be represented in two ways:
\begin{items}
\item as the full history
$$ h_t ~=~ [ (a_1,y_1), (a_2,y_2), ..., (a_{t\1},y_{t\1}) ]$$

\item as the \textbf{belief}
$$ b_t(\t) = P(\t | h_t)$$
where $\t$ are the unknown parameters $\t=(\t_1,..,\t_n)$ of all machines
\end{items}

~

\item In the bandit case:
\begin{items}
\item The belief factorizes
$b_t(\t) = P(\t|h_t) = \prod_i b_t(\t_i|h_t)$

e.g.\ for Gaussian bandits with constant noise, $\t_i = \mu_i$
\begin{align*}
b_t(\mu_i|h_t) = \NN(\mu_i | \hat y_i, \hat s_i)
\end{align*}

e.g.\ for binary bandits, $\t_i = p_i$, with prior $\Beta(p_i|\a, \b)$:
\begin{align*}
b_t(p_i|h_t) &= \Beta(p_i | \a+a_{i,t}, \b+b_{i,t}) \\
&\quad  a_{i,t} = \Sum_{s=1}^{t-1} [a_s\=i][y_s\=0]
 \comma b_{i,t} = \Sum_{s=1}^{t-1} [a_s\=i][y_s\=1]
\end{align*}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The Belief MDP}{

\item The process can be modelled as 

\show[.5]{bandit1}

or as Belief MDP

\show[.5]{bandit2}
{\small$$
P(b'|y,a,b)
 = \begin{cases} 1 & \text{if $b' = b'_{[b,a,y]}$} \\ 0 & \text{otherwise} \end{cases}
\comma
P(y|a,b)
 = \Int_{\t_a} b(\t_a)~ P(y | \t_a)
$$}

{\small
\item The Belief MDP describes a \emph{different} process: the
interaction between the information available to the agent ($b_t$ or $h_t$)
and its actions, where \emph{the agent uses his current belief to
anticipate observations, $P(y|a,b)$.}


\item The belief (or history $h_t$) is all the information the agent
has avaiable; $P(y|a,b)$ the ``best'' possible anticipation of
observations. If it acts optimally in the Belief MDP, it acts
optimally in the original problem.

}

\emph{Optimality in the Belief MDP $~\To~$ optimality in the original problem}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimal policies via Dynamic Programming in Belief Space}{

\item The Belief MDP:

\show[.5]{bandit2}
{\small$$
P(b'|y,a,b)
 = \begin{cases} 1 & \text{if $b' = b'_{[b,a,y]}$} \\ 0 & \text{otherwise} \end{cases}
\comma
P(y|a,b)
 = \Int_{\t_a} b(\t_a)~ P(y | \t_a)
$$}

%% \item The belief is a sufficient (for optimal decision making)
%%   ``aggregation'' of the history if the world is Markovian in $X$:

%% $$\text{Indep}(y_s, h_t | X_t, a_{t\po}) \qquad \text{for any $s\ge t$ and $\pi$}$$


\item Belief Planning: Dynamic Programming on the value function
\begin{align*}
\forall_b:~ V_{t\1}(b)
 &= \max_\pi \< \Sum_{t=t}^T y_t \> \\
 &= \max_{a_t} \Int_{y_t} P(y_t|a_t,b)~ \[y_t + V_t(b'_{[b,a_t,y_t]})\]
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{
\tiny
\vspace*{-10mm}\begin{align}
V_t^*(h)
&:= \max_\pi \int_\t P(\t|h)~ V^{\pi,\t}_t(h) \\
V^\pi_t(b)
&:= \int_\t b(\t)~ V^{\pi,\t}_t(b) \\
V_t^*(b)
&:= \max_\pi V^\pi_t(b) = \max_\pi \int_\t b(\t)~ V^{\pi,\t}_t(b) \\
&= \max_\pi \int_\t P(\t|b)~ \[ R(\pi(b),b) + \int_{b'} P(b'|b,\pi(b),\t)~ V^{\pi,\t}_{t\po}(b') \] \\
&= \max_a \max_\pi \int_\t P(\t|b)~ \[ R(a,b) + \int_{b'} P(b'|b,a,\t)~ V^{\pi,\t}_{t\po}(b') \] \\
&= \max_a \[ R(a,b) + \max_\pi \int_\t \int_{b'} P(\t|b)~ P(b'|b,a,\t)~ V^{\pi,\t}_{t\po}(b') \] \\
P(b'|b,a,\t)
&= \int_y P(b',y|b,a,\t) \\
&= \int_y \frac{P(\t|b,a,b',y)~ P(b',y|b,a)}{P(\t|b,a)} \\
&= \int_y \frac{b'(\t)~ P(b',y|b,a)}{b(\t)} \\
V_t^*(b)
&= \max_a \[ R(a,b) + \max_\pi \int_\t \int_{b'} \int_y b(\t)~ \frac{b'(\t)~ P(b',y|b,a)}{b(\t)}~ V^{\pi,\t}_{t\po}(b') \] \\
&= \max_a \[ R(a,b) + \max_\pi \int_{b'} \int_y P(b',y|b,a) \int_\t b'(\t)~ V^{\pi,\t}_{t\po}(b') \] \\
&= \max_a \[ R(a,b) + \max_\pi \int_y P(y|b,a) \int_\t b'_{[b,a,y]}(\t)~ V^{\pi,\t}_{t\po}(b'_{[b,a,y]}) \] \\
&= \max_a \[ R(a,b) + \max_\pi \int_y P(y|b,a)~ V^\pi(b'_{[b,a,y]}) \] \\
&= \max_a \[ R(a,b) + \int_y P(y|b,a)~ \max_\pi V^\pi(b'_{[b,a,y]}) \] \\
&= \max_a \[ R(a,b) + \int_y P(y|b,a)~ V^*_{t\po}(b'_{[b,a,y]}) \]
\end{align}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimal policies}{

\item The value function assigns a value (maximal achievable expected return)
  to a state of knowledge

%% \item The optimal policy is greedy w.r.t.\ the value function (in the sense of the $\max_{a_t}$ above)

\item While UCB approximates the value of an action by an optimistic estimate of
immediate return; Belief Planning acknowledges that this really is a
sequencial decision problem that requires to plan

\item Optimal policies ``navigate through belief space''
\begin{items}
\item This automatically implies/combines ``exploration'' and ``exploitation''
\item There is no need to explicitly address ``exploration vs.\
exploitation'' or decide for one against the other. Optimal policies
will automatically do this.
\end{items}

\item Computationally heavy: $b_t$ is a probability distribution, $V_t$ a
  function over probability distributions

~

\tiny

\item The term $\Int_{y_t} P(y_t|a_t,b)~ \[y_t +
V_t(b'_{[b,a_t,y_t]})\]$ is related to the \emph{Gittins Index}: it
can be computed for each bandit separately.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example exercise}{

\item Consider 3 binary bandits for $T=10$.

\begin{items}
\item The belief is 3 Beta distributions $\Beta(p_i|\a+a_i,\b+b_i)$ ~ $\to$ ~ 6 integers

\item $T=10$ ~ $\to$ ~ each integer $\le10$

\item $V_t(b_t)$ is a function over $\{0,..,10\}^6$
\end{items}

~

\item Given a prior $\a=\b=1$, 

a) compute the optimal value function and policy for the final
  reward and the average reward problems,

b) compare with the UCB policy.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item The concept of Belief Planning transfers to other uncertain
domains: Whenever decisions influence also the state of knowledge
\begin{items}
\item Active Learning
\item Optimization
\item Reinforcement Learning (MDPs with unknown environment)
\item POMDPs
\end{items}

~

\item Planning in Belief Space is fundamental 
\begin{items}
\item Describes optimal solutions to Bandits, POMDPs, RL, etc
\item But computationally heavy
\item Silver's MCTS for POMDPs annotates nodes with history and belief
representatives
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Relation to Stochastic Optimal Control**}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Control}
\slide{Controlled System}{

\item Time is continuous, $t\in\RRR$

\item The system state, actions and observations are continuous,
$x(t)\in\RRR^n, u(t)\in\RRR^d, y(t)\in\RRR^m$

~

\item A controlled system can be described as

\medskip

\twocol[.1]{.4}{.4}{
linear:\vspace*{-3mm}
\begin{align*}
\dot x
&= A x + B u \\
y
&= C x + D u
\end{align*}
with matrices $A,B,C,D$
}{
non-linear:\vspace*{-3mm}
\begin{align*}
\dot x
&= f(x,u) \\
y
&= h(x,u)
\end{align*}
with functions $f, h$
}

~

~

\item A typical ``agent model'' is a \emph{feedback regulator} ~ (stimulus-response)
$$u = K y$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Stochastic Control}{\label{lastpage}

\item The differential equations become stochastic
\begin{align*}
dx
&= f(x,u)~ dt + d\xi_x\\
dy
&= h(x,u)~ dt + d\xi_y
\end{align*}
$d\xi$ is a Wiener processes with $\<d\xi,d\xi\> = C_{ij}(x, u)$

~

\item This is the control theory analogue to POMDPs

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
