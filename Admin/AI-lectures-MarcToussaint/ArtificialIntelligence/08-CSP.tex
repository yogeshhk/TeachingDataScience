\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Constraint Satisfaction Problems}
\renewcommand{\keywords}{}

\slides[(slides based on Stuart Russell's AI course)]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

Here is a little cut in the lecture series. Instead of focussing on
sequential decision problems we turn to problems where there exist
many coupled variables. The problem is to find values (or, later,
probability distributions) for these variables that are consistent
with their coupling. This is such a generic problem setting that it
applies to many problems, not only map colouring and sudoku. In fact,
many computational problems can be reduced to Constraint Satisfaction
Problems or their probabilistic analogue, Probabilistic Graphical
Models. This also includes sequential decision problems, as I
mentioned in some extra lecture. Further, the methods used to solve
CSPs are very closely related to descrete optimization.

From my perspective, the main motivation to introduce CSPs is as a
precursor to introduce their probabilistic version, graphical
models. These are a central language to formulate probabilistic models
in Machine Learning, Robotics, AI, etc. Markov Decision Processes,
Hidden Markov Models, and many other problem settings we can't discuss
in this lecture are special cases of graphical models. In both
settings, CSPs and graphical models, the core is to understand what it
means to do inference. Tree search, constraint propagation and belief
propagation are the most important methods in this context.

In this lecture we first define the CSP problem, then introduce basic
methods: sequential assignment with some heuristics, backtracking, and
constraint propagation.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Problem Formulation \& Examples}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Inference}
\slide{Inference}{

\item The core topic of the following lectures is
\begin{quote}
\textbf{Inference:} Given some pieces of information on some things
(\emph{observed variabes}, prior, knowledge base) what is the implication (the implied
information, the posterior) on other things (\emph{non-observed variables},
sentence)
\end{quote}

\item Decision-Making and Learning can be viewed as Inference:
\begin{items}
\item given pieces of information: ~ about the world/game, collected
data, assumed model class, \emph{prior} over model parameters

\item make decisions about actions, classifier, model parameters, etc
\end{items}

\item In this lecture:
\begin{items}
\item ``Deterministic'' inference in CSPs
\item Probabilistic inference in graphical models
variabels)
\item Logic inference in propositional \& FO logic
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Constraint satisfaction problems (CSPs): Definition}
\slide{Constraint satisfaction problems (CSPs)}{

\item In previous lectures we considered sequential decision problems

CSPs are \emph{not} sequential decision problems.
However, the basic methods address them by sequentially making 
partial decisions

~

\item CSP:
\begin{items}
\item We have $n$ \defn{variables} $x_i$, each with
\defn{domain} $D_i$,~ $x_i \in D_i$
\item We have $K$ \defn{constraints} $C_k$, each of which
determines the feasible configurations of a subset of variables
\item The goal is to find a configuration $X=(X_1,..,X_n)$ of all
variables that satisfies all constraints
\end{items}

\item Formally, a constraint is defined as $C_k = (I_k,c_k)$, where $I_k\subseteq\{1,..,n\}$ determines the
subset of variables, and $c_k: D_{I_k} \to \{0,1\}$ determines
whether a configuration $x_{I_k}\in D_{I_k}$ of this subset of variables is feasible

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Constraint graph}{

%\item \defn{Pair-wise CSP}: each constraint relates at most two variables

\item A CSP corresponds to a \emph{constraint graph}:

A \defn{constraint graph} is a \emph{bi-partite graph} where nodes are variables and boxes are constraints

Constraints may constrain several (or one) variables
($|I_k|\not=2$)

~

\show[0.35]{mapColoring.pdf}

%% General-purpose CSP algorithms use the graph structure\\
%% to speed up search. E.g., Tasmania is an independent subproblem!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Map-Coloring Problem}
\slide{Example: Map-Coloring}{

\show[0.5]{russell/australia.pdf}

\item Variables: $W$, $N$, $Q$, $E$,
$V$, $S$, $T$ \quad ($E$ = New South Wales)

\item Domains: $D_i = \{red,green,blue\}$ for all variables

\item Constraints: adjacent regions must have different colors, e.g., $W\neq N$,

\cen{$(W,N) \in \{(red,green),(red,blue),(green,red),(green,blue),\ldots\}$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Map-Coloring contd.}{

\show[0.5]{russell/australia-solution.pdf}

~

\item \emph{Solutions} are assignments satisfying all constraints, e.g.,

\cen{$\{W = red,N = green,Q = red,E = green,V = red,S = blue,T = green\}$}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Varieties of CSPs}{

\item Discrete variables:
  finite domains; each $D_i$ of size $|D_i|=d$
  $\implies$ $O(d^n)$ complete assignments
\begin{items}
    \item e.g., Boolean CSPs, incl.~Boolean satisfiability
  infinite domains (integers, strings, etc.)
    \item e.g., job scheduling, variables are start/end days for each job
    \item linear constraints solvable, nonlinear
  undecidable
\end{items}

\item Continuous variables
\begin{items}
    \item e.g., start/end times for Hubble Telescope observations
    \item linear constraints solvable in poly time by LP methods
\end{items}

\item Real-world examples
\begin{items}
\item Assignment problems, e.g.\ who teaches what class?
\item Timetabling problems, e.g.\ which class is offered when and where?
\item Hardware configuration
\item Transportation/Factory scheduling
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Varieties of constraints}{

\defn{Unary} constraints involve a single variable, $|I_k|=1$ \\ \quad
   e.g., $S\neq green$

\defn{Pair-wise} constraints involve pairs of variables, $|I_k|=2$ \\ \quad
   e.g., $S\neq W$

\defn{Higher-order} constraints involve 3 or more variables, $|I_k|>2$\\ \quad
   e.g., Sudoku

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Methods for solving CSPs}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Backtracking}
\slide{Sequential assignment approach}{

\item Let's start with the straightforward but inefficient approach, then fix it

~

\item We assign variable values \textbf{sequentially}; the solver state is defined by the values assigned so far. The solver's decision process is defined by:

\begin{items}
\item Initial state: the empty assignment, $\emptyset$

\item Successor function: assign a value to an unassigned variable
that does not conflict with current assignment
$\implies$ fail if no feasible assignments (not fixable!)

\item Goal test: the current assignment is complete
\end{items}

~

\item Every solution appears at depth $n$ with $n$ variables
   $\implies$ use depth-first search\\

\item $b = (n-\ell)d$ at depth $\ell$, hence $n!d^n$ leaves!

%3) Path is irrelevant, so can also use complete-state formulation\\

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Backtracking sequential assignment}{

\item Two variable assignment decisions are \defn{commutative}, i.e.,\\
 [$W = red$ then $N = green$] ~ same as ~ [$N = green$ then $W = red$]

\item We can fix a single next variable to assign a value to at
each node! This drastically reduces the branching factor of the search tree.

\item This does not compromise completeness (ability to find the solution)\\
   $\implies$ $b  = d$ and there are $d^n$ leaves\\

\item \emph{Depth-first search for CSPs with single-variable assignments
is called \textbf{backtracking} search}

\item Backtracking search is the basic uninformed algorithm for CSPs

~

Can solve $n$-queens for $n \approx 25$


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Backtracking search}{

%\input{algorithms/backtracking-search-algorithm}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Backtracking example}{

\only<1>{\show[.2]{russell/backtrack-progress1c}}
\only<2>{\show[.7]{russell/backtrack-progress2c}}
\only<3>{\show[0.8]{russell/backtrack-progress3c}}
\only<4>{\show[0.75]{russell/backtrack-progress4c}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Improving backtracking efficiency}{

Simple heuristics can give huge gains in speed:
\begin{enumerate}
\item Which variable should be assigned next?
\item In what order should its values be tried?
\item Can we detect inevitable failure early?
\item Can we take advantage of problem structure?
\end{enumerate}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Variable order: Minimum remaining values}
\slide{Variable order: Minimum remaining values}{

Minimum remaining values (MRV):\\ \quad
   choose the variable with the fewest legal values

\vspace*{0.3in}

\show[1.0]{russell/australia-most-constrained-variable.pdf}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Variable order: Degree heuristic}
\slide{Variable order: Degree heuristic}{

Tie-breaker among MRV variables

Degree heuristic:\\ \quad
   choose the variable with the most constraints on remaining variables

\vspace*{0.3in}

\show[1.0]{russell/australia-most-constraining-variable.pdf}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Value order: Least constraining value}
\slide{Value order: Least constraining value}{

Given a variable, choose the least constraining value:
   the one that rules out the fewest values in the remaining variables

\vspace*{0.3in}

\show[1.0]{russell/australia-least-constraining-value.pdf}
Combining these heuristics makes 1000 queens feasible




}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{Forward checking}
%% \slide{Forward checking}{

%% \emph{Idea}: Keep track of remaining legal values for unassigned variables\\
%% \phantom{\emph{Idea}: }Terminate search when any variable has no legal values

%% \vspace*{0.1in}

%% \only<1>{\show[0.9]{russell/forward-checking-progress1c}}


%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Forward checking}{

%% \emph{Idea}: Keep track of remaining legal values for unassigned variables\\
%% \phantom{\emph{Idea}: }Terminate search when any variable has no legal values

%% \vspace*{0.1in}

%% \only<1>{\show[0.9]{russell/forward-checking-progress2c}}


%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Forward checking}{

%% \emph{Idea}: Keep track of remaining legal values for unassigned variables\\
%% \phantom{\emph{Idea}: }Terminate search when any variable has no legal values

%% \vspace*{0.1in}

%% \only<1>{\show[0.9]{russell/forward-checking-progress3c}}


%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Forward checking}{

%% \emph{Idea}: Keep track of remaining legal values for unassigned variables\\
%% \phantom{\emph{Idea}: }Terminate search when any variable has no legal values

%% \vspace*{0.1in}

%% \only<1>{\show[0.9]{russell/forward-checking-progress4c}}

%% Use a data structure \prog{Domain[X]} to explicitly store $D_i$ for each node

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Constraint propagation}
\slide{Constraint propagation}{

\item After each decision (assigning a value to one variable) we can compute what are the remaining feasible
values for all variables.

\item Initially, every variable has the full domain $D_i$. Constraint
propagation reduces these domains, deleting entries that are
inconsistent with the new decision.

\item These dependencies are recursive: Deleting a value from the domain of
one variable might imply infeasibility of some value of another
variable $\to$ contraint \emph{propagation}. We update domains until
they're all consistent with the constraints.

~

\cen{\emph{This is Inference}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Constraint propagation}{

~

\item Example: quick failure detection after 2 decisions

\only<1>{\show[0.8]{russell/forward-checking-progress3c}}

$N$ and $S$ cannot both be blue!

\item Constraint Propagation: propagate the implied constraints serveral steps to reduce remaining domains and detect failures early.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Constraint propagation}{

\item Constraint propagation generally loops through the set of constraint, considers each constraint \emph{separately}, and deletes inconsistent values from its adjacent domains.

\item As it considers constraints separately, it does not compute a final solution, as backtracking search does.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Arc consistency (=constraint propagation for pair-wise constraints)}{

\item Simplest form of propagation makes each arc \defn{consistent}

\item $X\rightarrow Y$ is consistent iff\\ \quad
  for \emph{every} value $x$ of $X$ there is \emph{some} allowed $y$

\vspace*{0.1in}

\only<1>{\show[0.8]{russell/ac-example1c}}
\only<2>{\show[0.8]{russell/ac-example2c}}
\only<3>{\show[0.8]{russell/ac-example3c}}
\only<4>{\show[0.8]{russell/ac-example4c}}

\pause

\item If $X$ loses a value, neighbors of $X$ need to be rechecked

\pause

Arc consistency detects failure earlier than forward checking

Can be run as a preprocessor or after each assignment

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Arc consistency algorithm}{

%\input{algorithms/ac3-algorithm}

$O(n^2d^3)$, can be reduced to $O(n^2d^2)$
%(but detecting \emph{all} is NP-hard)


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Constraint propagation}{

\item Very closely related to \defn{message passing} in probabilistic models

~

\item In practice: design approximate constraint propagation for specific
problem\\
E.g.:\ Sudoku: If $X_i$ is assigned, delete this value from all peers\\


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Problem structure}{

~

%\vspace*{-0.4in}

\show[0.4]{mapColoring}

Tasmania and mainland are \defn{independent subproblems}

Identifiable as \defn{connected components} of constraint graph

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Problem structure contd.}{

%% Suppose each subproblem has $c$ variables out of $n$ total

%% Worst-case solution cost is $n/c \cdot d^c$, \emph{linear} in $n$

%% E.g., $n = 80$, $d = 2$, $c = 20$\\ \quad
%%   $2^{80}$ = 4 billion years at 10 million nodes/sec\\ \quad
%%   $4\cdot 2^{20}$ = 0.4 seconds at 10 million nodes/sec

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Tree-structured CSPs}
\slide{Tree-structured CSPs}{

\vspace*{0.3in}

\show[0.3]{russell/tree-csp1.pdf}
\emph{Theorem}: if the constraint graph has no loops, the CSP can be solved in 
$O(n\,d^2)$ time

Compare to general CSPs, where worst-case time is $O(d^n)$

~

This property also applies to logical and probabilistic reasoning!
%% an important example of the relation between syntactic restrictions\\
%% and the complexity of reasoning.



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Algorithm for tree-structured CSPs}{

\item[1.] Choose a variable as root, order variables from root to leaves\\
such that every node's parent precedes it in the ordering 

\vspace*{0.2in}

\show[0.8]{russell/tree-csp2.pdf}
\item[2.] For {$j$} from {$n$} down to {$2$}, apply
\cen{\texttt{RemoveInconsistent}{{$(Parent(X_j),X_j)$}}}

This is \textbf{backward constraint propagation}

~

\item[3.] For {$j$} from {$1$} to {$n$}, assign {$X_j$}
consistently with {$Parent(X_j)$}

This is \textbf{forward sequential assignment} (trivial backtracking)


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Nearly tree-structured CSPs}{

\defn{Conditioning}: instantiate a variable, prune its neighbors' domains

\vspace*{0.2in}

\show[0.75]{russell/australia-cutset.pdf}


\defn{Cutset conditioning}: instantiate (in all ways) a set of variables\\ 
such that the remaining constraint graph is a tree

Cutset size {$c$} $\implies$ runtime {$O(d^c\cdot (n-c)d^2)$}, very fast for small {$c$}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Iterative algorithms for CSPs}{

%% Hill-climbing, simulated annealing typically work with\\
%% ``complete'' states, i.e., all variables assigned

%% To apply to CSPs:\\ \quad
%% allow states with unsatisfied constraints\\ \quad
%% operators \emph{reassign} variable values

%% Variable selection: randomly select any conflicted variable

%% Value selection by \defn{min-conflicts} heuristic:\\ \quad
%% choose value that violates the fewest constraints\\ \quad
%% i.e., hillclimb with $h(n)$ = total number of violated constraints

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Example: 4-Queens}{

%% \emph{States}: 4 queens in 4 columns ($4^4 = 256$ states)

%% \emph{Operators}: move queen in column

%% \emph{Goal test}: no attacks

%% \emph{Evaluation}: $h(n)$ = number of attacks

%% \vspace*{0.3in}

%% \show[0.7]
%% \show{russell/4queens-iterative.pdf}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Performance of min-conflicts}{

%% Given random initial state, can solve $n$-queens in almost constant time for
%% arbitrary $n$ with high probability (e.g., $n$ = 10,000,000)

%% The same appears to be true for any randomly-generated CSP\\
%% \emph{except} in a narrow range of the ratio
%% \[
%% R = \frac{\mbox{number of constraints}}{\mbox{number of variables}}
%% \]

%% \vspace*{0.2in}

%% \show[0.6]
%% \show{russell/random-csp-runtime.pdf}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Summary}{\label{lastpage}

\item CSPs are a fundamental kind of problem:\\ \quad
finding a feasible configuration of $n$ variables\\ \quad
the set of \defn{constraints} defines the (graph) structure of the problem

\item Sequential assignment approach\\ \quad
\defn{Backtracking} = depth-first search with one variable assigned per node

\item Variable ordering and value selection heuristics help significantly

%Forward checking prevents assignments that guarantee later failure

\item Constraint propagation (e.g., arc consistency) does additional work
to constrain values and detect inconsistencies

\item The CSP representation allows analysis of problem structure

\item Tree-structured CSPs can be solved in linear time

If after assigning some variables, the remaining structure is a tree\\ \quad
$\to$ linear time feasibility check by tree CSP

%Iterative min-conflicts is usually effective in practice

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
