\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Reinforcement Learning}
\renewcommand{\keywords}{}

%\newcommand{\liter}{\helvetica{8}{1.1}{m}{n}\parskip 1ex}
\newcommand{\rmax}{{\textsc{R-max}}}
\newcommand{\ignore}[1]{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

Reinforcement Learning means to learn to perform well in an previously
unknown environment. So it naturally combines the problems of learning
about the environment and decision making to receive rewards. In that
sense, I think that the RL framework is a core of AI. (But one should
also not overstate this: standard RL solvers typically address limited
classes of MDPs---and therefore do not solve many other aspects AI.)

The notion of \emph{state} is central in the framework that underlies
Reinforcement Learning. One assumes that there is a `world state' and
decisions of the agent change the state. This process is formalized as
Markov Decision Process (MDP), stating that a new state may only
depend the previous state and decision. This formalization leads to a
rich family of algorithms underlying both, planning in known
environments as well as learning to act in unknown ones.

This lecture first introduces MDPs and standard Reinforcement Learning
methods. We then briefly focus on the exploration problem---very much
related to the exploration-exploitation problem represented by
bandits. We end with a brief illustration of policy search, imitation
and inverse RL without going into the full details
of these. Especially inverse RL is really worth knowing about: the
problem is to learn the underlying reward function from example
demonstrations. That is, the agent tries to ``understand'' (human)
demonstrations by trying to find a reward function consistent with them.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

~

\twocol{.5}{.4}{\center
  \mov{\show[.7]{mov-balance}}{05-sethu-movies/DA_PoleLearn.avi}
}{\center
  \mov{\show[.7]{mov-juggle}}{05-sethu-movies/DB_juggle.avi}
}

~

{\hfill\tiny (around 2000, by Schaal, Atkeson, Vijayakumar)}

~

\mov{\show[.3]{helicopter}}{07-andrew-ng/Aerobatic_rolls.wmv}

{\hfill\tiny (2007, Andrew Ng et al.)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Long history of RL in AI}{

\helvetica{9}{1.5}{m}{n}

Idea of programming a computer to learn by trial and error (Turing, 1954)

SNARCs (Stochastic Neural-Analog Reinforcement Calculators) (Minsky, 54)

Checkers playing program (Samuel, 59)

Lots of RL in the 60s (e.g., Waltz \& Fu 65; Mendel 66; Fu 70)

MENACE (Matchbox Educable Naughts and Crosses Engine (Mitchie, 63)

RL based Tic Tac Toe learner (GLEE) (Mitchie 68)

Classifier Systems (Holland, 75)

Adaptive Critics (Barto \& Sutton, 81)

Temporal Differences (Sutton, 88)

~

{\hfill\tiny from  Satinder Singh's \emph{Introduction to RL}, videolectures.com}

~

\item Long history in Psychology

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Outline}{

%% \item Learning

%% {\small
%% -- Temporal Difference \& Q-learning

%% -- Limitations of the model-free view

%% -- Model-based RL

%% }\medskip

%% \item Exploration

%% \item Briefly

%% {\small
%% -- Imitation Learning \& Inverse RL

%% -- Continuous states and actions (LSPI, Policy Gradients)

%% }\medskip

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Recall: Markov Decision Process}{

~

\shows[.7]{mdp1}

\cen{\small$P(s_{0:T\po},a_{0:T},r_{0:T};\pi) = P(s_0)~ \prod_{t=0}^T
P(a_t|s_t;\pi)~ P(r_t|s_t,a_t)~ P(s_{t\po}|s_t,a_t)$}

~

\begin{items}
\item world's initial state distribution $P(s_0)$

\item world's transition probabilities $P(s_{t\po} \| s_t,a_t)$

\item world's reward probabilities $P(r_t \| s_t,a_t)$

\item agent's \emph{policy} $\pi(a_t \| s_t) = P(a_0|s_0;\pi)$ ~ (or
   deterministic $a_t = \pi(s_t)$)
\end{items}

~

\item \textbf{Stationary MDP:}
\begin{items}
\item We assume $P(s' \| s,a)$ and $P(r|s,a)$ independent of time

\item We also define $R(s,a) := \Exp{r|s,a} = \int r~ P(r|s,a)~ dr$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Recall}{

\item Bellman equations
\begin{align*}
V^*(s)
&= \max_a Q^*(s,a) 
 = \max_a \[R(s,a) + \g \tsum_{s'} P(s'\|s,a)~ V^*(s') \] \\
Q^*(s,a)
&= R(s,a) + \g \sum_{s'} P(s'\|s,a)~ \max_{a'} Q^*(s',a')
\end{align*}

~

%% \item With finite horizon $T$ (non stationary MDP), initializing $V_{T\po}(s)=0$
%% \begin{align*}
%% V^*_t(s)
%% &= \max_a Q^*_t(s,a) 
%%  = \max_a \[R_t(s,a) + \g \tsum_{s'} P_t(s'\|s,a)~ V^*_{t\po}(s') \] \\
%% Q^*_t(s,a)
%% &= R_t(s,a) + \g \sum_{s'} P_t(s'\|s,a)~ \max_{a'} Q^*_{t\po}(s',a')
%% \end{align*}

\item Value-/Q-Iteration
\begin{align*}
\forall s:~
  V_{k+1}(s)
& = \max_a \[ R(s,a) + \g \sum_{s'} P(s'|s,a) ~V_k(s') \] \\
\forall_{s,a}:~
  Q_{k+1}(s,a)
& = R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'} Q_k(s',a')
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Towards Learning}{

\item From Sutton \& Barto's \emph{Reinforcement Learning} book:

{\tiny
The term \textbf{dynamic programming (DP)} refers to a collection of algorithms
that can be used to compute optimal policies given a perfect model of
the environment as a Markov decision process (MDP). Classical DP
algorithms are of limited utility in reinforcement learning both
because of their assumption of a perfect model and because of their
great computational expense, but they are still important
theoretically. DP provides an essential foundation for the
understanding of the methods presented in the rest of this book. In
fact, all of these methods can be viewed as attempts to achieve much
the same effect as DP, only with less computation and without assuming
a perfect model of the environment. 

}

~

\item So far, we introduced basic notions of an MDP and value functions and
methods to compute optimal policies \textbf{assuming that we know the
world} (know $P(s'|s,a)$ and $R(s,a)$)

\medskip

Value Iteration and Q-Iteration are instances of Dynamic Programming

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Learning in MDPs}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Outline}{

%% \item Markov Decision Processes as formal model

%% {\small
%% -- Definition

%% -- Value/$Q$-function

%% -- Planning as computing $V$/$Q$ given a model

%% }\medskip

%% \item \textbf{Learning}

%% {\small
%% -- Temporal Difference \& Q-learning

%% -- Limitations of the model-free view

%% -- Model-based RL

%% }\medskip

%% \item Exploration

%% \item Briefly

%% {\small
%% -- Imitation Learning \& Inverse RL

%% -- Continuous states and actions (LSPI, Policy Gradients)

%% }\medskip

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Five approaches to behavior learning}{

%% ~

%% ~

%% \show[1.]{RLover2}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{model-free RL}
\key{model-based RL}
\slide{}{

\shows[1.3]{RLoverview}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Learning in MDPs}{

\item While interacting with the world, the agent collects data of the
  form

\cen{$D = \{ (s_t,a_t,r_t,s_{t\po}) \}_{t=1}^T$}

(state, action, immediate reward, next state)

~

\cen{\emph{What could we learn from that?}}

~

\item \textbf{Model-based RL:}

~~ learn to predict next state: ~ estimate $P(s'|s,a)$

~~ learn to predict immediate reward: ~ estimate $P(r|s,a)$

~

\item \textbf{Model-free RL:}

~~ learn to predict \emph{value}: ~ estimate $V(s)$ or $Q(s,a)$

~

\item \textbf{Policy search:}

~~ e.g., estimate the ``policy gradient'', or directly use black box
(e.g.\ evolutionary) search
 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

We introduce basic \emph{model-free} methods first.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{Temporal difference (TD)}
%% \slide{Temporal difference (TD) learning with $V$}{

%% \item Recall the recursive property of $V(s)$:
%% $$
%% V^\pi(s)
%%  = {\color{red}R(s,\pi(s))} + \g {\color{greencol}\tsum_{s'} P(s'\|s,\pi(s))~ V^\pi(s')}
%% $$

%% \mypause

%% \item \textbf{TD learning:} Given a new experience $(s,a,r,s')$
%% \begin{align*}
%% V_\new(s)
%%  &= (1-\a)~ V_\old(s)
%%  + \a~ [{\color{red}r} + \g {\color{greencol}V_\old(s')} ] \\
%%  &= V_\old(s) + \a~ [r + \g V_\old(s') - V_\old(s) ] ~.
%% \end{align*}
%% %% {\small(stochastic variant of Dynamic Programming, convergence with
%% %%   probability 1)}

%% ~

%% \item \textbf{Reinforcement:}

%% -- more reward than expected ~ ($r
%% > V_\old(s) - \g V_\old(s')$)\\ ~~ $\to$~ increase $V(s)$

%% -- less reward than expected ~ ($r
%% < V_\old(s) - \g V_\old(s')$)\\ ~~ $\to$~ decrease $V(s)$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{model-free RL}
\key{Temporal difference (TD)}
%% \slide{Sarsa: Temporal difference (TD) learning of \protect$Q^\pi$}{

%% \item Recall the recursive property of $Q(s,a)$:
%% $$
%% Q^\pi(s,a)
%%   = {\color{red}R(s,a)} + \g {\color{green}\tsum_{s'}
%%   P(s'|s,a)~ Q^*(s',\pi(s'))}
%% $$

%% \mypause

%% \item \textbf{TD learning:} Given a new experience $(s,a,r,s',a'\=\pi(s'))$
%% \begin{align*}
%% Q_\new(s,a)
%%  &= (1-\a)~ Q_\old(s,a) + \a~ [{\color{red}r} + \g {\color{green} Q_\old(s',a')} ]\\
%%  &= Q_\old(s,a) + \a~ [r + \g Q_\old(s',a') - Q_\old(s,a)]
%% \end{align*}

%% ~

%% \item \textbf{Reinforcement:}

%% -- more reward than expected ~ ($r > Q_\old(s,a) - \g Q_\old(s',a')$)\\ ~~ $\to$~ increase $Q(s,a)$

%% -- less reward than expected ~ ($r < Q_\old(s,a) - \g Q_\old(s',a')$)\\ ~~ $\to$~ decrease $Q(s,a)$


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Q-learning}
\slide{Q-learning: Temporal-Difference (TD) learning of \protect$Q^*$}{

\item Recall the Bellman optimality equation for the $Q$-function:
$$
Q^*(s,a)
  = {\color{red}R(s,a)} + \g {\color{green}\tsum_{s'}
  P(s'|s,a)~ \max_{a'} Q^*(s',a')}
$$

%\mypause

\item \textbf{Q-learning} (Watkins, 1988) Given a new experience $(s,a,r,s')$
\begin{align*}
Q_\new(s,a)
 &= (1-\a)~ Q_\old(s,a) + \a~ [{\color{red}r} + \g {\color{green}\max_{a'} Q_\old(s',a')} ]\\
 &= Q_\old(s,a) + \a~ [\underbrace{r + \g \max_{a'} Q_\old(s',a') - Q_\old(s,a)}_{\text{TD error}}]
\end{align*}

~

\item \textbf{Reinforcement:}

-- more reward than expected ~ ($r > Q_\old(s,a) - \g \max_{a'} Q_\old(s',a')$)\\ ~~ $\to$~ increase $Q(s,a)$

-- less reward than expected ~ ($r < Q_\old(s,a) - \g \max_{a'} Q_\old(s',a')$)\\ ~~ $\to$~ decrease $Q(s,a)$


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Q-learning pseudo code}{

~

\item \textbf{Q-learning:}\\
\begin{algo}
\State Initialize $Q(s,a)=0$
\Repeat \Comment{for each episode}
\State Initialize start state $s$
\Repeat \Comment{for each step of episode}
\State Choose action $a\approx_\e \argmax_a Q(s,a)$
\State Take action $a$, observe $r, s'$
\State $Q(s,a) \gets Q(s,a) + \a~ [r + \g \max_{a'} Q(s',a') - Q(s,a)]$
\State $s\gets s'$
\Until end of episode
\Until happy
\end{algo}

~

\item \textbf{$\e$-greedy action selection:}

\tiny
$$a\approx_\e \argmax_a Q(s,a)
\quad\iff\quad
a = \begin{cases}
\text{random} & \text{with prob.}~ \e \\
\argmax_a Q(s,a) & \text{else}
\end{cases}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Proof of convergence of Q-learning}
\slide{Q-learning convergence with prob 1}{

\item Q-learning is a stochastic approximation of Q-Iteration (Watkins, 1988):

~

{\small

Q-learning: \hfill
$Q_\new(s,a)
 = (1-\a) Q_\old(s,a)
 + \a [r + \g \max_{a'} Q_\old(s',a') ] $

Q-Iteration: \hfill
 $\forall_{s,a}:~
  Q_{k+1}(s,a)
  = R(s,a) + \g \sum_{s'} P(s'|s,a)~ \max_{a'} Q_k(s',a')$

}

~

We've shown convergence of Q-Iteration to $Q^*$

~


\item Convergence of Q-learning:

Q-Iteration is a deterministic update: $Q_{k+1} = T(Q_k)$

Q-learning is a stochastic version: $Q_{k+1} = (1-\a) Q_k + \a [ T(Q_k) + \eta_k ]$

$\eta_k$ is zero mean!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Q-learning impact}{

\item Q-Learning was the first provably convergent direct
  adaptive optimal control algorithm

~

\item Great impact on the field of Reinforcement Learning in 80/90ies

\begin{items}
\item ``Smaller representation than models''

\item ``Automatically focuses attention to where it is
needed,''

~~ i.e., no sweeps through state space

\item Can be made more efficient with eligibility traces
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{on-policy RL}
\key{off-policy RL}
\key{SARSA RL}
\key{Temporal Difference RL}
\slide{Variants: TD($\l$), Sarsa($\l$), $Q(\l)$}{

INDICES!!

\item TD($\l$):

$\forall \tilde s:~ V(\tilde s) \gets V(\tilde s)
 + \a~ e(\tilde s)~ [r + \g V_\old(s') - V_\old(s)]
$

\item Sarsa($\l$)

$\forall_{\tilde s,\tilde a}:~ Q(\tilde s,\tilde a) \gets Q(\tilde s,\tilde a)
 + \a~ e(\tilde s,\tilde a)~ [r + \g Q_\old(s',a') - Q_\old(s,a)]$

\item $Q(\l)$

$\forall_{\tilde s,\tilde a}:~ Q(\tilde s,\tilde a) \gets Q(\tilde s,\tilde a)
 + \a~ e(\tilde s,\tilde a)~ [r + \g \max_{a'} Q_\old(s',a') - Q_\old(s,a)]$

~

\item \textbf{On-policy vs.\ off-policy} learning:
\begin{items}
\item On-policy: estimate $Q^\pi$ while executing $\pi$ ~ (Sarsa, TD)
\item Off-policy: estimate $Q^*$ while executing $\pi$ ~ (Q-learning)
\end{items}

~

{\tiny\textit{Meta-note: Typical text books introduce TD and Sarsa first, then Q-learning. But TD and Sarsa are practically not as relevant as Q-learning.

}}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Eligibility traces}
\slide{Eligibility traces}{

\item Temporal Difference: ~ based on single experience $(s_0, r_0, s_1)$
\begin{align*}
V_\new(s_0)
 &= V_\old(s_0)
 + \a [r_0 + \g V_\old(s_1) - V_\old(s_0) ]
\end{align*}

\item Longer experience sequence, e.g.: $(s_0, r_0, r_1, r_2, s_3)$
  
\mypause

  \textbf{Temporal credit assignment}, think further backwards: receiving
  $r_{0:2}$ and ending up in $s_3$ also tells us something about $V(s_0)$
\begin{align*}
V_\new(s_0)
 &= V_\old(s_0)
 + \a [r_0 + \g r_1 + \g^2 r_2 + \g^3 V_\old(s_3) - V_\old(s_0) ]
\end{align*}

\mypause

\item \textbf{TD($\l$)}: equivalent to the above!: remember where you've been recently (``eligibility trace'') and update those values as well:
\begin{align*}
&e(s_t) \gets e(s_t)+1 \\
&\forall s:~ V_\new(s) = V_\old(s)
 + \a~ e(s)~ [r_t + \g V_\old(s_{t\po}) - V_\old(s_t)] \\
&\forall s:~ e(s) \gets \g \l e(s)
\end{align*}

\item This is a core topic of Sutton \& Barto book

$\to$ great improvement of basic RL algorithms

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Q(\protect{$\l$}) pseude code}{

\begin{algo}
\State Initialize $Q(s,a)=0$, $e(s,a)=0$
\Repeat \Comment{for each episode}
\State Initialize start state $s$
\Repeat \Comment{for each step of episode}
\State Choose action $a\approx_\e \argmax_a Q(s,a)$
\State Update eligibility: \quad $e(s,a) \gets 1$ \quad or \quad $e(s,a) \gets e(s,a)+1$
\State Take action $a$, observe $r, s'$
\State Compute TD-error $D = [r + \g \max_{a'} Q(s',a') - Q(s,a)]$
\State $\forall_{\tilde s,\tilde a}$ with $e(\tilde s,\tilde a)>0$: ~ $Q(\tilde s,\tilde a) \gets Q(\tilde s,\tilde a) + \a~ e(\tilde s,\tilde a)~ D$
\State Discount all eligibilities $\forall_{\tilde s,\tilde a}:~ e(\tilde s,\tilde a) \gets \g \l e(\tilde s,\tilde a)$
\State $s\gets s'$
\Until end of episode
\Until happy
\end{algo}

~

\item Analogously for TD($\l$) and SARSA($\l$)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Experience Replay}{

\item In large state spaces, the Q-fucntion is represented using function approximation

We cannot store a full table $e(s,a)$ and update $\forall_{\tilde s,\tilde a}$

\item Instead we store the full data $D=\{ (s_i,a_i,r_i,s_{i+1}) \}_{i=o}^t$ up to now (time $t$), called the \textbf{replay buffer}

\item Update the Q-function for a $B$ subsample (of contant size) of $D$ plus the most recent experience
$$\forall (s,a,r,s')\in B: ~ Q(s,a) \gets Q(s,a) + \a~ [r + \g \max_{a'} Q(s',a') - Q(s,a)] $$

(See paper ``A Deeper Look at Experience Replay'' (Zhang, Sutton))

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Discussion**}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{TD-Gammon, by Gerald Tesauro**}{

{\tiny (See section 11.1 in Sutton \& Barto's book.)}

\item MLP to represent the value function $V(s)$

~

\show[.5]{TDgammon}

~

\item Only reward given at end of game for win.

\item \textbf{Self-play}: use the current policy to sample moves on
  \emph{both} sides!

\item random policies $\to$ games take up to thousands of
  steps. Skilled players $\sim 50-60$ steps.


\item TD($\l$) learning (gradient-based update of NN weights)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{TD-Gammon notes**}{

\item Choose features as raw position inputs (number of pieces at each place)

$\to$ as good as previous computer programs

\item Using previous computer program's expert features

$\to$ world-class player

~

\item Kit Woolsey was world-class player back then:

\begin{items}
\item TD-Gammon particularly good on vague positions

\item not so good on calculable/special positions

\item just the opposite to (old) chess programs
\end{items}

~

\item See anotated matches: \url{http://www.bkgm.com/matches/woba.html}

~

\item Good example for

\begin{items}
%\item being creative in applying RL when it's supposed to work!

\item value function approximation

\item game theory, self-play
\end{items}

%~

%\item perhaps backgammon is not too difficult after all?

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Dopamine**}{

\show[.7]{dopamine}

Montague, Dayan \& Sejnowski:
\emph{A Framework for Mesencephalic Dopamine Systems based on Predictive
Hebbian Learning.}
Journal of Neuroscience, 16:1936-1947, 1996. 

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

So what does that mean?

~

\begin{items}
\item We derived an algorithm from a general framework

\item This algorithm involves a specific variable (reward residual)

\item We find a neural correlate of exactly this variable
\end{items}

~

\cen{\emph{Great!}}

\mypause

Devil's advocate:

~

\begin{items}
\item Does not proof that TD learning is going on

Only that an expected reward is compared with a experienced reward

~

\item Does not discriminate between model-based and model-free

(Both can induce an expected reward)
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Limitations of the model-free view}{

\item Given learnt values, behavior is a fixed SR (or state-action) mapping

\item If the ``goal'' changes: need to re-learn values for every state in
   the world! all previous values are obsolete

\item No general ``knowledge'', only values

\item No anticipation of general outcomes ($s'$), only of value

\item No ``planning''
  
%\centerline{\color{blue}\it That cannot be the whole story!}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\center

\twocol{.4}{.4}{
\mov{\show[.6]{koehler-monkeys}
}{10-goalDirectedBehavior/APigeonSolvesTheClassicBox-and-BananaProblem.flv}
}{
Wolfgang K\"ohler (1917)

\emph{Intelligenzpr\"ufungen am\\Menschenaffen}

\emph{The Mentality of Apes}

~

~

model-free RL?

NO WAY!
}

%% ~


%% ~%\mypause

%% {\color{red}\it Goal-directed
%%   Decision Making}\\
%% What are computational principles for such behavior?

%% %(model-based view)

}

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \center

%% \twocol{.4}{.4}{
%% \mov{\show[.6]{koehler-monkeys}
%% }{10-goalDirectedBehavior/APigeonSolvesTheClassicBox-and-BananaProblem.flv}
%% }{\center
%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Behavioral Psychology**}{

\twocol{.45}{.45}{
{\center
\showh[.4]{tolman}\\
Edward Tolman (1886 - 1959)

~

\showh[.4]{koehler}\\
Wolfgang K\"ohler (1887--1967)
}

~

{\helvetica{7}{1}{m}{n} learn facts about the world that they could subsequently
    use in a flexible manner, rather than simply learning automatic
    responses\\

}

}{

%% {\center
%% \showh[.4]{thorndike}\\
%% Edward Thorndike (1874 – 1949)\\

%% \emph{``Law of Effect''}\\

%% }

%% ~

{\center
\showh[.4]{hull}\\
Clark Hull (1884 - 1952)\\

\emph{Principles of Behavior} (1943)\\

}

~


{\helvetica{7}{1}{m}{n} learn stimulus-response mappings based on
  reinforcement\\

}

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\slide{Goal-directed vs.\ habitual: Devaluation**}{

\mov{[skinner]}{10-goalDirectedBehavior/SkinnerBox-Eddy-BellTraining.mp4}

~

\show[.8]{devaluation}

~

Niv, Joel \& Dayan:
\emph{A normative perspective on motivation.}
TICS, 10:375-381, 2006.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Goal-directed vs.\ habitual: Devaluation**}{

~

\show[.3]{devaluation2}

~

Niv, Joel \& Dayan:
\emph{A normative perspective on motivation.}
TICS, 10:375-381, 2006.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

{\small
By definition, goal-directed behavior is performed to obtain a desired
goal. Although all instrumental behavior is \textbf{instrumental} in
achieving its contingent goals, it is not necessarily purposively
\textbf{goal-directed}. Dickinson and Balleine [1,11] proposed that
behavior is goal-directed if: (i) it is sensitive to the contingency
between action and outcome, and (ii) the outcome is desired. Based on
the second condition, motivational manipulations have been used to
distinguish between two systems of action control: if an instrumental
outcome is no longer a valued goal (for instance, food for a sated
animal) and the behavior persists, it must not be goaldirected.
Indeed, after moderate amounts of training, outcome revaluation brings
about an appropriate change in instrumental actions
(e.g. leverpressing) [43,44], but this is no longer the case for
extensively trained responses ([30,31], but see [45]). That extensive
training can render an instrumental action independent of the value of
its consequent outcome has been regarded as the experimental parallel
of the folk psychology maxim that wellperformed actions become
\textbf{habitual} [9] (see Figure I).

%%   This distinction between two
%% types of behavior is also paralleled by a distinction between two
%% different neural pathways to action selection. Habitual behavior is
%% thought to be dependent on the dorsolateral striatum [8,32] and its
%% dopaminergic afferents, whereas goal-directed behavior is controlled
%% more by circuitry involving frontal cortical areas and the dorsomedial
%% striatum [8,20,21]. These two pathways have been suggested as
%% subserving two action controllers with different computational
%% characteristics, which operate in parallel during action selection
%% [10].

~}

Niv, Joel \& Dayan:
\emph{A normative perspective on motivation.}
TICS, 10:375-381, 2006.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Model-based RL}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Model-based RL}
\slide{Model-based RL}{

\shows[.6]{RLoverview}
%% \cen{\fbox{$D = \{ (s,a,r,s')_t \}_{t=0}^T$
%% $\quad\stackrel{\text{learn}}\to\quad$
%% $P(s'|s,a)$
%% $\quad\stackrel{\text{DP}}\to\quad$
%% $V(s)$
%% $\quad\to\quad$
%% $\pi(s)$}}

\item \textbf{Model learning:} Given data $D = \{
(s_t,a_t,r_t,s_{t\po}) \}_{t=1}^T$ estimate $P(s'|s,a)$ and
$R(s,a)$. For instance:

\begin{items}
\item discrete state-action: ~  $\hat P(s'|s,a) = \frac{\#(s',s,a)}{\#(s,a)}$

\item continuous state-action: ~ $\hat P(s'|s,a) = \NN(s' \| \phi(s,a)^\T \b, \S)$

estimate parameters $\b$ (and perhaps $\S$) as for regression

(including non-linear features, regularization, cross-validation!)
\end{items}

\item \textbf{Planning}, \small for instance:

\begin{items}
\item discrete state-action: ~ Value Iteration with the estimated model

\item continuous state-action: ~ Least Squares Value Iteration

Stochastic Optimal Control (Riccati, Differential Dynamic Prog.)
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\mov{\show[.7]{mov-balance}}{05-sethu-movies/DA_PoleLearn.avi}

{\hfill\tiny (around 2000, by Schaal, Atkeson, Vijayakumar)}

~

\item Use a simple regression method (locally weighted Linear
Regression) to estimate

\cen{$P(\dot x|u,x) \stackrel{\text{local}}= \NN(\dot x \| Ax + Bu, \s)$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Exploration}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Outline}{

%% \item Markov Decision Processes as formal model

%% {\small
%% -- Definition

%% -- Value/$Q$-function

%% -- Planning as computing $V$/$Q$ given a model

%% }\medskip

%% \item Learning

%% {\small
%% -- Temporal Difference \& Q-learning

%% -- Limitations of the model-free view

%% -- Model-based RL

%% }\medskip

%% \item \textbf{Exploration}

%% \item Briefly

%% {\small
%% -- Imitation Learning \& Inverse RL

%% -- Continuous states and actions (LSPI, Policy Gradients)

%% }\medskip

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Epsilon-greedy exploration in Q-learning}
\slide{\protect$\e$-greedy exploration in Q-learning}{

~

\begin{algo}
\State Initialize $Q(s,a)=0$
\Repeat \Comment{for each episode}
\State Initialize start state $s$
\Repeat \Comment{for each step of episode}
\State {\color{blue} Choose action
$a = \begin{cases}
{\color{blue} \text{random}} & {\color{blue} \text{with prob.}~ \e}\\
\argmax_a Q(s,a) & \text{else}
\end{cases}
$}
\State Take action $a$, observe $r, s'$
\State $Q_\new(s,a) \gets Q_\old(s,a) + \a~ [r + \g \max_{a'} 
Q_\old(s',a' ) - Q_\old(s,a)]$
\State $s\gets s'$
\Until end of episode
\Until happy
\end{algo}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimistic initialization \& UCB}{

\item Initialize the Q function optimistically! E.g., if you know $R_{max}$, $Q(s,a) = 1/(1-\g) R_{max}$ (in practise, this is often too large..)

\item UCB: If you can estimate a confidence bound $\s(s,a)$ for your Q-function (e.g., using bootstrap estimates when using function approximation), choose your action based on $Q(s,a) + \b \s(s,a)$

\item Generally, we need better ways to explore than $\e$-greedy!!

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{R-Max}
\slide{\rmax}{

~

Brafman and Tennenholtz (2002)

\item Model-based RL: We estimate $R(s,a)$ and $P(s'|s,a)$ on the fly

\item Use an \emph{optimistic} reward function:
1
\begin{align}
\nonumber
R^{\rmax}(s,a) =
\left\{
\begin{array}{ll}
R(s,a) & \! c(s,a) \!\ge\! m ~ \text{($s,a$ known)} \\
R_{max} & \! c(s,a) \!<\! m ~ \text{($s,a$ unknown)}
\end{array}
\right.
\end{align}

\item Is PAC-MDP efficient

\item Optimism in the face of uncertainty

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{KWIK-R-max**}{

(Li, Littman, Walsh, Strehl, 2011)

~

\item Extension of \rmax\ to \emph{more general
representations}

~

\item Let's say the transition model $P(s' \| s,a)$ is defined by $n$
parameters

Typically, \emph{$n \ll$ number of states}!

~

\item Efficient KWIK-learner $L$ requires a number of samples which
is polynomial in $n$ to estimate approximately correct $\hat{P}(s' \|
s,a)$

(KWIK = Knows-what-it-knows framework)

~

\item KWIK-\rmax\ using $L$ is \emph{PAC-MDP efficient in $n$}

$\to$ polynomial in number of parameters of transition model!

$\to$ more efficient than plain \rmax\ by several orders of magnitude!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bayesian RL}
\slide{Bayesian RL**}{

\item There exists an optimal solution to the exploration-exploitation
  trade-off: belief planning (see my tutorial ``Bandits, Global
  Optimization, Active Learning, and Bayesian RL -- understanding the
  common ground'')


$$
V^\pi(b,s)
 = R(s,\pi(b,s)) 
 + \int_{b',s'} P(b',s' \|b,s,\pi(b,s))~ V^\pi(b',s')
$$

\begin{items}
\item Agent maintains a \emph{distribution (belief) $b(m)$ over
MDP models $m$}

\item typically, MDP structure is fixed; belief over the parameters

\item belief updated after each observation $(s,a,r,s')$: $b \to b'$

\item only tractable for very simple problems
\end{items}

~

\item \emph{Bayes-optimal policy} $\pi^*=\argmax_\pi V^\pi(b,s)$
\begin{items}
\item no other policy leads to more rewards in expectation w.r.t.~prior
distribution over MDPs

\item solves the exploration-exploitation tradeoff
%\item is \emph{not} PAC-MDP efficient!
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Optimistic heuristics}
\slide{General View: Optimistic heuristics}{

\item As with UCB, choose estimators for $R^*$, $P^*$ that are optimistic/over-confident
$$V_t(s)
= \max_a\[ R^* + \Sum_{s'} P^*(s'|s,a)~ V_{t\po}(s') \]$$

\item Rmax:
\begin{items}
\item $R^*(s,a)=\begin{cases} R_{\text{max}} & \text{if
$\#_{s,a}<n$} \\ \hat\t_{rsa} & \text{otherwise} \end{cases}$
\comma $P^*(s'|s,a)=\begin{cases} \d_{s's^*} & \text{if
$\#_{s,a}<n$} \\ \hat\t_{s'sa} & \text{otherwise} \end{cases}$
\item Guarantees over-estimation of values, polynomial PAC results!
\item Read about ``KWIK-Rmax''! (Li, Littman, Walsh, Strehl, 2011)
\end{items}

\item Bayesian Exploration Bonus (BEB), Kolter \& Ng (ICML 2009)
\begin{items}
\item Choose $P^*(s'|s,a) = P(s'|s,a,b)$ integrating over the current belief
$b(\t)$ (non-over-confident)
\item But choose $R^*(s,a) = \hat\t_{rsa} + \frac{\b}{1+\a_0(s,a)}$
with a hyperparameter $\a_0(s,a)$, over-estimating return
\end{items}

\item Confidence intervals for $V$-/$Q$-function
(Kealbling '93, Dearden et al.\ '99)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{More ideas about exploration}{

\item \textbf{Intrinsic rewards} for \emph{learning progress}

\begin{items}
\item ``fun'', ``curiousity''

\item in addition to the external ``standard'' reward of the MDP

\item \textit{``Curious agents are interested in learnable but yet
unknown regularities, and get bored by both predictable and inherently
unpredictable things.''} (J.~Schmidhuber)

\item Use of a meta-learning system which learns to predict the error that
the
learning machine makes in its predictions; meta-predictions measure the
\emph{potential interestingness of situations} (Oudeyer et al.)
\end{items}


~

\item \emph{Dimensionality reduction} for model-based
exploration in \emph{continuous spaces}: low-dimensional
representation of the transition function; focus exploration on relevant
dimensions (A.~Nouri, M.~Littman)

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{More ideas about exploration}{

%% \item Exploration to reduce uncertainty of a belief $p(x)$

%% \begin{items}
%% \item In robotics, $p(x)$ might be the belief about the robot position $x$

%% \item \emph{Entropy}: a probabilistic measure of information

%% \hspace{0.5cm} $H(p(x)) = - \int p(x) \log p(x) dx$

%% \hspace{0.5cm} $H_p(x)$ is maximal if $p$ is uniform,

%% \hspace{0.5cm} and minimal if $p$ is a point mass distribution

%% \item \emph{Information gain} of action $a$:

%% \hspace{0.5cm} $I(a) = H(p(x)) - E_z[H(p(x' \| z, a))]$

%% \hspace{0.5cm} ($z$ is the potential observation)

%% \hspace{0.5cm} expected change of entropy when executing an action

%% \item \emph{maximizing information gain = minimizing uncertainty
%% in belief}
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Policy Search, Imitation, \& Inverse RL**}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Outline}{

%% \item Markov Decision Processes as formal model

%% {\small
%% -- Definition

%% -- Value/$Q$-function

%% -- Planning as computing $V$/$Q$ given a model

%% }\medskip

%% \item Learning

%% {\small
%% -- Temporal Difference \& Q-learning

%% -- Limitations of the model-free view

%% -- Model-based RL

%% }\medskip

%% \item Exploration

%% \item \textbf{Briefly}

%% {\small
%% -- Policy Search

%% -- Imitation Learning \& Inverse RL
%% }\medskip

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\shows[1.3]{RLoverview}

~

\begin{items}
\item Policy gradients are one form of policy search.
\item There are other, \emph{direct} policy search methods

(e.g., plain stochastic search, ``Covariance Matrix Adaptation'')
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Five approaches to learning behavior**}{

~

~

\show[1.]{RLover2}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Policy gradients}
\slide{Policy Gradients**}{

\item In continuous state/action case, represent the 
policy as linear in arbitrary state features:
\begin{align*}
\pi(s)
 &= \sum_{j=1}^k \phi_j(s) \b_j = \phi(s)^\T \b &&\text{(deterministic)}\\
\pi(a\|s)
 &= \NN(a \| \phi(s)^\T \b, \S) && \text{(stochastic)}
\end{align*}
with $k$ features $\phi_j$.

~

\item Basically, given an episode $\xi =(s_t,a_t,r_t)_{t=0}^H$,
we want to estimate

$$\frac{\del V(\b)}{\del \b}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Policy Gradients**}{

\item One approach is called REINFORCE:
{\small\begin{align*}
\hspace*{-5mm}
&\frac{\del V(\b)}{\del \b}
=
\frac{\del}{\del \b}
\int P(\xi|\b)~ R(\xi)~ d\xi
= \int P(\xi|\b) \frac{\del}{\del \b} \log P(\xi|\b) R(\xi) d\xi \\
\hspace*{-5mm}
&=
\Exp[\xi|\b]{\frac{\del}{\del \b} \log P(\xi|\b) R(\xi)}
 = \Exp[\xi|\b]{\sum_{t=0}^H \g^t \frac{\del \log\pi(a_t|s_t)}{\del \b}
 \underbrace{\sum_{t'=t}^H \g^{t'-t} r_{t'}}_{Q^\pi(s_t,a_t,t)}}
\end{align*}}

~

\item Another is PoWER, which requires $\frac{\del V(\b)}{\del \b}=0$
\begin{align*}
\b\gets \b + \frac{
\Exp[\xi|\b]{\sum_{t=0}^H  \e_t Q^\pi(s_t,a_t,t)}
}{
\Exp[\xi|\b]{\sum_{t=0}^H  Q^\pi(s_t,a_t,t)}
}
\end{align*}

~

{\tiny See: Peters \& Schaal (2008): \emph{Reinforcement learning of
motor skills with policy gradients}, Neural Networks.

Kober \& Peters: \emph{Policy Search for Motor Primitives in
Robotics}, NIPS 2008.

Vlassis, Toussaint (2009): \emph{Learning Model-free Robot Control by
a Monte Carlo EM Algorithm.} Autonomous Robots 27, 123-130. 

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \mov{\show[.7]{Peters-ball-in-a-cup}}{08-Peters-ball-in-a-cup.flv}

%% ~

%% \tiny\hfill
%% Kober \& Peters: \emph{Policy Search for Motor Primitives in
%% Robotics}, NIPS 2008.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Imitation Learning}
\slide{Imitation Learning**}{

~

\cen{\fbox{$D = \{ (s_{0:T},a_{0:T})^d \}_{d=1}^n$
$\quad\stackrel{\text{learn/copy}}\to\quad$
$\pi(s)$}}


~

\item Use ML to imitate demonstrated state trajectories $x_{0:T}$

~

Literature:

~\liter

Atkeson \& Schaal: Robot learning from demonstration (ICML 1997)

Schaal, Ijspeert \& Billard: Computational approaches to motor
learning by imitation (Philosophical Transactions of the Royal Society
of London. Series B: Biological Sciences 2003)

Grimes, Chalodhorn \& Rao: Dynamic Imitation in a Humanoid Robot
through Nonparametric Probabilistic Inference. (RSS 2006)

%Rao's paper on policy recognition

%% David Silver, James Bagnell, Anthony Stentz: High Performance Outdoor Navigation from Overhead Data using Imitation Learning, RSS 2008.
%% http://www.roboticsproceedings.org/rss04/p34.html 

R\"udiger Dillmann: Teaching and learning of robot tasks via observation
of human performance (Robotics and Autonomous Systems, 2004)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Imitation Learning**}{

\item There a many ways to imitate/copy the oberved policy:

~

Learn a density model $P(a_t\|s_t) P(s_t)$ (e.g., with mixture of
   Gaussians) from the observed data and use it as policy (Billard et
   al.)

~

Or trace observed trajectories by minimizing perturbation costs
(Atkeson \& Schaal 1997)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Imitation Learning**}{

~

\mov{\show{DA-PoleBalancing-Imitation}}{05-sethu-movies/DA-PoleBalancing-Imitation.mov}

{\tiny\hfill Atkeson \& Schaal}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Inverse RL}
\slide{Inverse RL**}{

~

\cen{\fbox{$D=\{ (s_{0:T},a_{0:T})^d \}_{d=1}^n$
$\quad\stackrel{\text{learn}}\to\quad$
$R(s,a)$
$\quad\stackrel{\text{DP}}\to\quad$
$V(s)$
$\quad\to\quad$
$\pi(s)$}}

~

\item Use ML to ``uncover'' the latent reward function in observed
behavior

~

Literature:

~\liter

Pieter Abbeel \& Andrew Ng: Apprenticeship learning via inverse
reinforcement learning (ICML 2004)

Andrew Ng \& Stuart Russell: Algorithms for Inverse Reinforcement
Learning (ICML 2000)

{\color{blue}Nikolay Jetchev \& Marc Toussaint: Task Space Retrieval
Using Inverse Feedback Control (ICML 2011).}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Inverse RL ~ (Apprenticeship Learning)**}{

\item Given: demonstrations $D=\{ x_{0:T}^d \}_{d=1}^n$

\item Try to find a reward function
that \textbf{discriminates demonstrations from other policies}

-- Assume the reward function is linear in some features $R(x) =
   w^\T \phi(x)$

-- Iterate:

\begin{enumerate}
\item Given a set of candidate policies $\{ \pi_0, \pi_1, .. \}$

\item Find weights $w$ that maximize the value margin between teacher and
all other candidates
\begin{align*}
\max_{w,\xi}
 ~&~ \xi \\
\text{s.t.}
 ~&~ \forall_{\pi_i}:~ \underbrace{w^\T \<\phi\>_D}_\text{value of
 demonstrations}
 \ge \underbrace{w^\T \<\phi\>_{\pi_i}}_\text{value of $\pi_i$} + \xi \\
 ~&~ \norm{w}^2 \le 1
\end{align*}

\item Compute a new candidate policy $\pi_i$ that optimizes $R(x) =
w^\T \phi(x)$ and add to candidate list.
\end{enumerate}
{\hfill\tiny (Abbeel \& Ng, ICML 2004)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\mov{\show{04-abbeel-NastyDriver-invRL}}{04-abbeel-NastyDriver-invRL.avi}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Continuous state/actions in model-free RL}{

%% \item All of this is fine in small finite state \& action spaces.

%% $Q(s,a)$ is a $|S|\times|A|$-matrix of numbers.

%% $\pi(a|s)$ is a $|S|\times|A|$-matrix of numbers.

%% ~\mypause

%% \item In the following:

%% % two examples for handling continuous states/actions

%% % -- use function approximation to estimate $Q(s,a)$ (LSPI)

%% -- optimize a parameterized $\pi(a|s)$ (policy search)

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{LSPI: Least Squares Policy Iteration}{
% 
% \item The $Q$-functions for a given policy $\pi$ fulfills for any $s,a$:
% \begin{align*}
% Q^\pi(s,a)
%  &= R(s,a)  +  \g \sum_{s'} P(s'\|s,a)~ Q^\pi(s',\pi(s'))
% \end{align*}
% 
% ~\mypause
% 
% \item If we have $n$ data points $D = \{ (s_i,a_i,r_i,s_i')
%   \}_{i=1}^n$, we require that this equation holds (approximatly) for
%   these $n$ data points:
% \begin{align*}
% \forall_i:~ Q^\pi(s_i,a_i) = r_i  +  \g Q^\pi(s'_i,\pi(s'_i))
% \end{align*}
% 
% ~\mypause
% 
% \item Written in vector notation: $\vec Q = \vec R + g \vec{\bar Q}$
%  with $N$-dim data vectors $\vec Q, \vec R, \vec{\bar Q}$
% 
% ~\mypause
% 
% \item Written as optmization: minimize the \textbf{Bellman residual
% error}
% \begin{align*}
% L(Q^\pi) = \sum _{i=1}^n [Q^\pi(s_i,a_i) - r_i  -  \g
% Q^\pi(s'_i,\pi(s'_i))]^2 = \norm{\vec R - \vec Q + \g \vec{\bar Q}}^2
% \end{align*}
% 
% }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{LSPI: Least Squares Policy Iteration}{
% 
% \item Approximate $Q(s,a)$ as a linear function
% $$\hat Q(s,a) = \tsum_{j=1}^k \phi_j(s,a) \b_j = \phi(s,a)^\T \b$$
% 
% in $k$ features $\phi_j$. Then
% \begin{align*}
% \vec Q
%  &= \vec \phi \b \comma
%  \vec \phi_{ij} = \phi_j(s_i,a_i) \in \RRR^{n\times k}\\
% \vec{\bar Q}
%  &= \vec{\bar\phi} \b \comma
% \vec{\bar \phi}_{ij} = \phi_j(s'_i,\pi(s'_i)) \in \RRR^{n\times k}
% \end{align*}
% 
% ~
% 
% \item The loss becomes
% \begin{align*}
% L(\b)
%  &= \norm{\vec R - (\vec \phi - \g \vec{\bar\phi})^\T \b}^2
% \end{align*}
% 
% ~\mypause
% 
% \item Given the $Q$-function, we can \emph{numerically} find
% $\argmax_a Q(s,a)$ for the current state to update the policy
% (as in Policy Iteration)
% 
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{LSPI: Least Squares Policy Iteration}{
% 
% Technical detail: The $L(\b)$ we defined is called the Bellman
% residual error. If instead we minimize the norm
% $\norm{\vec\phi^\T \vec R - \vec \phi^\T(\vec \phi
% - \g \vec{\bar\phi})^\T \b}^2$ we minimize an error projected to the
% feature space -- which is called fixed point error. The latter seems
% more efficient.
% 
% ~
% 
% See Lagoudakis \& Parr (JMLR 2003) for details.
% 
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{LSPI: Riding a bike}{
% 
% \show{LSPI-ridingBike}
% 
% \hfill from Lagoudakis \& Parr (JMLR 2003)
% 
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% \slide{LSPI: Riding a bike}{
% 
% \show{LSPI-ridingBike2}
% 
% \hfill from Lagoudakis \& Parr (JMLR 2003)
% 
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{\label{lastpage}

\show[1.]{RLover2}

}

\slidesfoot

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conclusions}{

\item Markov Decision Processes and RL provide a solid framework for
describing behavioural learning \& planning

~

\item Little taxonomy:

~

\show[.7]{RLover2}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Basic topics not covered}{

\item Partial Observability (POMDPs)

{\small
What if the agent does not observe the state $s_t$? $\to$ The
policy $\pi(a_t \| b_t)$ needs to build on an internal representation,
called \emph{belief} $\b_t$.

}

~

\item Continuous state \& action spaces, function approximation in RL

~

\item Predictive State Representations, etc etc...

}




