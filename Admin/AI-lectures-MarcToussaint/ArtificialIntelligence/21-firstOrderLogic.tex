\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{First-Order Logic**}
\renewcommand{\keywords}{}

\slides[(slides based on Stuart Russell's AI course)]


\story{

First-order logic (FOL) is exactly what is sometimes been thought of as
``Good Old-Fashioned AI'' (GOFAI) -- and what was the central target
of critique on AI research coming from other fields like probabilistic
reasoning and machine learning. A bit over-simplified, in the AI
winter many researchers said ``logic doesn't work'', therefore AI
doesn't work, and instead the focus should be on learning and
probabilistic modelling. Some comments on this:

First, I think one should clearly distinguish between 1) logic
reasoning and inference, and 2) ``first-order (or relational)
representations''. Logic reasoning indeed is only applicable on
discrete \& deterministic knowledge bases. And as learnt knowledge is
hardly deterministic (it cannot be in a Bayesian view), logic
reasoning does not really apply well. In my view, this is one of the
core problems with GOFAI: the fact that logic reasoning does not unify
well with learning and learned models.

However, using ``first-order (or relational) representations'' means
to represent knowledge in such a way that it refers only to object
properties and relations, and therefore generalizes across object
identities. Sure, classical FOL knowledge bases are first-order
knowledge representations. But here research has advanced
tremendously: nowadays we can also represent learned
classifiers/regressions, graphical models, and Markov Decision
Processes in a first-order (also called ``relational'' or ``lifted'')
way. The latter are core probabilistic formalisms to account for
uncertainty and learning. Therefore the current state-of-the-art
provides a series of unifications of probabilistic and first-order
representations. I think this is what makes it important to learn and
understand first-order representations -- which is best taught in the
context of FOL.

The reasoning and inference methods one requires for
modern relational probabilistic models are of course different to
classical logical reasoning. Therefore, I think knowing about ``logical
reasoning'' is less important than knowing about ``logical
representations''. Still, some basic aspects of logical reasoning,
such as computing all possible substitutions for an abstract sentence,
thereby \emph{grounding} the sentence, are essential in all
first-order models.

Modern research on relational machine learning has, around 2011, lead
to some new optimism about modern AI, also called the spring of AI
(see, e.g., ``I, algorithm: A new dawn for artificial intelligence'',
2011). That wave of optimism now got over-rolled by the new hype on
deep learning, which in the media is often equated with AI. However,
at least up to now, one should clearly distinguish between deep
learning as a great tool for machine learning with huge amounts of
data; and reasoning, which includes model-based decision making,
control, planning, and also (Bayesian) learning from few data.

This lecture introduces to FOL. The goal is to understand FOL as the
basis for decision-making problems such as STRIPS rules, as well as
for relational probabilistic models such as relational Reinforcement
Learning and statistical relational learning methods. The latter are
(briefly) introduced in the next lecture.

We first introduce the FOL language, then basic inference
algorithms. Perhaps one of the most important concepts is the problem
of computing substitutions (also called unification or matching
problem), where much of the computational complexity of FOL
representations arises.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{The FOL language}{
FOL is a language---we define the syntax, the semantics, and give examples.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The limitation of propositional logic}{

\item Propositional logic has nice properties:
\begin{items}
\item Propositional logic is \emph{declarative}: pieces of syntax
  correspond to facts

\item Propositional logic allows partial/disjunctive/negated information
  (unlike most data structures and databases)

\item Propositional logic is \emph{compositional}: 
  meaning of $B_{1,1}\land P_{1,2}$ is derived from meaning
  of $B_{1,1}$ and of $P_{1,2}$

\item Meaning in propositional logic is \emph{context-independent}
  (unlike natural language, where meaning depends on context)
\end{items}

~

\item Limitation:
\begin{items}
\item Propositional logic has very limited expressive power, unlike
  natural language.  E.g., we cannot express ``pits cause breezes in
    adjacent squares'' except by writing one sentence for each square
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{First-order logic}{

\item Whereas propositional logic assumes that a world contains \emph{facts},
first-order logic (like natural language) assumes the world contains
\begin{items} 
\item \emph{Objects}: people, houses, numbers, theories,
Ronald McDonald, colors, baseball games, wars, centuries $\ldots$ 
\item \emph{Relations}: red, round, bogus, prime, multistoried $\ldots$, 
brother of, bigger than, inside, part of, has color, occurred after, owns, comes between, $\ldots$
\item \emph{Functions}: father of, best friend, third inning of, one more
than, end of $\ldots$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{FOL: Syntax}
\slide{FOL syntax elements}{

\begin{tabular}{ll}
Constants & $KingJohn,\ 2,\ UCB,\ldots$ \\
Predicates & $Brother,\ >,\ldots$ \\
Variables & $x,\ y,\ a,\ b,\ldots$ \\
Connectives & $\land\ \lor\ \lnot\ \implies\ \iff$ \\
Equality & $=$ \\
Quantifiers & $\forall\ \exists$ \\
Functions & $Sqrt,\ LeftLegOf,\ldots$ \\
\end{tabular}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{FOL syntax grammar}{

\begin{tabular}{lcl}
$\<$sentence$\>$ & $\to$ & $\<$atomic sentence$\>$ \\
&& $\mid$ $\<$complex sentence$\>$ \\
&& $\mid$ [$\forall \mid \exists$] $\<$variable$\>$~ $\<$sentence$\>$ \\[2ex]
%
$\<$atomic sentence$\>$
 & $\to$ & predicate($\<$term$\>$,\dots) \\
&& $\mid$ $\<$term$\>$=$\<$term$\>$ \\[2ex]
$\<$term$\>$
 & $\to$ & function($\<$term$\>$,\dots) \\
&& $\mid$ constant \\
&& $\mid$ variable \\[2ex]
$\<$complex sentence$\>$
 & $\to$ & $\lnot$ $\<$sentence$\>$ \\
&& $\mid$ ($\<$sentence$\>$ [$\land \mid \lor \mid \implies \mid \iff$]
$\<$sentence$\>$) \\
\end{tabular}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Universal quantification}
\key{Existential quantification}
\slide{Quantifiers}{

\item \defn{Universal quantification}

\cen{$\forall\,\<variables\>:~ \<sentence\>$}

$\forall\,x:~ P$ is true in a model $m$ iff $P$ is true
with $x$ being \emph{each} possible object in the model

{\small Example: ``Everyone at Berkeley is smart:'' $\forall\,x:~ At(x,Berkeley) \implies Smart(x)$}

~

\item \defn{Existential quantification}

\cen{$\exists\,\<variables\>:~ \<sentence\>$}

$\exists\,x:~ P$ is true in a model $m$ iff $P$ is true
with $x$ being \emph{some} possible object in the model

{\small Example: ``Someone at Stanford is smart:'' $\exists\,x:~ At(x,Stanford) \land Smart(x)$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Properties of quantifiers}{

\item $\forall\,x:~\forall\,y:~$ is the same as $\forall\,y:~\forall\,x:~$% (\emph{why})

\item $\exists\,x:~\exists\,y:~$ is the same as $\exists\,y:~\exists\,x:~$% (\emph{why})

\item $\exists\,x:~\forall\,y:~$ is \emph{not} the same as $\forall\,y:~\exists\,x:~$

{\small
$\exists\,x:~\forall\,y:~ Loves(x,y)$: ``There is a person who loves everyone in the world''

$\forall\,y:~\exists\,x:~ Loves(x,y)$: ``Everyone in the world is loved by at least one person''

}
\item \emph{Quantifier duality}: each can be expressed using the other

$\forall\,x:~ Likes(x,IceCream) \quad\equiv\quad \lnot \exists\,x:~ \lnot Likes(x,IceCream)$ 

$\exists\,x:~ Likes(x,Broccoli) \quad\equiv\quad \lnot \forall\,x:~ \lnot Likes(x,Broccoli)$ 

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Semantics: Truth in first-order logic}{

\item Sentences are true with respect to a \defn{model} and an \defn{interpretation}

\item A model contains $\geq 1$ objects and
relations among them

\item An interpretation specifies referents for\\ \quad
{constant symbols} $\rightarrow$ \emph{objects}\\ \quad
{predicate symbols} $\rightarrow$ \emph{relations}\\ \quad
{function symbols} $\rightarrow$ \emph{functional relations}

\item An atomic sentence $predicate(term_1,\ldots,term_n)$ is true\\
iff the \emph{objects} referred to by $term_1,\ldots,term_n$
are in the \emph{relation} referred to by $predicate$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Models for FOL: Example}{

\show[.7]{russell/fol-model.pdf}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Models for FOL: Lots!}{

\item Entailment in propositional logic can be computed by enumerating models

\item We can also enumerate the FOL models for a given KB:
\begin{items}
\item For each number of domain elements $n$ from $1$
to $\infty$
\item \quad For each $k$-ary predicate $P_k$ in the vocabulary\\
\item \quad\quad For each possible $k$-ary relation on $n$ objects\\
\item \quad\quad\quad For each constant symbol $C$ in the vocabulary\\
\item \quad\quad\quad\quad For each choice of referent for $C$
from $n$ objects $\ldots$
\end{items}

\item Enumerating FOL models is very inefficient

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example sentences}{

\item ``Brothers are siblings''

\pause

\quad$\forall\,x,y:~ Brother(x,y) \implies Sibling(x,y)$.

\item ``Sibling'' is symmetric

\pause

\quad$\forall\,x,y:~ Sibling(x,y) \iff Sibling(y,x)$.

\item ``One's mother is one's female parent''

\pause

\quad$\forall\,x,y:~ Mother(x,y) \iff (Female(x) \land Parent(x,y))$.

\item ``A first cousin is a child of a parent's sibling''

\pause

\quad{$\forall\,x,y:~ FirstCousin(x,y) \iff 
\exists\,p,ps:~ Parent(p,x) \land Sibling(ps,p) \land Parent(ps,y)$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Equality}{

%% $term_1 = term_2$ is true under a given interpretation\\
%% if and only if $term_1$ and $term_2$ refer to the same object

%% \begin{tabular}{ll}
%% E.g., & $1=2$ and $\forall\,x:~ {\times}(Sqrt(x),Sqrt(x)) = x$ are satisfiable\\
%%       & $2=2$ is valid
%% \end{tabular}

%% E.g., definition of (full) $Sibling$ in terms of $Parent$:\\ \quad
%% $\forall\,x,y:~ Sibling(x,y) \iff [\lnot(x = y) \land \exists\,m,f:~ \lnot(m = f) \land {}$\\
%% $Parent(m,x) \land Parent(f,x) \land Parent(m,y) \land Parent(f,y)]$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{FOL Inference}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{A brief history of reasoning}{

%% \small
%% \cen{\begin{tabular}{lll}
%% \emph{450{\sc b.c.}} & Stoics      & propositional logic, inference (maybe) \\
%% \emph{322{\sc b.c.} }& Aristotle   & ``syllogisms'' (inference rules), quantifiers \\
%% \emph{1565 }& Cardano     & probability theory (propositional logic + uncertainty) \\
%% \emph{1847 }& Boole       & propositional logic (again) \\
%% \emph{1879 }& Frege       & first-order logic \\
%% \emph{1922 }& Wittgenstein& proof by truth tables \\
%% \emph{1930 }& G\"odel     & $\exists$ complete algorithm for FOL \\
%% \emph{1930 }& Herbrand    & complete algorithm for FOL (reduce to propositional) \\
%% \emph{1931 }& G\"odel     & $\lnot\exists$ complete algorithm for arithmetic \\
%% \emph{1960 }& Davis/Putnam& ``practical'' algorithm for propositional logic \\
%% \emph{1965 }& Robinson    & ``practical'' algorithm for FOL---resolution
%% \end{tabular}}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Universal instantiation (UI)}{

\item Whenever a KB contains a universally quantified sentence, we may
add to the KB any instantiation of that sentence, where the logic
variable $v$ is replaced by a concrete ground term $g$:

{$$\frac{\forall\,v:~ \alpha}{\text{Subst}(\{v/g\},\alpha)}$$}

~\small

E.g., $\forall\,x:~ King(x) \land Greedy(x) \implies Evil(x)$ yields
{$$\begin{array}{lll}
   King(John) \land Greedy(John)  \implies Evil(John)\\
   King(Richard) \land Greedy(Richard)  \implies Evil(Richard)\\
   King(Father(John)) \land Greedy(Father(John))  \implies Evil(Father(John))\\
   \quad\vdots
\end{array}$$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Existential instantiation (EI)}{

\item Whenever a KB contains a existentially quantified sentence
$\exists\,v:~ \alpha$, we may add a single instantiation of that sentence
to the KB, where the logic variable $v$ is replaced by a \defn{Skolem constant}
symbol $k$ \emph{which must not appear elsewhere in the knowledge base}:
{$$\frac{\exists\,v:~ \alpha}{\text{Subst}(\{v/k\},\alpha)}$$}

~\small

E.g., $\exists\,x:~ Crown(x) \land OnHead(x,John)$ yields
{$$
Crown(C_1) \land OnHead(C_1,John)
$$}
provided $C_1$ is a new constant symbol, called a \defn{Skolem constant}

Another example: from $\exists\,x:~ {{d(x^y)}/{dy}}  =  x^y$ we obtain
{$$
{{d(e^y)}/{dy}}  =  e^y
$$}
where $e$ is a new constant symbol

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Instantiations contd.}{

\item UI can be applied several times to \emph{add} new sentences;
the new KB is logically equivalent to the old

\item EI can be applied once to \emph{replace} the existential sentence;
the new KB is \emph{not} equivalent to the old, but is satisfiable
iff the old KB was satisfiable

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Reduction to propositional inference}
\slide{Reduction to propositional inference}{

\item Instantiating all quantified sentences allows us
to \defn{ground} the KB, that is, to make the KB propositional

\small

\item Example: Suppose the KB contains just the following:
{$$\begin{array}{lll}
   \forall\,x:~ King(x) \land Greedy(x) \implies Evil(x)\\
   King(John)\\
   Greedy(John)\\
   Brother(Richard,John)
\end{array}$$}
Instantiating the universal sentence in \emph{all possible} ways, we have
{$$\begin{array}{lll}
   King(John) \land Greedy(John) \implies Evil(John)\\
   King(Richard) \land Greedy(Richard) \implies Evil(Richard)\\
   King(John)\\
   Greedy(John)\\
   Brother(Richard,John)
\end{array}$$}
The new KB is \defn{propositionalized}: proposition symbols are
{$
  King(John),\ Greedy(John),\ Evil(John), King(Richard)\, \text{etc.}
$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Theory on propositionalization}{

\item Claim: A ground sentence is entailed by the
propositionalized KB iff entailed by original FOL KB

(or ``Every FOL KB can be propositionalized so as to preserve entailment'')

\item Then, FOL inference can be done by: propositionalize KB and query, apply resolution, return result

\item Problem: with \emph{function} symbols, there are infinitely many ground terms,
  e.g., $Father(Father(Father(John)))$

\item Theorem: Herbrand (1930). If a sentence $\a$ is entailed by an FOL KB,
  it is entailed by a \emph{finite} subset of the propositional KB

\item Idea: For $n$ = $0$ to $\infty$ do\\
    create a propositional KB by instantiating with depth-$n$ terms\\
    see if $\alpha$ is entailed by this KB

\item Problem: works if $\alpha$ is entailed, loops if $\alpha$ is not entailed

\item Theorem: Turing (1936), Church (1936), entailment in FOL is \defn{semidecidable}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Inefficiency of naive propositionalization}{

\item Propositionalization generates lots of irrelevant sentences.

{\small Example:
{$$\begin{array}{lll}
   \forall\,x:~ King(x) \land Greedy(x) \implies Evil(x)\\
   King(John)\\
   \forall\,y:~ Greedy(y)\\
   Brother(Richard,John)
\end{array}$$}
propositionalization produces not only
$Greedy(John)$, but also $Greedy(Richard)$ which is
irrelevant for a query $Evil(John)$

}

\item With $p$ $k$-ary predicates and $n$ constants, there are $p\cdot n^k$ instantiations

With function symbols, it gets much much worse!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Unification}
\slide{Unification}{

\item Instead of instantiating quantified sentences in all possible
ways, we can compute specific substitutions ``that make sense''. These
are substitutions that \emph{unify} abstract sentences so that rules
(Horn clauses, GMP, see next slide) can be applied.

\item In the previous example, the ``Evil-rule'' can be applied if we
can find a substitution $\theta$ such that $King(x)$
and $Greedy(x)$ match $King(John)$
and $Greedy(y)$. Namely, $\theta = \{x/John,y/John\}$ is
such a substitutions.

\cen{We write $\theta \unifies (\alpha,\beta)$
iff $\alpha\theta = \beta\theta$}

\small
\item Examples:
{$$\begin{array}{l|l|l}
p & q & \theta \\
\hline
Knows(John,x) & Knows(John,Jane) & \pause \hbox{\emph{$\{x/Jane\}$}}\\
Knows(John,x) & Knows(y,OJ)      & \pause \hbox{\emph{$\{x/OJ,y/John\}$}}\\
Knows(John,x) & Knows(y,Mother(y))& \pause \hbox{\emph{$\{y/John,x/Mother(John)\}$}}\\
Knows(John,x) & Knows(x,OJ) & \pause \hbox{\emph{$fail$}}
\end{array}$$}
\defn{Standardizing apart} the names of logic variables eliminates the
overlap of variables, e.g., $Knows(z_{17},OJ)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Generalized Modus Ponens}
\slide{Generalized Modus Ponens (GMP)}{

\item For every substitution $\theta$ such that
$\forall_i: \theta \unifies (p_i', p_i)$ we can apply:

{\small$$\frac{{p_1}', ~~ {p_2}', ~ \ldots, ~ {p_n}', ~~
( p_1 \land p_2 \land \ldots \land p_n \Rightarrow q)}{q\theta}$$}

{\small Example:
{$$\begin{array}{lll}
{p_1}'  \text{ is }  King(John)  & p_1 \text{ is }  King(x) \\
{p_2}' \text{ is }  Greedy(y)  & p_2  \text{ is }  Greedy(x) \\
\theta  \text{ is }  \{x/John,y/John\} & q \text{ is } Evil(x) \\
q\theta \text{ is } Evil(John)
\end{array}$$}

}

\item This GMP assumes a KB of \defn{definite clauses} (\emph{exactly}
one positive literal)

By default, all variables are assumed universally quantified.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Soundness of GMP}{

%% Need to show that 
%% {$${p_1}', ~ \ldots, ~ {p_n}', ~~
%% ( p_1 \land \ldots \land p_n \Rightarrow q) \models q\theta$$}
%% provided that ${p_i}'\theta  = p_i\theta$ for all $i$

%% Lemma: For any definite clause $p$, we have $p \models p\theta$ by UI

%% 1. {$( p_1 \land \ldots \land p_n \Rightarrow q) \models 
%%     ( p_1 \land \ldots \land p_n \Rightarrow q)\theta \eq
%%     ( p_1\theta \land \ldots \land p_n\theta \Rightarrow q\theta)$}

%% 2. {$ {p_1}', ~ \ldots, ~ {p_n}' \models
%%      {p_1}' \land \ldots \land {p_n}' \models
%%      {p_1}'\theta \land \ldots \land {p_n}'\theta $}

%% 3. From 1 and 2, $q\theta$ follows by ordinary Modus Ponens

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Forward Chaining}
\slide{Forward chaining algorithm}{

%\input{algorithms/forward-chaining-algorithm}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Crime}{

The law says that it is a crime for an American to sell weapons to
hostile nations.  The country Nono\index{Nono}, an enemy of America,
has some missiles, and all of its missiles were sold to it by Colonel
West, who is American.

Prove that Col.~West is a criminal.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Crime -- formalization}{

\item $\ldots$ it is a crime for an American to sell weapons to hostile nations:\\ \quad\pause
  $American(x) \land  Weapon(y)\land Sells(x,y,z) \land  Hostile(z) \implies Criminal(x)$

\item Nono $\ldots$ has some missiles, i.e., $\exists\,x\ Owns(Nono,x)\land Missile(x)$:\\ \quad\pause
  $Owns(Nono,M_1)$ and $Missile(M_1)$

\item $\ldots$ all of its missiles were sold to it by Colonel West\\ \quad\pause
  $\forall\,x:~ Missile(x) \land Owns(Nono,x) \implies Sells(West,x,Nono)$

\item Missiles are weapons:\\ \quad\pause
  $Missile(x)\Rightarrow Weapon(x)$

\item An enemy of America counts as ``hostile'':\\ \quad\pause
  $Enemy(x,America)\implies Hostile(x)$

\item West, who is American $\ldots$\\ \quad\pause
  $American(West)$

\item The country Nono, an enemy of America $\ldots$\\ \quad\pause
  $Enemy(Nono,America)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Crime -- forward chaining proof}{

~

\only<1>{\show[1.0]{russell/crime-fc1c.pdf}}
\only<2>{\show[1.0]{russell/crime-fc2c.pdf}}
\only<3>{\show[1.0]{russell/crime-fc3c.pdf}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Properties of forward chaining}{

\item Sound and complete for first-order definite clauses 
(proof similar to propositional proof)

\item \defn{Datalog} = first-order definite clauses + \emph{no
functions} (e.g., crime KB). Forward chaining terminates for Datalog in poly iterations: at most $p\cdot n^k$ literals

\item May not terminate in general if $\alpha$ is not entailed

This is unavoidable: entailment with definite clauses is semidecidable

\item Efficiency:
\begin{items}
\item Simple observation: no need to match (=compute possible
substitutions) a rule on iteration $k$
if a premise wasn't added on iteration $k-1$ $\implies$ match
only rules whose premise contain a newly added literal

\item Matching (computing substitutions) can be expensive:
\begin{items}
\item \defn{Database indexing} allows $O(1)$ retrieval of known facts,
e.g., query $Missile(x)$ retrieves $Missile(M_1)$
\item But matching conjunctive premises against known facts is NP-hard
(is a CSP problem, see below)
\end{items}
\end{items}
%Forward chaining is widely used in \defn{deductive databases}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{Hard matching example: a CSP}{

\show[.3]{russell/australia-csp.pdf}

\item Consider the KB:\\
$\textit{Diff}(wa,nt)\land \textit{Diff}(wa,sa)\land\textit{Diff}(nt,q)\land \textit{Diff}(nt,sa)\land {}$\\
$\textit{Diff}(q,nsw)\land \textit{Diff}(q,sa)\land\textit{Diff}(nsw,v)\land \textit{Diff}(nsw,sa)\land {}$\\
$\textit{Diff}(v,sa) \implies Colorable()$\\
$\textit{Diff}(Red,Blue), \quad \textit{Diff}(Red,Green), \quad \textit{Diff}(Green,Red)$\\
$\textit{Diff}(Green,Blue), \quad \textit{Diff}(Blue,Red), \quad \textit{Diff}(Blue,Green)$

\item 
$Colorable()$ is inferred iff the CSP has a solution\\
CSPs include 3SAT as a special case, hence matching is NP-hard

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Backward Chaining}
\slide{Backward chaining algorithm*}{

%\input{algorithms/backward-chaining-algorithm}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Backward chaining example*}{

~

\only<1>{\show[1.06]{russell/crime-bc01c.pdf}}
\only<2>{\show[1.06]{russell/crime-bc02c.pdf}}
\only<3>{\show[1.06]{russell/crime-bc03c.pdf}}
\only<4>{\show[1.06]{russell/crime-bc04c.pdf}}
\only<5>{\show[1.06]{russell/crime-bc05c.pdf}}
\only<6>{\show[1.06]{russell/crime-bc06c.pdf}}
\only<7>{\show[1.06]{russell/crime-bc07c.pdf}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Properties of backward chaining*}{

\item Depth-first recursive proof search: space is linear in size of proof

\item Incomplete due to infinite loops\\ \quad
$\implies$ fix by checking current goal against every goal on stack

\item Inefficient due to repeated subgoals (both success and failure)\\ \quad
$\implies$ fix using caching of previous results (extra space!)

\item Widely used (without improvements!) for \defn{logic programming}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Prolog*}{

\item Declarative vs.\ imperative programming:

\hspace*{-5mm}{\small
\begin{tabular}{lll}
    & \emph{Logic programming}     & \emph{Ordinary programming}  \\
\hline
1.  & Identify problem             & Identify problem             \\
2.  & Assemble information         & Assemble information         \\
3.  & Tea break                    & Figure out solution          \\
4.  & Encode information in KB     & Program solution             \\
5.  & Encode problem instance as facts & Encode problem instance as data  \\
6.  & Ask queries                  & Apply program to data \\
7.  & Find false facts             & Debug procedural errors \\
\end{tabular}

}

~

\item Russell says ``should be easier to debug $Capital(NewYork,US)$ than $x:= x+2$!''...

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\begin{slidecore}{Prolog systems*}

\item Basis: backward chaining with Horn clauses + bells \& whistles\\
Widely used in Europe, Japan (basis of 5th Generation project)\\
Compilation techniques $\Rightarrow$ approaching a billion LIPS

\item Program = set of clauses ~ {\tt head :- literal$_1$, $\ldots$ literal$_n$.}
{\small\begin{verbatim}
   criminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).
\end{verbatim}}

\item Closed-world assumption (``negation as failure'')\\ \quad
   e.g., given {\tt alive(X) :- not dead(X).}\\ \quad
   {\tt alive(joe)} succeeds if {\tt dead(joe)} fails

\item Details:
\begin{items}
\item Efficient unification by \defn{open coding}
\item Efficient retrieval of matching clauses by direct linking
\item Depth-first, left-to-right backward chaining
\item Built-in predicates for arithmetic etc., e.g., {\tt X is Y*Z+3}
\end{items}

\end{slidecore}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
\begin{slidecore}{Prolog examples*}

\item Depth-first search from a start state {\tt X}:
\begin{verbatim}
dfs(X) :- goal(X).
dfs(X) :- successor(X,S),dfs(S).
\end{verbatim}
No need to loop over {\tt S}: {\tt successor} succeeds for each

\item Appending two lists to produce a third:
\begin{verbatim}
append([],Y,Y).                         
append([X|L],Y,[X|Z]) :- append(L,Y,Z). 
                                        
query:   append(A,B,[1,2]) ?            
answers: A=[]    B=[1,2]
         A=[1]   B=[2]
         A=[1,2] B=[]
\end{verbatim}

\end{slidecore}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Conversion to CNF}
\slide{Conversion to CNF}{

Everyone who loves all animals is loved by someone:\\ \quad 
  $\forall\,x:~ [\forall\,y:~ Animal(y) \implies Loves(x,y)] \implies [\exists\,y:~ Loves(y,x)]$

1. Eliminate biconditionals and implications
{$$
 \forall\,x:~ [\lnot \forall\,y:~ \lnot Animal(y) \lor Loves(x,y)] \lor [\exists\,y:~ Loves(y,x)]
$$}
2. Move $\lnot$ inwards: $\lnot \forall\,x,p:~ \equiv \exists\,x:~ \lnot p$,\quad
                         $\lnot \exists\,x,p:~ \equiv \forall\,x:~ \lnot p$:
{$$\begin{array}{lll}
 \forall\,x:~ [\exists\,y:~ \lnot(\lnot Animal(y) \lor Loves(x,y))] \lor [\exists\,y:~ Loves(y,x)] \\
 \forall\,x:~ [\exists\,y:~ \lnot\lnot Animal(y) \land \lnot Loves(x,y)] \lor [\exists\,y:~ Loves(y,x)] \\
 \forall\,x:~ [\exists\,y:~ Animal(y) \land \lnot Loves(x,y)] \lor [\exists\,y:~ Loves(y,x)] 
\end{array}$$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conversion to CNF contd.}{

3. Standardize variables: each quantifier should use a different one
{$$
 \forall\,x:~ [\exists\,y:~ Animal(y) \land \lnot Loves(x,y)] \lor [\exists\,z:~ Loves(z,x)] 
$$}
4. Skolemize: a more general form of existential instantiation.\\
   Each existential variable is replaced by a \defn{Skolem function}\\
   of the enclosing universally quantified variables:
{$$
 \forall\,x:~ [Animal(F(x)) \land \lnot Loves(x,F(x))] \lor Loves(G(x),x)
$$}
5. Drop universal quantifiers:
{$$
 [Animal(F(x)) \land \lnot Loves(x,F(x))] \lor Loves(G(x),x)
$$}
6. Distribute $\land$ over $\lor$:
{$$
 [Animal(F(x)) \lor Loves(G(x),x)] \land [\lnot Loves(x,F(x)) \lor Loves(G(x),x)]
$$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Resolution}
\slide{Resolution: brief summary}{

\item For any substitution $\theta\unifies(\ell_i,\lnot m_j)$ for some
$i$ and $j$, apply:

{\cen{$\frac{\ell_1 \lor \cdots\lor \ell_k,\qquad m_1 \lor \cdots\lor m_n}
        {(\ell_1 \lor \cdots\lor \ell_{i-1}\lor \ell_{i+1}\lor\cdots\lor \ell_k
        \lor m_1 \lor \cdots \lor m_{j-1}\lor m_{j+1}\lor\cdots\lor m_n)\theta}
$}}

{
\small Example:
{$$\begin{array}{lll}
{\begin{array}{l} \lnot Rich(x) \lor Unhappy(x), \quad   Rich(Ken)
 \end{array}}
\over
{\begin{array}{l} Unhappy(Ken)
 \end{array}}
\end{array}$$}
with $\theta = \{x/Ken\}$

}

\item Apply resolution steps to $CNF(KB\land \lnot \alpha)$; complete for FOL

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: crime -- resolution proof}{\label{lastpage}

\vspace*{0.2in}

\show[1.0]{russell/crime-resolution.pdf}

}

\slidesfoot
