\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\exnum}{6}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Programmieraufgabe: Spamfilter mit Naive Bayes}

Sie haben in der Vorlesung grafische Modelle und Inferenz in ihnen
kennengelernt. Auf dieser Grundlage basiert der viel verwendete \emph{Naive
Bayes} Klassifikator. Der Bayes Klassifikator wird zum Beispiel dafür verwandet,
Spam Emails automatisch zu erkennen. Dafür werden Trainings-Emails untersucht
und die Wahrscheinlichkeit des Auftreten eines Wortes bestimmt, abhängig davon, ob es eine Spam- oder Ham-Email ist.

Sei $c \in \{\text{Spam, Ham}\}$ die binäre Zufallsvariable, die angibt, ob es sich bei einer Email um Spam oder Ham handelt. Sei $x_i$ das $i$te Wort einer Email $\mathbf{X}$. Sei $p(x|c)$ die Wahrscheinlichkeit, dass ein Wort $x$ in einer Spam- bzw. Ham-Email vorkommt, die während des Trainings für jedes mögliche Wort bestimmt wurde. Dann berechnet der Naive-Bayes-Klassifikator
\begin{align*}
  p(c, \mathbf{X}) &= p(c) \prod_{i=1}^{D} p(x_i | c) \\
  p(c \| \mathbf{X}) &= \frac{p(c, \mathbf{X})}{p(\mathbf{X})}
  = \frac{p(c, \mathbf{X})}{\sum_c p(c,\mathbf{X})}
\end{align*}
als die Wahrscheinlichkeit, dass die Email $\mathbf{X}$ Spam oder Ham ist, wobei $D$ die Zahl der Wörter $x_i$ in der Email $\mathbf{X}$ ist.

(In praktischen Implementierungen werden häufig nur Wörter
berücksichtigt, bei denen $p(x|\text{Spam})$ und $p(x|\text{Ham})$
nicht Null sind.)

%% Nun kann man für eine neue eintreffende Email die vorkommenden Wörter
%% $\mathbf{x}^*$
%% analysieren und mit Hilfe von Bayes Formel die
%% Wahrscheinlichkeit berechnen, ob diese Email Spam oder Ham ist.

%% \begin{equation}
%%   p(c | \mathbf{x}^*) = \frac{p(\mathbf{x}^*| c) p(c)}{p(\mathbf{x}^*)} =
%%   \frac{p(\mathbf{x}^*|c) p(c)}{\sum_c p(\mathbf{x}^*|c) p(c)}
%% \end{equation}

\textbf{Aufgabe:} Implementieren Sie einen Naive Bayes Klassifikator für die
Spam-Emails. Sie finden Trainingsdaten und Python-Code, der mit diesen umgehen
kann, in Ihrem Repository.

Ihre Implementierung sollte zwei Funktionen enthalten:

\begin{verbatim}
class NaiveBayes(object):
    def train(self, database):
        ''' Train the classificator with the given database. '''
        pass

    def spam_prob(self, email):
        ''' Compute the probability for the given email that it is spam. '''
        return 0.

\end{verbatim}
    

\textbf{Tip:} David Barber gibt ein seinem Buch ``Bayesian Reasoning and Machine Learning''
eine sehr gute Einführung in den Naive Bayes Klassifikator (Seite 243 ff., bzw.
Seite 233 ff. in der kostenlosen Online Version des Buches, die man unter 
\url{http://www.cs.ucl.ac.uk/staff/d.barber/brml/} herunterladen kann). Zudem: Log-Wahrscheinlichkeiten zu addieren ist eine numerisch stabile Alternative zum Multiplizieren von Wahrscheinlichkeiten.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Votieraufgabe: Hidden Markov Modelle}

(Teilaufgaben werden separat votiert.)

Sie stehen bei Nacht auf einer Br"ucke "uber der B14 in Stuttgart und
m"ochten z"ahlen, wieviele LKW, Busse und Kleintransporter in Richtung
Bad Canstatt fahren. Da Sie mehrere Spuren gleichzeitig beobachten und
es dunkel ist machen Sie folgende Fehler bei der Beobachtung des Verkehrs:
\begin{itemize}
\item Einen LKW erkennen Sie in 30\% der F"alle als Bus, in 10\% der
  F"alle als Kleintransporter.
\item Einen Bus erkennen Sie in 40\% der F"alle als LKW, in 10\% der
  F"alle als Kleintransporter.
\item Einen Kleintransporter erkennen Sie in je 10\% der F"alle als
 Bus bzw. LKW.
\end{itemize}
Zudem nehmen Sie folgendes an:
\begin{itemize}
\item Auf einen Bus folgt zu 10\% ein Bus und zu 30\% ein LKW,
  ansonsten ein Kleintransporter.
\item Auf einen LKW folgt zu 60\% ein Kleintransporter und zu 30\% ein
  Bus, ansonsten ein weiterer LKW.
\item Auf einen Kleintransporter folgt zu 80\% ein Kleintransporter
  und zu je 10\% ein Bus bzw. ein LKW.
\end{itemize}
Sie wissen sicher, dass das erste beobachtete Fahrzeug tatsächlich ein
Kleintransporter ist.

a) Formulieren Sie das HMM dieses Szenarios. D.h., geben Sie explizit
$P(X_1)$, $P(X_{t\po}|X_t)$ und $P(Y_t|X_t)$ an.

b) Prädiktion: Was ist die Marginal-Verteilung $P(X_3)$ über das
3. Fahrzeug.

c) Filtern: Sie machten die Beobachtungen $Y_{1:3}=(K, B,
B)$. Was ist die Wahrscheinlichkeit $P(X_3|Y_{1:3})$ des 3. Fahrzeugs
gegeben diese Beobachtungen?

d) Glätten: Was ist die Wahrscheinlichkeit $P(X_2|Y_{1:3})$ des 2. Fahrzeugs,
gegeben die 3 Beobachtungen?

e) Viterbi (wahrscheinlichste Folge): Was ist die wahrscheinlichste
Folge \\ $\argmax_{X_{1:3}} P(X_{1:3}|Y_{1:3})$ an Fahrzeugen, gegeben
  die 3 Beobachtungen?

\begin{solution}
a) Wir schreiben Wahrscheinlichkeitstabellen als ``Vektor'' und
``Matrix'', wobei der Zeilenindex (wie bei Matrizen) das \emph{erste}
Argument indiziert. Alle Indizes entsprechen der Reihenfolge L (LKW),
B (Bus), K (Kleintransporter).
\begin{align}
P(X_1) = \vec p = \mat{c}{ 0 \\ 0 \\ 1} \comma
P(X_{t\po}|X_t) = \vec T = \mat{ccc}{
.1 & .3 & .1 \\
.3 & .1 & .1 \\
.6 & .6 & .8 }\comma
P(Y_t|X_t) = \mat{ccc}{
.6 & .4 & .1 \\
.3 & .5 & .1 \\
.1 & .1 & .8 }
\end{align}

b)
\begin{align}
P(X_3)
&= \sum_{X_1,X_2} P(X_1,X_2,X_3)
 = \sum_{X_1,X_2} P(X_3|X_2)~ P(X_2|X_1)~ P(X_1) = \vec T^2 \vec p
\end{align}

c) Dies ist die ``Vorwärts-Nachricht'' $\a_3(X_3)$ multipliziert mit der
Beobachtungs-Nachricht $\r_3(X_3)$. Wir notieren mit $\cdot$ 
das element-weise Produkt:
\begin{align}
P(X_3|Y_{1:3})
&\propto \r_3 \cdot \a_3
= \r_3 \cdot \vec T (\r_2 \cdot \a_2)
= \r_3 \cdot \vec T (\r_2 \cdot \vec T(\r_1 \cdot \a_1))\\
&\quad \a_1 = \vec p = \mat{c}{ 0 \\ 0 \\ 1} \comma
\r_1 = \mat{c}{ .1 \\ .1 \\ .8} \comma
\r_2 = \r_3 = \mat{c}{ .3 \\ .5 \\ .1}
\end{align}
Beachte: Das obige Message-Passing ist äquivalent zum
expliziten Ausmarginalisieren:
\begin{align}
P(X_3|Y_{1:3})
\propto \sum_{X_1, X_2} P(X_{1:3},Y_{1:3})
= P(Y=B|X_3)~ \sum_{X_2} P(X_3|X_2)~ P(Y=B|X_2)~ \sum_{X_1} P(X_2|X_1)~ P(Y=K|X_1)~ P(X_1)
\end{align}


d) \begin{align}
P(X_2|Y_{1:3})
&\propto \b_2 \cdot \r_2 \cdot \a_2
= [\vec T^\T (\b_3 \cdot \r_3)] \cdot \r_2 \cdot [\vec
T(\r_1 \cdot \a_1)] \comma \b_3(X_3) := P(\emptyset | X_3) \equiv \mat{c}{1\\1\\1}
\end{align}
Bemerkung: Auch dies ist äquivalent zum Marginalisieren $\sum_{X_1, X_3}
P(X_{1:3},Y_{1:3})$.

e) [Nicht Klausur-relevant.] Wir berechnen zunächst
Viterbi-Vorwärtsnachrichten $\hat\a_t$ indem wir die Summation (implizit in den
obigen Matrixmultiplikationen) durch Maximierung ersetzen. Zur
Vereinfachung schreiben wir $A\cdot x$ für ein ``element-weises
Produkt'' von Matrix $A$ mit Vektor $x$ (genauer, Tensorprodukt).
\begin{align}
\hat a_2
&= \max\{ \vec T\cdot (\r_1 \cdot \vec p) \} 
 = \max\{ \vec T\cdot \mat{c}{.0 \\ .0 \\ .8 } \} 
 = .8~ \max\{ \mat{ccc}{
0 & 0 & .1 \\
0 & 0 & .1 \\
0 & 0 & .8 } \} = .8~ \mat{c}{ .1 \\ .1 \\ .8 } \\
\hat a_3
&= \max\{ \vec T\cdot (\r_2 \cdot \hat\a_2) \} 
 = .8~ \max\{ \vec T\cdot \mat{c}{.03 \\ .05 \\ .08 } \} 
 = .8~ \max\{ \mat{ccc}{
.003 & .015 & .008 \\
.009 & .005 & .008 \\
.018 & .030 & .064 } \} = .8~ \mat{c}{ .015 \\ .009 \\ .064 } \\
\hat a_3 \cdot \r_3
&= .8~ \mat{c}{ .0045 \\ .0045 \\ .0064 }
\end{align}
Damit ist die wahrscheinlichste Belegung von $X_3$ Kleinbus. Man
könnte auf gleiche Weise Viterbi-Rückwärtsnachrichten berechnen und
somit für alle Variablen die wahrscheinlichste Belegung
finden. Effizienter ist das sogenannte Viterbi-Decoding: Verfolgt
man zurück, durch welche $\max$-Entscheidungen die Zahl $.0064$
zustande kam, so erkennt man als wahrscheinlichstes $X_{1,2,3}^*=$
(Kleinbus, Kleinbus, Kleinbus).

\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exerfoot
