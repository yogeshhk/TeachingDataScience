\input{../shared/shared}

\renewcommand{\course}{Robotics}
\renewcommand{\coursepicture}{roboticsLecture}
\renewcommand{\coursedate}{Winter 2014}
\renewcommand{\exnum}{10}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{The value function in Markov Decision Processes}

On slide 5 of the RL lecture we defined MDPs as
\begin{align}
P(s_{0:T\po},a_{0:T},r_{0:T};\pi) = P(s_0)~ \prod_{t=0}^T
P(a_t|s_t;\pi)~ P(r_t|s_t,a_t)~ P(s_{t\po}|s_t,a_t) ~,
\end{align}
where $P(a_0|s_0;\pi)$ described the agent's policy. We assume a
deterministic agent and write $a_t = \pi(s_t)$. The \emph{value} of a
state $s$ is defined as the expected discounted sum of future rewards,
\begin{align}
V^\pi(s) = \Exp{ r_0 + \g r_1 + \g^2 r_2 + \cdots \| s_0\=s }
\end{align}
given that the agent starts in state $s$ and from there executes the
policy $\pi$.

a) Prove
\begin{align}
V^\pi(s)
 &= \Exp{r_0 \| s, \pi(s)} ~+~ \g \tsum_{s'} P(s'\|s,\pi(s))~ V^\pi(s')~.
\end{align}

b) Consider the following T-maze:\\
\show[.5]{mouseMaze2}
We distinguish 7 states $s_1,..,s_7$ in the maze. The first 3 states
are the T-junctions; the last 4 states receive rewards
$(4,0,2,3)$. At each T-junction we have two possible actions: left,
right. Everything is deterministic. Assume a discounting $\g=0.5$.

Compute (by hand) the value function $V^{\pi}$ over all states when $\pi$ is
the \emph{random policy} (50/50 left/right at each junction).

c) Bellman's principle of optimality says that the optimal policy
$\pi^*$ has a value function $V^{\pi^*}(s)=V^*(s)$,
\begin{align}
V^*(s)
 &= \max_a\[ \Exp{r_0 \| s, a} ~+~ \g \tsum_{s'} P(s'\|s,a)~
 V^\pi(s') \]~.
\end{align}

Compute (by hand) the optimal value function $V^*$ over all states for the
example above.

d) Now consider continuous state $s$ and action $a$. Let the policy be
stochastic and linear in features $\phi(s)\in\RRR^k$, that is,
\begin{align}
\pi(a|s;\b)
 &= \NN(a | \phi(s)^\T \b,  \phi(s)^\T \S \phi(s)) ~.
\end{align}
The covariance matrix $\phi(s)^\T \S \phi(s)$ describes that each action
$a_t=\phi(s_t)^\T (\b+\e_t)$ was generated by adding a noise term
$\e_t \sim \NN(0,\S)$ to the parameter $\b$. We always start in the same state $\hat s$ and the value $V^\pi(s_0)$ is
\begin{align}
V^\pi(\hat s) = \Exp{ \sum_{t=0}H r_t \| s_0\=\hat s }
\end{align}
(no discounting, but only a finite horizon $H$.)

Optimality now requires that $\frac{\del
V^{\pi(\b)}}{\del\b}=0$. Assume that $a\in\RR$ is just 1-dimensional
and $\S\in\RRR$ just a number. Try to prove (see slide 18) that we can
derive
\begin{align*}
\b^*
&= \b^\old +
\[\Exp[\xi|\b]{\sum_{t=0}^H W(s_t) Q^{\pi(\b)}(s_t,a_t,t)}\]^\1
\Exp[\xi|\b]{\sum_{t=0}^H W(s_t) e_t Q^{\pi(\b)}(s_t,a_t,t)}
\comma W(s) = \phi(s) (\phi(s)^\T \S \phi(s))^\1 \phi(s)^\T
\end{align*}
from $\frac{\del V^{\pi(\b^*)}}{\del\b}=0$. (In the scalar case,
$W(s)=1$.) As a first step, derive
$$\frac{\del}{\del\b}\log \pi(a|s)$$
Then insert $a_t = \phi(s_t)^\T(\b^\old+\e_t)$ and solve
$$\Exp[\xi|\b]{\sum_{t=0}^H \frac{\del}{\del\b}\log \pi(a_t|s_t) ~ Q(s_t,a_t,t) } = 0$$
for $\b$. This shows how you can
get the optimal policy parameters $\b^*$ based on samples generated
with $\b$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exerfoot



