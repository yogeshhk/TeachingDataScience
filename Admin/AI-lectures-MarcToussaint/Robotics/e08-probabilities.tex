\input{../shared/shared}

\renewcommand{\course}{Robotics}
\renewcommand{\coursepicture}{roboticsLecture}
\renewcommand{\coursedate}{Winter 2014}
\renewcommand{\exnum}{8}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Bayes Basics}

a) Box 1 contains 8 apples and 4 oranges. Box 2 contains 10 apples and 2
oranges. Boxes are chosen with equal probability. What is the
probability of choosing an apple? If an apple is chosen, what is the
probability that it came from box 1?

b) The Monty Hall Problem: I have three boxes. In one I put a prize,
and two are empty. I then mix up the boxes. You want to pick the box
with the prize in it. You choose one box. I then open \emph{another}
one of the two remaining boxes and show that it is empty. I then give
you the chance to change your choice of boxes---should you do so?

c) Given a joint probability $P(X,Y)$ over 2 binary random variables
as the table\\
\cen{\begin{tabular}{c|c|c}
& Y=0 & Y=1 \\
\hline
X=0 & .06 & .24 \\
\hline
X=1 & .14 & .56 \\
\end{tabular}}
What are $P(X)$ and $P(Y)$? Are $X$ and $Y$ conditionally independent?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \exsection{Conditionalized versions of product and Bayes rule}

%% Prove from first principles the conditionalized version of the product
%% rule (the same as the product rule, but every term is additionally
%% conditioned on $Z$):
%% \begin{align}
%%   P (X, Y | Z) = P(Y | X, Z)~ P (X|Z) ~.
%% \end{align}
%% Also prove the conditionalized version of Bayesâ€™ rule
%% \begin{align}
%%   P (X|Y, Z) = \frac{P (Y |X, Z)P (X|Z)}{P (Y |Z)} ~.
%% \end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\exsection{Gaussians}

On slide 06:17 there is the definition of a multivariate ($n$-dim)
Gaussian distribution. Proof the following using only the
definition. (You may ignore terms independent of $x$.)

a) Proof that:

~~ $\NN(x|a,A) = \NN(a|x,A)$

~~ $\NN(x\|a,A) = |F|~ \NN(Fx \| Fa,~FAF^\T)$

~~ $\NN(F x + f \| a,A) = \frac{1}{|F|}~ \NN(x \| ~ F^\1 (a-f),~ F^\1 AF^\mT)$

b) Multiplying two Gaussians is essential in many algorithms
(typically, a prior and a likelihood, to get a posterior). Prove the
general rule
\begin{align}
&\NN(x \| a,A)~ \NN(x \| b,B) \feed
 &\propto \NN(x \| (A^\1+B^\1)^\1 (A^\1 a + B^\1 b) ,(A^\1+B^\1)^\1) ~,
\end{align}
where the proportionality $\propto$ allows you to drop all terms
independent of $x$.

{\small Note: The so-called canonical form of a Gaussian is defined as
$\NN[x \| \bar a,\bar A] = \NN(x \| \bar A^\1 \bar a, \bar A^1)$; in
this convention the product reads much nicher: $\NN[x \| \bar a,\bar
A]~ \NN[x \| \bar b,\bar B] \propto \NN[x \| \bar a+\bar b,\bar A+\bar
B]$. }

c) The ``forward propagation'' of a Gaussian belief $\NN(y \| b, B)$
along a stochastic linear dynamics $\NN(x \| a + Fy, A)$ is given as
\begin{align}
\int_y \NN(x \| a + Fy, A)~ \NN(y \| b, B)~ dy = \NN(x \| a + Fb,
A+FBF^\T)
\end{align}
Prove the equation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \exsection{A kind of particle filter}

%% A distribution over continuous domain can be represented as
%% a \emph{set of weighted particles} $\{ (x^i, w^i) \}_{i=1}^N$ (see
%% slide 06:12). In other contexts, this is also called empirical
%% distribution, or polulation, or sample set.

%% In this exercise you write a simple kind of evolutionary algorithm
%% that replaces fitness selection by Bayes rule. First sample a
%% population of $N=100$ particles in 2D space uniformly from
%% $[-1,1] \times \in[-1,1]$. Assume that 




\exerfoot
