\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Local Learning \& Emsemble Learning}
\renewcommand{\keywords}{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Local learning and ensemble approaches}
\slide{Local learning \& Ensemble learning}{

\item ``Simpler is Better''
\pause
\begin{items}
\item We've learned about [kernel] ridge\|logistic regression
\item We've learned about high-capacity NN training
\item Sometimes one should consider also much simpler methods as baseline
\end{items}

~

\item Content:
\begin{items}
\item Local learners
\begin{items}[7]
\item local \& lazy learning, kNN, Smoothing Kernel, kd-trees
\end{items}

\item Combining weak or randomized learners
\begin{items}[7]
\item Bootstrap, bagging, and model averaging
\item Boosting
\item (Boosted) decision trees \& stumps, random forests
\end{items}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Local \& lazy learning}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Smoothing kernel}
\slide{Local \& lazy learning}{

\item Idea of local (or ``lazy'') learning:

Do not try to build one global model $f(x)$ from the data. Instead,
whenever we have a query point $x^*$, we build a specific local model
in the neighborhood of $x^*$.

~\mypause

\item Typical approach:

-- Given a query point $x^*$, find all $k$NN in the data
$D=\{(x_i,y_i)\}_{i=1}^N$

-- Fit a local model $f_{x^*}$ only to these $k$NNs, perhaps weighted

-- Use the local model $f_{x^*}$ to predict $x^* \mapsto \hat y_0$

~\mypause

\item \textbf{Weighted local least squares:}

\medskip
\eqbox{$L^{\text{local}}(\b,x^*) = \sum_{i=1}^n K(x^*,x_i) (y_i
- f(x_i))^2 + \l \norm{\b}^2$}
\medskip

where $K(x^*,x)$ is called \textbf{smoothing kernel}. The optimum is:
$$\hat \b = (X^\T W X + \l I)^\1 X^\T W y
  \comma W=\diag(K(x^*,x_{1:n}))$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{kNN smoothing kernel}
\key{Epanechnikov quadratic smoothing kernel}
\slide{Regression example}{

\small

kNN smoothing kernel: $K(x^*,x_i) = \begin{cases}
1 & \text{if } x_i \in \text{kNN}(x^*)\\
0 & \text{otherwise} \end{cases}$

Epanechnikov quadratic smoothing kernel:
$K_\l(x^*,x) = D(|x^*-x|/\l) \comma D(s) =
 \begin{cases} \frac{3}{4}(1-s^2) & \text{if }s\le 1\\
0 & \text{otherwise} \end{cases}$

~

~

\show[.45]{kernelSmoothing1}

\show[.45]{kernelSmoothing2}

\vspace*{-5mm}{\tiny\hfill(Hastie, Sec 6.3)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Smoothing Kernels}{

\show[.6]{SmoothingKernels}
{\tiny\hfill from Wikipedia}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Which metric to use for NN?}{

\item This is \emph{the} crutial question? The fundamental question of generalization.
\begin{items}
\item Given a query $x^*$, which data points $x_i$ would you consider as being ``related'', so that the label of $x_i$ is correlated to the correct label of $x^*$?
\end{items}

~\pause

\item Possible answers beyond naive Euclidean distance $|x^*-x_i|$
\begin{items}
\item Some other kernel function $k(x^*, x_i)$
\item First encode $x$ into a ``meaningful'' latent representation $z$; then use Euclidean distance there
\pause
\item Take some off-the-shelf pretrained image NN, chop of the head, use this internal representation
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{kd-trees}
\slide{kd-trees}{

\item For local \& lazy learning it is essential to efficiently
retrieve the kNN

~

Problem: Given data $X$, a query $x^*$, identify the kNNs of
$x^*$ in $X$.

~

\item Linear time (stepping through all of $X$) is far too slow.

~

A kd-tree pre-structures the data into a binary tree, allowing $O(\log n)$
retrieval of kNNs.

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{kd-trees}{

\show{kdt}
{\hfill\tiny(There are ``typos'' in this figure... Exercise to find them.)}

~

\item Every node plays two roles:

{\small

-- it defines a hyperplane that separates the data along \emph{one}
   coordinate

-- it hosts a data point, which lives exactly on the hyperplane
(defines the division)

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{kd-trees}{

\item Simplest (non-efficient) way to construct a kd-tree:

{\small
-- hyperplanes divide alternatingly along 1st, 2nd, ... coordinate

-- choose random point, use it to define hyperplane, divide data,
   iterate

}

~

\item Nearest neighbor search:

{\small
-- descent to a leave node and take this as initial nearest point

-- ascent and check at each branching the possibility that a nearer
   point exists on the other side of the hyperplane

}

~

\item Approximate Nearest Neighbor ~ (libann on Debian..)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Combining weak and randomized learners}{

%% \tiny
%% -- Bootstrap, bagging, and model averaging

%% -- Boosting

%% -- (Boosted) decision trees \& stumps

%% -- Random forests

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Combining learners}{

\item The general idea is:

-- Given data $D$, let us learn \emph{various models} $f_1,..,f_M$

-- Our prediction is then some combination of these, e.g.
$$f(x) = \sum_{m=1}^M \a_m f_m(x)$$

~

\item \emph{``Various models''} could be:

\medskip

\textbf{Model averaging:}~ Fully different types of models (using
different (e.g.\ limited) feature sets; neural nets; decision trees;
hyperparameters)

\medskip

\textbf{Bootstrap:}~ Models of same type, trained on randomized versions of $D$

\medskip

\textbf{Boosting:}~ Models of same type, trained on cleverly designed
modifications/reweightings of $D$

~

\item How can we choose the $\a_m$? ~ {\tiny (You should know that!)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bootstrapping}
\key{Bagging}
\slide{Bootstrap \& Bagging}{

\item \textbf{Bootstrap:}

-- Data set $D$ of size $n$

-- Generate $M$ data sets $D_m$ by resampling $D$ \emph{with
replacement}

-- Each $D_m$ is also of size $n$ ~ (some samples doubled or missing)

~

{\small

-- Distribution over data sets $\oto$ distribution over $\b$ (compare slide 02:13)

-- The ensemble $\{f_1,..,f_M\}$ is similar to cross-validation

-- Mean and variance of $\{f_1,..,f_M\}$ can be used for model
   assessment

}

~

~

\item \textbf{Bagging:} (``bootstrap aggregation'')
$$f(x) = \frac{1}{M} \sum_{m=1}^M f_m(x) $$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Bagging has similar effect to regularization:

~

\show[.5]{bagging}
{\tiny\hfill(Hastie, Sec 8.7)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Model averaging}
\slide{Bayesian Model Averaging}{

\item If $f_1,..,f_M$ are very different models

-- Equal weighting would not be clever

-- More confident models {\tiny(less variance, less parameters,
   higher likelihood)} \\ ~~ $\to$ higher weight

~

\item Bayesian Averaging

$$P(y|x) = \sum_{m=1}^M P(y|x,f_m,D)~ P(f_m|D)$$

The term $P(f_m|D)$ is the weighting $\a_m$: it is high, when the
model is likely under the data ($\oto$ the data is likely under the
model \& the model has ``fewer parameters'').

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Weak learners as features}
\slide{The basis function view:~ Models are features!}{

\item Compare model averaging $f(x) = \sum_{m=1}^M \a_m f_m(x)$ with
regression:
$$f(x) = \sum_{j=1}^k \phi_j(x)~ \b_j = \phi(x)^\T \b$$

~

\item We can think of the $M$ models $f_m$ as \textbf{features}
$\phi_j$ for linear regression!

-- We know how to find optimal parameters $\a$

-- But beware overfitting!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Boosting}
\slide{Boosting}{

\item In Bagging and Model Averaging, the models are trained on the
``same data'' (unbiased randomized versions of the same data)

~

\item Boosting tries to be cleverer:

-- It adapts the data for each learner

-- It assigns each learner a differently \emph{weighted} version of the data

~

\item With this, boosing can

-- \emph{Combine many ``weak'' classifiers to produce a powerful
  ``committee''}

-- A weak learner only needs to be somewhat better than random

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{AdaBoost**}
\slide{AdaBoost**}{

(Freund \& Schapire, 1997)

(classical Algo; use Gradient Boosting instead in practice)

~

\item Binary classification problem with data
$D=\{(x_i,y_i)\}_{i=1}^n$, $y_i\in\{-1,+1\}$

\item We know how to train discriminative functions $f(x)$; let
$$G(x) = \sign{f(x)} ~~ \in\{-1,+1\}$$ 

~

\item We will train a sequence of classificers $G_1,..,G_M$, each on
differently weighted data, to yield a classifier
$$G(x) = \sign{\sum_{m=1}^M \a_m G_m(x)}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{AdaBoost**}{ 

\show[.55]{adaboost}
{\tiny\hfill(Hastie, Sec 10.1)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{AdaBoost**}{


\begin{algo}
\Require data $D=\{(x_i,y_i)\}_{i=1}^n$
\Ensure family of $M$ classifiers $G_m$ and weights $\a_m$
\State initialize $\forall_i:~ w_i = 1/n$
\For{$m= 1,..,M$}
\State Fit classifier $G_m$ to the training data weighted by $w_i$
\State $\text{err}_m = \frac{\sum_{i=1}^n w_i~ [y_i \not= G_m(x_i)]}{\sum_{i=1}^n w_i}$
\State $\a_m = \log[\frac{1-\text{err}_m}{\text{err}_m}]$
\State $\forall_i:~ w_i \gets w_i~ \exp\{\a_m~ [y_i \not= G_m(x_i)]\}$
\EndFor
\end{algo}

{\tiny\hfill(Hastie, sec 10.1)}

~

Weights unchanged for correctly classified points

Multiply weights with $\frac{1-\text{err}_m}{\text{err}_m}>1$ for mis-classified data points

~

\item \emph{Real AdaBoost:} A variant exists that combines
probabilistic classifiers $\s(f(x)) \in [0,1]$ instead of discrete
$G(x) \in \{-1,+1\}$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The basis function view}{

\item In AdaBoost, each model $G_m$ depends on the data weights $w_m$

We could write this as
$$f(x) = \sum_{m=1}^M \a_m f_m(x,w_m)$$

The ``features'' $f_m(x,w_m)$ now have additional parameters $w_m$

We'd like to optimize
$$\min_{\a,w_1,..,w_M} L(f)$$
w.r.t.\ $\a$ and all the feature parameters $w_m$.

~

\item In general this is hard.

But assuming $\a_{\hat m}$ and $w_{\hat m}$ fixed, optimizing for $\a_m$ and $w_m$ is efficient.

~

\item AdaBoost does exactly this, choosing $w_m$ so that
the ``feature'' $f_m$ will best reduce the loss (cf.\ PLS)

{\tiny (Literally, AdaBoost uses exponential loss or
neg-log-likelihood; Hastie sec 10.4 \& 10.5)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Gradient boosting}
\slide{Gradient Boosting}{

\item AdaBoost generates a series of basis functions by using
different data weightings $w_m$ depending on so-far classification
errors

\item We can also generate a series of basis functions
$f_m$ by fitting them to the gradient of the so-far loss

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gradient Boosting}{

\item Assume we want to miminize some loss function
$$\min_f L(f) = \sum_{i=1}^n L(y_i, f(x_i))$$
We can solve this using gradient descent
$$f^*
 = f_0
 + \a_1 \underbrace{\frac{\del L(f_0)}{\del f}}_{\approx f_1}
 + \a_2 \underbrace{\frac{\del L(f_0+\a_1 f_1)}{\del f}}_{\approx f_2}
 + \a_3 \underbrace{\frac{\del L(f_0+\a_1 f_1+\a_2 f_2)}{\del f}}_{\approx f_3}
 + \cdots$$

-- Each $f_m$ aproximates the so-far loss gradient

-- We use linear regression to choose $\a_m$ (instead of line
   search)

\item More intuitively: $\frac{\del L(f)}{\del f}$ ``points into the
   direction of the error/redisual of $f$''. It shows how
   $f$ could be improved.

   Gradient boosting uses the next lerner $f_k \approx \frac{\del
   L(f_\text{so far})}{\del f}$ to approximate how $f$ can be improved.

   Optimizing $\a$'s does the improvement.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gradient Boosting}{

\begin{algo}
\Require function class $\FF$ (e.g., of discriminative functions),
data $D=\{(x_i,y_i)\}_{i=1}^n$, an arbitrary loss function $L(y,\hat y)$
\Ensure function $\hat f$ to minimize $\sum_{i=1}^n L(y_i,f(x_i))$
\State Initialize a constant $\hat f = f_0 = \argmin_{f\in\FF} \sum_{i=1}^n L(y_i,f(x_i))$
\For{$m=1:M$}
\State For each data point $i=1:n$ compute $r_{im}=-\frac{\del L(y_i,
f(x_i))}{\del f(x_i)}\big|_{f=\hat f}$
\State Fit a \textbf{regression} $f_m\in\FF$ to the
targets $r_{im}$, minimizing squared error\label{stepReg}
\State Find optimal coefficients (e.g., via feature logistic regression)\newline
 \cen{$\a = \argmin_\a \sum_i L(y_i,\sum_{j=0}^m \a_m f_m(x_i))$}
\mbox{}\hfill(often:~ fix $\a_{0:m\1}$ and only optimize over $\a_m$)
\label{stepFit}
\State Update $\hat f = \sum_{j=0}^m \a_m f_m$
%% \State Fit a \textbf{regression} tree to the targets $r_{im}$
%%  giving terminal regions $R_{jm}$, $j=1,2,..,J_m$
%% \State For $j=1,2,..,J_m$ compute terminal constants\newline
%% \cen{$\g_{jm} = \argmin_\g \sum_{i:x_i\in R_{jm}} L(y_i, f_{m−1}(x_i) + \g)$}
%% \State Update $f_m(x) = f_{m\1}(x) + \sum_{j=1}^{J_m} \g_{jm} [x\in R_{jm}]$
\EndFor
\end{algo}

~

\small
\item If $\FF$ is the set of regression/decision trees, then
step \ref{stepFit} usually re-optimizes the terminal constants of all
leave nodes of the regression tree $f_m$. (Step \ref{stepReg} only
determines the terminal regions.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gradient boosting is the preferred method}{

\item Hastie's book quite ``likes'' gradient boosting
\begin{items}
\item Can be applied to any loss function
\item No matter if regression or classification
\item Very good performance
\item Simpler, more general, better than AdaBoost
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Classical examples for boosting}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Decision trees**}
\slide{Decision Trees**}{

%% \item So far we always referred to our ``core learners'' (Part I of
%% this lecture)

\item Decision trees are particularly used in Bagging and Boosting contexts

~

\item Decision trees are ``linear in features'', but the features are
the terminal regions of a tree, which are constructed depending on the
data

~


\item We'll learn about

-- Boosted decision trees \& stumps

-- Random Forests

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Decision Trees}{

\item We describe CART (classification and regression tree)

\item Decision trees are linear in features:
$$f(x) = \sum_{j=1}^k c_j~ [x\in R_j]$$
where $R_j$ are disjoint rectangular regions and $c_j$ the
constant prediction in a region

\item The regions are defined by a binary decision tree

\cen{
\showh[.3]{decisionTree1}\qquad
\showh[.3]{decisionTree2}\qquad
\showh[.3]{decisionTree3}
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Growing the decision tree}{

\item The constants are the region averages $c_j =
 \frac{\sum_i y_i~ [x_i\in R_j]}{\sum_i [x_i \in R_j]}$

\item Each split $x_a > t$ is defined by a choice of
input dimension $a \in \{1,..,d\}$ and a threshold $t$

\item Given a yet unsplit region $R_j$, we split it by choosing
$$\min_{a,t} \[
 \min_{c_1} \sum_{i: x_i\in R_j \wedge x_a\le t} (y_i-c_1)^2
 +
 \min_{c_2} \sum_{i: x_i\in R_j \wedge x_a> t} (y_i-c_2)^2 \]$$
\begin{items}
\item Finding the threshold $t$ is really quick (slide along)
\item We do this for every input dimension $a$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Deciding on the depth ~ (if not pre-fixed)}{

\item We first grow a very large tree (e.g.\ until at most 5 data
points live in each region)

~

\item Then we rank all nodes using ``weakest link pruning'':

Iteratively remove the node that least increases
$$\sum_{i=1}^n (y_i - f(x_i))^2$$

~

\item Use cross-validation to choose the eventual level of pruning

~

{\tiny

This is
equivalent to choosing a regularization parameter $\lambda$ for

\cen{$L(T) = \Sum_{i=1}^n (y_i - f(x_i))^2 ~+~ \lambda |T|$}
where the regularization $|T|$ is the tree size\\

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\hspace*{-10mm}
\twocol{.35}{.7}{
\small
Example:

CART on the Spam data set

{\tiny(details: Hastie, p 320)}

~

\show[1]{decisionTree_SpamExample1}

~

\show[.7]{decisionTree_SpamExample3}

Test error rate: 8.7\%

}{
\show[.95]{decisionTree_SpamExample2}
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Interpretability}{

%% \item As for the ``weakest link pruning'', we can for each split
%% evaluate the increase in $\sum_{i=1}^n (y_i - f(x_i))^2$, if we would
%% remove it. Then

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Boosting decision trees**}
\slide{Boosting trees \& stumps**}{

~

\item A \textbf{decision stump} is a decision tree with fixed depth 1
(just one split)

~

\item Gradient boosting of decision trees (of fixed depth $J$) and
stumps is very effective

~

Test error rates on Spam data set:
\center
\begin{tabular}{cc}
\hline
full decision tree & 8.7\% \\
boosted decision stumps & 4.7\% \\
boosted decision trees with $J=5$ & 4.5\%\\
\hline
\end{tabular}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Random forests**}
\slide{Random Forests:~ Bootstrapping \& randomized splits**}{

\item Recall that Bagging averages models $f_1,..,f_M$ where each
$f_m$ was trained on a bootstrap resample $D_m$ of the data $D$

This randomizes the models and avoids over-generalization

~

\item Random Forests do Bagging, but additionally randomize the
trees:
\begin{items}
\item When growing a new split, choose the input dimension $a$
only from a \emph{random subset} $m$ features

\item $m$ is often very small; even $m=1$ or $m=3$
\end{items}

~

\item Random Forests are the prime example for ``creating many randomized
weak learners from the same data $D$''

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Random Forests vs.\ gradient boosted trees}{\label{lastpage}

~

\show[.6]{decisionTree_SpamExample4}
{\tiny\hfill(Hastie, Fig 15.1)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot

