\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Introduction}
\renewcommand{\keywords}{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is Machine Learning?}{

~

%% \textbf{1)} Machine Learning is an approach to understand learning\\ ~~~ by building
%% learning systems

%% \cen{(``synthetic'' approach to a science of learning)}

%% ~\mypause

1) A long list of methods/algorithms for different data anlysis problems
\begin{items}
\item in sciences
\item in commerce
\end{items}

~

\textbf{2) Frameworks to develop your own learning algorithm/method}

~

3) Machine Learning ~ = ~ model formulation + optimization

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is Machine Learning?}{

\item Pedro Domingos: \emph{A Few Useful Things to Know about Machine
Learning}

~

%\cen{LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION}
\cen{learning = representation + evaluation + optimization}

~

\item ``Representation'': Choice of model, choice of hypothesis space

\item ``Evaluation'': Choice of objective function, optimality
principle

{\small
\item[] Notes: The \emph{prior} is both, a choice of representation
and, usually, a part of the objective function.

In Bayesian settings, the choice of model often directly implies
also the ``objective function''

}

\item ``Optimization'': The algorithm to compute/approximate the best model

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

Pedro Domingos: \emph{A Few Useful Things to Know about Machine
Learning}

\item It's generalization that counts
\begin{items}
\item Data alone is not enough
\item Overfitting has many faces
\item Intuition fails in high dimensions
\item Theoretical guarantees are not what they seem
\end{items}

\item Feature engineering is the key
\item More data beats a cleverer algorithm
\item Learn many models, not just one
\item Simplicity does not imply accuracy
\item Representable does not imply learnable
\item Correlation does not imply causation
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is Machine Learning?}{

\item In large parts, ML is: \quad (let's call this ML$^0$)

\cen{\emph{Fitting a function $f: x\mapsto y$ to given data $D=\{ (x_i,y_i) \}_{i=1}^n$}}

~\pause

\item Why is function fitting so omnipresent?
\pause
\begin{items}
\item Dynamics, behavior, decisions, control, predictions -- are all about functions
\item Thinking? ~ (i.e., planning, optimization, (logical) inference, CSP solving, etc?)
\item In the latter case, algorithms often provide the scaffolding, ML the heuristics to accelerate/scale them. (E.g., an evaluation function within a MCTS planning algorithm.)
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{ML$^0$ objective: Empirical Risk Minimization}{

\item We have a hypothesis space $\HH$ of functions $f:x\mapsto y$

In a standard parameteric case $\HH = \{ f_\t \| \t\in\RRR^n \}$ are functions $f_\t: x \mapsto y$ that are described by $n$ parameters $\t\in\RRR^n$

~

\item Given data $D=\{ (x_i,y_i) \}_{i=1}^n$, the standard objective is to minimize the ``error'' on the data
\begin{align*}
f^* \argmin _{f\in\HH} \sum_{i=1}^n \ell(f(x_i), y_i) ~,
\end{align*}
where $\ell(\hat y, y) > 0$ penalizes a discrepancy between a model output $\hat y$ and the data $y$.

\begin{items}
\item Squared error $\ell(\hat y, y) = (\hat y - y)^2$
\item Classification error $\ell(\hat y, y) = [ \hat y \not= y ]$
\item neg-log likelihood $\ell(\hat y, y) = -\log p(y \| \hat y)$
\item etc
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is Machine Learning beyond ML$^0$?}{

%% \item In large parts, ML is: \quad (let's call this ML$^0$)

%% \cen{\emph{Fitting a function $f: x\mapsto y$ to given data $D=\{ (x_i,y_i) \}_{i=1}^n$}}

%% ~

%% Beyond ML$^0$:

\item Fitting more structured models to data, which includes
\begin{items}
\item Time series, recurrent processes
\item Graphical Models
\item Unsupervised learning (semi-supervised learning)
\end{items}
{\small
...but in all these cases, the scenario is still not interactive, the data $D$ is static, the decision is about picking a single model $f$ from a hypothesis space, and the objective is a loss based on $f$ and $D$ only.

}

\item Active Learning, where the ``ML agent'' makes decisions about what data label to query next

\item Bandits, Reinforcement Learning, manipulating the domain (and thereby data source)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Machine Learning is everywhere}{

~

NSA, Amazon, Google, Zalando, Trading, ...

Chemistry, Biology, Physics, ...

Control, Operations Reserach, Scheduling, ...

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Face recognition}{

~

\cen{\twocol{.3}{.4}{\center
\show{facial-recognition}
keypoints
}{\center
\show{eigenfaces}
eigenfaces
}}

~

\hfill (e.g., Viola \& Jones)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Hand-written digit recognition (US postal data)}{

~

\show{digits}

~

\hfill (e.g., Yann LeCun)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gene annotation}{

~

\show[.9]{wormbase}

~

\hfill (e.g., Gunnar R\"atsch, mGene Project)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Speech recognition}{

~

\show{speechRecognition}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Spam filters}{

~

\show{emailSpamWords}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

Machine Learning became an important technology\\in
science as well

~

(Stuttgart Cluster of Excellence ``Data-integrated Simulation Science (SimTech)'')

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Learning from data}{

%% \small

%% \item Predict whether a patient, hospitalized due to a heart attack, will
%% have a second heart attack. The prediction is to be based on
%% demographic, diet and clinical measurements for that patient.

%% \item Predict the price of a stock in 6 months from now, on the basis of
%% company performance measures and economic data.

%% \item Identify the numbers in a handwritten ZIP code, from a digitized
%% image.

%% \item Estimate the amount of glucose in the blood of a diabetic person,
%% from the infrared absorption spectrum of that person√¢s blood.

%% \item Identify the risk factors for prostate cancer, based on clinical and
%% demographic variables.

%% {\hfill\tiny (from Hastie et al)}

%% \mypause

%% \item Predict which ads/offers a web customer will follow based on his
%% history of browsing.

%% \item Recognize the words in spoken natural language.

%% \item Classify DNA snipplets to be genes or introns.

%% \item Classify an email as spam ~ (or site access as hostile attack).

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% Examples of ML \emph{for behavior}...

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Learing motor skills}{

%% ~

%% ~

%% \twocol{.5}{.4}{\center
%%   \mov{\show{mov-balance}}{05-sethu-movies/DA_PoleLearn.avi}
%% }{\center
%%   \mov{\show{mov-juggle}}{05-sethu-movies/DB_juggle.avi}
%% }

%% ~

%% \hfill (around 2000, by Schaal, Atkeson, Vijayakumar)

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Learning to walk}{

%% ~

%% ~

%% \mov{\show[.3]{../pics-robotics/tedrake-LearningToWalk}}{10-RoboticsLecture/tedrake-LearningToWalk.avi}

%% ~

%% \hfill{\tiny (Rus Tedrake et al.)}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Learning effects of actions}{

%% ~

%% ~

%% \mov{\show[.5]{tobias-explore}}{10-relationalExploration/movie_clearance_big.avi}

%% ~

%% \hfill{(Tobias Lang \& M Toussaint)}
%% %% \mov{rel. exp. 1}{tobias1.flv}

%% %% \mov{rel. exp. 2}{tobias2.flv}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Learing for robot manipulation}{


%% ~

%% ~

%% \mov{\show[.5]{robi}}{09-robi/ICRAmovie.avi}

%% ~

%% \hfill{(Toussaint et al.)}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Types of ML}{

%% ~

%% \item \emph{Supervised} learning: ~ learn from ``labelled'' data $\{(x_i,y_i)\}_{i=1}^N$

%% \emph{Unsupervised} learning: ~ learn from ``unlabelled'' data $\{x_i\}_{i=0}^N$ only
%% %(density estimation)

%% \emph{Semi-supervised} learning: ~ many unlabelled data, few
%% labelled data

%% ~

%% \item \emph{Reinforcement} learning: ~ learn from data
%% $\{(s_t,a_t,r_t,s_{t\po})\}$
%% \begin{items}
%% \item learn a predictive model $(s,a) \mapsto s'$

%% \item learn to predict reward $(s,a) \mapsto r$

%% \item learn a behavior $s \mapsto a$ that maximizes reward
%% \end{items}

%% %% ~\mypause

%% %% \item generative vs.\ discriminative modelling

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Organization of this lecture}{

~

\cen{See TOC of last year's slide collection}

~

\item Part 1: The Core: Regression \& Classification

\item Part 2: The Breadth of ML methods

\item Part 3: Bayesian Methods

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Is this a theoretical or practical course?}{

~

Neither alone.

~\mypause

\item The goal is to teach how to design good learning algorithms

\begin{center}\tiny
data\\
$\downarrow$\\
modelling [requires theory \& practise]\\
$\downarrow$\\
algorithms [requires practise \& theory]\\
$\downarrow$\\
testing, problem identification, restart
\end{center}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{How much math do you need?}{

~

\item Let $L(x)= \norm{y - A x}^2$. What is
$$\argmin_x L(x)$$

~

\item Find
$$\min_x \norm{y - A x}^2 \st x_i\le 1$$

~

\item Given a discriminative function $f(x,y)$ we define
$$p(y\|x) = \frac{e^{f(y,x)}}{\sum_{y'} e^{f(y',x)}}$$

~

\item Let $A$ be the covariance matrix of a Gaussian. What does the
Singular Value Decomposition $A = V D V^\T$ tell us?


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{How much coding do you need?}{

\item A core subject of this lecture: learning to go from principles (math) to
code

~

\item Many exercises will implement algorithms we derived in the
lecture and collect experience on small data sets

~

\item Choice of language is fully free. I support C++;
tutors might prefer Python; Octave/Matlab or R is also good choice.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Basic regression \& classification}{

%% ~

%% \showh[.4]{codepics/cubicReg}
%% \quad
%% \showh[.4]{codepics/kernelRidgeClass}

%% ~

%% \item Regression: ~ map input $x$ to continuous value $y\in\RRR$

%% Classification: ~ map input $x$ to one of $M$ classes $y\in\{1,2,..,M\}$

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Basic regression \& classification}{

%% \item A must-know!

%% \item High practical relevance for applications
%% % (including cog.\ \& neuro sciences)

%% \item Focus on linear
%%    methods on non-linear features, regularization, cross-validation

%% \cen{``linear$|$polynomial$|$Kernel ~ Ridge$|$Lasso ~
%% Regression$|$Classification''}

%% \item Relations to SVM, GPs, feature selection

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Bayesian Modelling}{

%% ~

%% \item Mr. Holmes lives in Los Angeles. One morning when Holmes leaves
%% his house, he realizes that his grass is wet. Is it due to rain, or
%% has he forgotten to turn off his sprinkler?

%% %% \item Calculate $P(R|H)$, $P(S|H)$ and compare these values to the
%% %%  prior probabilities

%% %% \item Calculate $P(R, S|H)$. $R$ and $S$ are marginally independent,
%% %%  but conditionally dependent

%% \item Holmes checks Watson‚Äôs grass, and finds it is also
%% wet. What does that imply on rain vs.\ sprinkler?

%% %Calculate $P(R|H,W)$, $P(S|H,W)$

%% ~

%% \begin{center}
%% \input{figs/vl2-rain}
%% \end{center}
%% $
%% \iff ~ P(H,W,S,R) = P(H|S,R)~ P(W|R)~ P(S)~ P(R)
%% $


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Bayesian Modelling}{

%% the ASIA network: a model for lung disease
%% \begin{center}
%% \input{figs/vl2-asia}
%% \end{center}
%% $\iff
%% P(D,X,E,B,L,T,S,A) =
%% P(D|E,B)~ P(X|E)~ P(E|T,L)~ P(B|S)~ P(L|S)~ P(T|A)~ P(S)~ P(A)
%% $

%% ~

%% ~

%% \twocol{.5}{.5}{
%% $A$=trip to asia\\
%% $S$=smoking\\
%% $T$=Tuberculosis\\
%% $L$=lung cancer
%% }{
%% $E$=abnormality in chest\\
%% $X$=X-ray\\
%% $D$=Dyspnea\\
%% $B$=Bronchitis
%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Bayesian modelling}{

%% \item Fundamental view on information processing and learning

%% ~

%% \item Provides general tools for formulating structured probabilistic
%%    models

%% ~~ (e.g., latent variables, mixtures, hierarchical,
%%    deep models)

%% $\to$ a framework for formulating novel learning algorithms

%% ~

%% \item Bayesian view on linear models + regularization

%% -- regularization $\oto$ prior, ``error'' $\oto$ likelihood

%% %% ~

%% %% \item I'll introduce basics: Bayes, likelihood maximization, MAP,
%% %%    EM, Bayes Nets \& probabilistic inference

%% %% <li> Bayesian modelling and reasoning
%% %% <li> Regression & Gaussian Processes
%% %% <li> Bayesian Networks
%% %% <li> Probabilistic Inference
%% %% <li> Sampling & Approximate Inference
%% %% <li> Decision Theory & Reinforcement Learning 

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Reinforcement Learning (another course..)}{

%% \moviex{[PacMan]}{movs/Reinforcement_learning_agent_plays_Ms._Pac-Man.mp4}

%% ~

%% ~

%% I Szita, A Lorincz:
%% \emph{Learning to Play Using Low-Complexity Rule-Based Policies:
%% Illustrations through Ms. Pac-Man.} JAIR 2007.

%% }

%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Reinforcement Learning (another course..)}{

%% \item Behavior!, learning to act

%% \item Basic RL methods (Temporal Difference, Q-learning, traces)

%% \item Regression in RL, Bayesian methods in RL

%% \item Applications

%% %% -- Current research often asks: Do humans/animals take decisions in a
%% %%    Bayesian way? What does that mean anyway? Is this Q meaningful, or
%% %%    is it like asking ``do humans/animals use information processing''?

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Books}{

~

\twocol{.4}{.5}{
\show{HastieEtAl}
}{

Trevor Hastie, Robert Tibshirani and Jerome Friedman:
\emph{The Elements of Statistical Learning: Data Mining, Inference,
and Prediction} Springer, Second Edition, 2009.

\url{http://www-stat.stanford.edu/~tibs/ElemStatLearn/}

(recommended: read introductory chapter)

}

~

~

\hfill \tiny(this course will not go to the full depth in math of Hastie et al.)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Books}{

~

\twocol{.4}{.5}{
\show{Bishop}
}{

Bishop, C. M.: \emph{Pattern Recognition and Machine Learning}.

Springer, 2006


\url{http://research.microsoft.com/en-us/um/people/cmbishop/prml/}

(some chapters are fully online) 

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Books \& Readings}{

~

\item more recently:
\begin{items}
\item David Barber: Bayesian Reasoning and Machine Learning
\item Kevin Murphy: Machine learning: a Probabilistic Perspective
\end{items}

~

\item See the readings at the bottom of:

\cen{\tiny\url{http://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/index.html\#readings}}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Books}{

%% ~

%% \twocol{.4}{.5}{
%% \show{SuttonBarto}
%% }{


%% Richard S. Sutton and Andrew G. Barto: \emph{Reinforcement Learning:
%% An Introduction}. MIT Press, 1998.


%% \url{http://www.cse.iitm.ac.in/~cs670/book/the-book.html}

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Organization}{\label{lastpage}

\item Course Webpage:

\cen{\tiny\url{http://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/19-MachineLearning/}}
\begin{items}
\item Slides, Exercises \& Software (C++)
\item Links to books and other resources
\end{items}

\item Admin things, please first ask:

  Carola Stahl, {\tiny\tt
  Carola.Stahl\@ipvs.uni-stuttgart.de}, Raum 2.217

~

\item Rules for the tutorials:

\begin{items}
\item Doing the exercises is crucial!

\item \textbf{Nur Votieraufgaben.} At the beginning of each tutorial:

-- sign into a list

-- vote on exercises you have (successfully) worked on

\item Students are randomly selected to present their solutions

\item {\color{red}You need 50\% of completed exercises to be allowed
to the exam}

\item Please check 2 weeks before the end of the term, if you can take
the exam
\end{items}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Organisation}{

%% %\item Bitte in beidem, KVV \& Campus Management anmeldem.


%% ~

%% \item Klausur:

%% -- Dienstag 10.7.2012, 14-16h

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
