\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Recap\vskip2cm Philipp Kratzer \& Janik Hager\vskip-2cm $ $} %Sorry for this hack ;-)
\renewcommand{\keywords}{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What is Machine Learning?}{

\item Pedro Domingos: \emph{A Few Useful Things to Know about Machine
Learning}

~

\cen{learning = representation + evaluation + optimization}

~

\item ``Representation'': Choice of model, choice of hypothesis space

\item ``Evaluation'': Choice of objective function, optimality
principle

\item ``Optimization'': The algorithm to compute/approximate the best model

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Regression: Ridge Regression}{
	
	\item \textbf{Representation:} ~ choice of features
	$$ f(x) = \phi(x)^\T \b $$
	
	~
	
	\item \textbf{Objective:} ~ squared error ~ + ~ Ridge/Lasso regularization
	$$L^{\text{ridge}}(\b) = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 + \l \norm{\b}^2_I$$
	
	~
	
	\item \textbf{Solver:} ~ analytical (or quadratic program for Lasso)
	$$\hat \b^\text{ridge} = (X^\T X + \l I)^\1 X^\T y$$
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Classification: Logistic Regression}{
	
	\item \textbf{Representation:} ~ choice of features
	$$ f(x) = \phi(x, y)^\T \b $$
	
	~
	
	\item \textbf{Objective:} ~ neg-log-likelihood
	$$L^\text{logistic}(\b) = - \sum_{i=1}^n \log p(y_i \| x_i) +  \l\norm{\b}^2$$
	
	$$p(y|x) \propto e^{f(x,y)}$$
	
	~
	
	\item \textbf{Solver:} ~ numerical (Newton algorithm)
	$$\b \gets \b - (X^\T W X + 2 \l I)^\1~ (X^\T (p - y) + 2\l I \b)$$
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Neural Networks}{
	
	\item \textbf{Representation:} ~ multi-layer, sequential mapping
	$$ f(x) = W_2 \s (W_1 \s (W_0 x + b_0) + b_1) + b_2 $$
	
	~
	
	\item \textbf{Objective:} ~ e.g. a squared loss for regression
	$$L(f) = \sum_{i=1}^n (y_i - f(x_i))^2$$
	
	~
	
	\item \textbf{Solver:} ~ Propagating the error backwards, while compute the gradients
	$\frac{d L(f)}{d W_l}$ for each layer. Weight update can be done using e.g. stochastic
	gradient descent ($\b = (W_{1:L}, b_{1:L})$)
	$$\b \gets \b - \eta \nabla_\b L(\b,\hat D)$$
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Neural Networks}{
	
	\item \textbf{activation functions:} ~ ReLU, leaky ReLU, sigmoid, ...
	
	\item \textbf{regularization:} ~ dropout, data augmentation, early stopping, ...
	
	\item \textbf{special NNs:} ~ Convolutional NNs (images), LSTM (time series), ...
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernelization}{
	
	\item \textbf{Representation:} ~ Kernel Ridge Regression
	\begin{align*}
		f^\text{rigde}(x)
		&= \k(x)^\T (K + \l I)^\1 y \\
		\text{with} \,\,\, K_{ij}
		& = k(x_i,x_j)\\
		\k_i(x)
		&= k(x,x_i)
	\end{align*}
	
	\item \textbf{Kernel:} ~ Every choice of features implies a kernel and the other way round.
	$$ k(x_i,x_j) = \phi(x_i)^\T \phi(x_j) $$
	
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Unsupervised Learning: PCA}{
	
	$V_p^\T$ is the matrix that projects to the largest variance directions of $\tilde{X}^\T \tilde{X}$
	
	\item \textbf{Representation:}
	$$ x \approx V_p z + \m $$
	
	\item \textbf{Objective:}
	$$\sum_{i=1}^n \norm{x_i - (V_p z_i + \m)}^2$$
	
	\item \textbf{Solver:} ~ Eigenvector decomposition of $\tilde{X}^\T \tilde{X}$
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Unsupervised Learning: Clustering}{
	
	\textbf{$k$-means:}
	
	\item \textbf{Representation:} $K$ centers $\mu_k$ and a data assignment $c: i \mapsto k$
	
	\item \textbf{Objective:}
	$$\min_{c,\mu} \sum_i (x_i - \mu_{c(i)})^2$$
	
	\item \textbf{Solver:} ~
	\begin{items}
		\item Pick $K$ data points randomly to initialize the centers $\mu_k$
		\item Iterate adapting the assignments $c(i)$ and the centers $\mu_k$
	\end{items}

	~

	\textbf{Gaussian Mixture Models:}
	
	Approximate the "true" distribution, from which the data $\{x_i\}^N_{i=1}$ is generated,
	using a mixture of multivariate Gaussians (solved via EM-Algorithm).
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Local Learning \& Combining Models}{
	
	\item \textbf{Local Learning:} ~ Build local model using $k$NN of query $x^*$
	
	\item \textbf{Model Averaging:} ~ Fully different types of models (using
	different (e.g.\ limited) feature sets; neural nets; decision trees;
	hyperparameters)
	
	\item \textbf{Bootstrap:} ~ Models of same type, trained on randomized versions of $D$
	
	\item \textbf{Boosting:} ~ Models of same type, trained on cleverly designed
	modifications/reweightings of $D$
	
	\item \textbf{How to choose weights for combining models:}
	
	naive averaging, Bayesian Model Averaging, Function view, ...
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Bayesian Models}{
	
	Placing distributions on parameters, model classes, ...
	
	\item \textbf{Representation:} ~ e.g. Kernel Bayesian Logistic Regression
	
	$$P(X),~~ P(\b),~~ P(Y|X, \b)$$
	
	\item \textbf{Objective \& Solver:} ~ compute inference
	$$P(\b \| x_{1:n}, y_{1:n})	= \frac{\prod_{i=1}^n P(y_i \| \b , x_i)~ P(\b)}{Z}$$
	
	\item \textbf{Insights:}
	\begin{items}
		\item The \emph{neg-log posterior $P(\b\|D)$} is proportional
		to the cost function $L^{\text{ridge}}(\b)$.
		\item The mean $\hat\b$ is exactly the classical
		$\argmin_\b L^{\text{ridge}}(\b)$.
		\item The Bayesian inference approach not only
		gives a mean/optimal $\hat\b$, but also a variance $\S$ of that estimate.
	\end{items}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Summary}{
	
	\item Machine Learning is a large field with many real-world applications
	
	\item Includes many components from computer science and statistics
	
	\item Further points covered in the lecture: tree-based models, conditional random fields, ...
	
	~\mypause
	
	\textbf{All models are wrong, but some are useful.}\\
	George Box, 1919 - 2013
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
