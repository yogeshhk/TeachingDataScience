\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Unsupervised Learning}
\renewcommand{\keywords}{PCA, kernel PCA Spectral Clustering, Multidimensional Scaling, ISOMAP Non-negative Matrix Factorization*, Factor Analysis*, ICA*, PLS*, Clustering, k-means, Gaussian Mixture model Agglomerative
 Hierarchical Clustering}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Unsupervised learning}{

\item What does that mean? \qquad Generally: modelling $P(x)$

\item Instances:
\begin{items}
\item Finding lower-dimensional spaces
\item Clustering
\item Density estimation
\item Fitting a graphical model
\end{items}

\item ``Supervised Learning as special case''...

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{PCA and Embeddings}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Principle Component Analysis (PCA)}
\slide{Principle Component Analysis (PCA)}{

%% {\tiny Note: we introduce PCA here in the (equivalent) probabilistic
%%   formulation---for a discussion see Bishop sec.\ 12.2.}

\item Assume we have data $D=\{ x_i \}_{i=1}^n$, $x_i \in \RRR^d$.

~

Intuitively: ``We believe that there is an \textbf{underlying
  lower-dimensional space} explaining this data''.

~

\item How can we formalize this?

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PCA: minimizing projection error}{

\item For each $x_i\in\RRR^d$ we postulate a lower-dimensional latent variable $z_i \in \RRR^p$

$$x_i \approx V_p z_i + \m$$

~
\item Optimality:

\cen{Find $V_p,\m$ and values $z_i$ that minimize $\sum_{i=1}^n \norm{x_i - (V_p z_i + \m)}^2$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimal $V_p$}{

$$\hat \m,\hat z_{1:n}
=\argmin_{\m,z_{1:n}} \sum_{i=1}^n \norm{x_i - V_p z_i - \m}^2$$

$\To
 \hat\m = \< x_i \>  = \frac{1}{n} \sum_{i=1}^n x_i
\comma
 \hat z_i = V_p^\T(x_i-\m)
$

~\mypause

\item Center the data $\tilde x_i = x_i-\hat\m$. Then
$$\hat V_p =\argmin_{V_p} \sum_{i=1}^n \norm{\tilde x_i - V_p V_p^\T \tilde x_i}^2$$

~\mypause

\item Solution via Singular Value Decomposition

-- Let $X \in\RRR^{n\times d}$ be the centered data matrix containing all
$\tilde x_i$

-- We compute a sorted Singular Value Decomposition
$X^\T X = V D V^\T$ 

~~ $D$ is diagonal with sorted singular values
$\l_1 \ge \l_2 \ge \cdots \ge \l_d$

~~ $V = (v_1 ~ v_2 ~ \cdots ~ v_d)$ contains largest eigenvectors $v_i$ as columns

\eqbox{$V_p := V_{1:d,1:p} = (v_1 ~ v_2 ~ \cdots ~ v_p)$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Principle Component Analysis (PCA)}{

\show[.4]{pca1}

$V_p^\T$ is the matrix that projects to the largest
  variance directions of $X^\T X$
$$z_i = V_p^\T (x_i-\mu) \comma Z = X V_p$$

\item In non-centered case: Compute SVD of variance
$$A = \Var{x} = \<x x^\T\> - \m \m^\T = \frac{1}{n} 
   X^\T X - \m \m^\T$$

%% ~

%% \item Generally: \textbf{Apply ML method on top of $Z$ instead of
%% $X$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example:~ Digits}{

\show{pcaThrees1}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example:~ Digits}{

\item The ``basis vectors'' in $V_p$ are also \textbf{eigenvectors}

Every data point can be expressed in these eigenvectors
\begin{align*}
x
 &\approx \mu + V_p z \\
 &= \mu + z_1 v_1 + z_2 v_2 + \dots \\
 &= 
\raisebox{-1.5ex}{\text{\showhs[.3]{pcaThrees2}}}
+ z_1 \cdot \raisebox{-1.5ex}{\text{\showhs[.3]{pcaThrees3}}}
+ z_2 \cdot \raisebox{-1.5ex}{\text{\showhs[.3]{pcaThrees4}}}
+ \cdots
\end{align*}

~

\show{pcaThrees5}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example:~ Eigenfaces}{

~

\show{eigenfaces}

~

\hfill (Viola \& Jones)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Autoencoders}
\slide{Non-linear Autoencoders}{

\item PCA given the ``optimal linear autoencode''

\item We can relax the encoding ($V_p$) and decoding ($V_p^\T$) to be non-linear mappings, e.g., represented as a neural network

\show[.3]{autoencoder}

A NN which is trained to reproduce the input: $\min_i \norm{y(x_i) -
x_i}^2$

The hidden layer (``bottleneck'') needs to find a good
representation/compression.

~

\item Stacking autoencoders:

\show[.5]{stacked_autoencoder}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Augmenting NN training with semi-supervised embedding objectives}{

\item Weston et al.\ (ICML, 2008)

~

\show[.3]{deepLearningWeston1}

\small
Mnist1h dataset, deep NNs of 2, 6, 8, 10
and 15 layers; each hidden layer 50 hidden units

\show[.7]{deepLearningWeston2}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What are good representations?}{

~

\begin{items}
\item Reproducing/autoencoding data, maintaining maximal information
\item Disentangling correlations (e.g., ICA)
\item those that are most correlated with desired \emph{outputs} (PLS, NNs)
\item those that maintain the clustering
\item those that maintain relative distances (MDS)
\end{items}

...

\begin{items}
\item those that enable efficient reasoning, decision making \& learning in the real world
\item How do we represent our 3D environment, enabeling physical \& geometric reasoning?
\item How do we represent things to enable us inventing novel things, machines, technology, science?
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Independent component analysis**}
\slide{Independent Component Analysis**}{

\small

\item Assume we have data $D=\{ x_i \}_{i=1}^n$, $x_i \in \RRR^d$.

PCA: $P(x_i \| z_i) = \NN(x_i \| W z_i + \m, \Id)\comma P(z_i) =
\NN(z_i \| 0,\Id)$

Factor Analysis: $P(x_i \| z_i) = \NN(x_i \| W z_i + \m, \S)\comma P(z_i) =
\NN(z_i \| 0,\Id)$

ICA: $P(x_i \| z_i) = \NN(x_i \| W z_i + \m, \e\Id)\comma P(z_i) =
\prod_{j=1}^d P(z_{ij})$

~

\show[.25]{ica}

\item In ICA

1) We have (usually) as many latent variables as observed $\dim(x_i) =
\dim(z_i)$

2) We require all latent variables to be \textbf{independent}

3) We allow for latent variables to be \textbf{non-Gaussian}

~

Note: without point (3) ICA would be without sense!

%% ~

%% \item E.g., use gradient methods to find $W$ that maximizes
%%   likelihood.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Partial least squares (PLS)**}
\slide{Partial least squares (PLS)**}{

\item Is it really a good idea to just pick the $p$-higest variance
components??

~

Why should that be a good idea?

~

\show[.35]{PLS1}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PLS*}{

\item Idea: The first dimension to pick should be the one \textbf{most
correlated with the OUTPUT}, not with itself!

\begin{algo}
\Require data $X\in\RRR^{n\times d}$, $y\in\RRR^n$
\Ensure predictions $\hat y\in\RRR^n$
\State initialize the \emph{predicted output}:~ $\hat y = \<y\> 1_n$
\State initialize the \emph{remaining input dimensions}:~ $\hat X = X$
\For{$i=1,..,p$}
\State $i$-th input `basis vector':~ $z_i = \hat X \hat X^\T y$
\State update prediction:~ $\hat y \gets \hat y
+ Z_i y$ \quad where $Z_i=\frac{z_i z_i^\T }{z_i^\T z_i}$
\State remove ``used'' input dimensions:~ $\hat X \gets \hat X (\Id - Z_i)$
\EndFor
\end{algo}

\hfill(Hastie, page 81)

\tiny

Line 4 identifies a new input ``coordinate'' via maximal correlation
between the remaning input dimensions and $y$.

Line 5 updates the prediction to include the project of $y$ onto $z_i$

Line 6 removes the projection of input data $\hat X$ along $z_i$. All
$z_i$ will be orthogonal.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PLS for classification*}{

~

\item Not obvious.

~

\item We'll try to invent one in the exercises :-)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item back to linear autoencoding, i.e., PCA - but now linear in RKHS

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Kernel PCA**}
\slide{``Feature PCA'' \& Kernel PCA**}{

\item The \emph{feature} trick: $X
 = \mat{c}{\phi(x_1)^\T \\ \vdots \\ \phi(x_n)^\T}\in\RRR^{n\times k}$

~

\item The \emph{kernel} trick: rewrite all necessary equations
such that they only involve scalar products $\phi(x)^\T \phi(x') =
k(x,x')$:

~

{\tiny We want to compute eigenvectors of $X^\T X
= \sum_i \phi(x_i) \phi(x_i)^\T$. We can rewrite this as
\vspace*{-3mm}
\begin{align*}
X^\T X v_j &= \l v_j \\
\underbrace{X X^\T}_{K} \underbrace{X v_j}_{K\a_j}
 &= \l \underbrace{X v_j}_{K\a_j} \comma v_j = \sum_i \a_{ji} \phi(x_i) \\
K\a_j
 &= \l \a_j
\end{align*}
Where $K = X X^\T$ with entries $K_{ij}
= \phi(x_i)^\T \phi(x_j)$.

$\to$ We compute SVD of the kernel matrix $K$ $\to$ gives
eigenvectors $\a_j \in\RRR^n$.

Projection: \quad $x \mapsto z = V_p^\T \phi(x)
= \sum_i \a_{1:p,i} \phi(x_i)^\T \phi(x) = A \k(x)$

\hfill (with matrix $A\in\RRR^{p\times n}$, $A_{ji} = \a_{ji}$ and vector
$\k(x)\in\RRR^n$, $\k_i(x) = k(x_i,x)$)

Since we cannot \emph{center the features $\phi(x)$} we actually need
``the double centered kernel matrix'' $\widetilde{K} = (\Id
- \frac{1}{n}{\vec 1}{\vec 1}^\T) K (\Id - \frac{1}{n}{\vec
1}{\vec 1}^\T)$, where $K_{ij}
= \phi(x_i)^\T \phi(x_j)$ is uncentered.

}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernel PCA}{

red points: data

green shading: eigenvector $\vec \a_j$ represented as functions $\sum_i \a_{ji} k(x_j,x)$

~

\show{bishopKernelPCA}

~

Kernel PCA ``coordinates'' allow us to discriminate clusters!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernel PCA}{

\item Kernel PCA uncovers quite surprising structure:

~

While PCA ``merely'' picks high-variance dimensions

Kernel PCA picks high variance \emph{features}---where features
correspond to basis functions (RKHS elements) over $x$

~

\item Kernel PCA may map data $x_i$ to latent coordinates $z_i$
where \emph{clustering} is much easier

~

\item All of the following can be represented as kernel PCA:

\small
-- Local Linear Embedding

-- Metric Multidimensional Scaling

-- Laplacian Eigenmaps (Spectral Clustering)

{\tiny\hfill see ``Dimensionality Reduction: A Short Tutorial'' by
Ali Ghodsi}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernel PCA clustering}{

~

\item Using a kernel function $k(x,x') = e^{-\norm{x-x'}^2/c}$:

~

\cen{\showh[.3]{kernelPCAcluster1} \quad
\showh[.6]{kernelPCAcluster2}}

~

\item Gaussian mixtures or $k$-means will easily cluster this

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Spectral clustering**}
\slide{Spectral Clustering**}{

Spectral Clustering is very similar to kernel PCA:

\item Instead of the kernel matrix $K$ with entries $k_{ij} = k(x_i,x_j)$
we construct a weighted \emph{adjacency matrix}, e.g.,
\begin{align*}
w_{ij} = \left\{\arr{cc}{
0 & \text{ if $x_i$ are not a $k$NN of $x_j$ }\\
e^{-\norm{x_i-x_j}^2/c} & \text{ otherwise }
}\right.
\end{align*}

$w_{ij}$ is the weight of the \emph{edge} between data point $x_i$ and
$x_j$.

~

\item Instead of computing \emph{maximal} eigenvectors of
  $\widetilde{K}$, compute \emph{minimal} eigenvectors of

\cen{$L = \Id - \widetilde W \comma
\widetilde W = \diag(\sum_j w_{ij})^\1 W$}
($\sum_j w_{ij}$ is called \emph{degree of node $i$}, $\widetilde W$
is the normalized weighted adjacency matrix)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Given $L = U D V^\T$, we pick the $p$ smallest eigenvectors
  $V_p = V_{1:n,1:p}$ (perhaps exclude the trivial smallest eigenvector)

~

\item The latent coordinates for $x_i$ are $z_i
 %= V_p^\T  \d_{\cdot i}
 = V_{i,1:p}$

~

\item Spectral Clustering provides a method to compute latent
  low-dimensional coordinates $z_i = V_{i,1:p}$ for each 
 high-dimensional $x_i \in \RRR^d$ input.

~

\item This is then followed by a standard clustering, e.g., Gaussian
  Mixture or k-means

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\show[.7]{spectralClustering-hastie}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Spectral Clustering is similar to kernel PCA:

-- The kernel matrix $K$ usually represents similarity

~~ The weighted adjacency matrix $W$ represents proximity \&
   similarity

-- High Eigenvectors of $K$ are similar to low EV of $L=\Id- W$

~

\item Original interpretation of Spectral Clustering:

-- $L=\Id- W$ (weighted graph Laplacian) describes a diffusion process:

~~ The diffusion rate $W_{ij}$ is high if $i$ and $j$ are close and similar

-- Eigenvectors of $L$ correspond to stationary solutions

~\tiny

\item The Graph Laplacian $L$: For some vector $f\in\RRR^n$, note the following identities:
\begin{align*}
(L f)_i
&= (\sum_j w_{ij}) f_i - \sum_j w_{ij} f_j = \sum_j w_{ij} (f_i - f_j) \\
f^\T L f
&= \sum_i f_i \sum_j w_{ij} (f_i - f_j)
 = \sum_{ij} w_{ij} (f_i^2 - f_i f_j) \\
&= \sum_{ij} w_{ij} (\half f_i^2 + \half f_j^2 - f_i f_j)
 = \half \sum_{ij} w_{ij} (f_i - f_j)^2
\end{align*}
where the second-to-last = holds if $w_{ij}=w_{ji}$ is symmetric.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Comparison}{

%% ~

%% \begin{tabular}{|cc@{\quad$\to$\quad}c|}
%% \hline
%% PCA & $x_i^\T x_j$ large & $z_i^\T z_j$ large \\
%% \hline
%% feature PCA & $\phi(x_i)^\T \phi(x_j)$ large & $z_i^\T z_j$ large \\
%% \hline
%% kernel PCA & $k(x_i,x_j)$ large & $z_i^\T z_j$ large \\
%% \hline
%% spectral clustering & $w_{ij}$ large, $L_{ij}$ small & $z_i \approx
%% z_j$ \\
%% \hline
%% classical scaling & $x_i$ and $x_j$ similar & $z_i^\T z_j$ large \\
%% \hline
%% \end{tabular}

%% ~

%% ~\tiny

%% Note: \quad $z_i^\T z_j$ large $\quad\oto\quad$ $z_i \approx
%% z_j$ since \\
%% {$\norm{z_i-z_j}^2
%%  = \norm{z_i-\bar x}^2
%%  + \norm{z_j-\bar x}^2
%%  - 2 (z_i-\bar x)^\T (z_j-\bar x)
%% $}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Multidimensional scaling**}
\slide{Metric Multidimensional Scaling**}{

\item Assume we have data $D=\{ x_i \}_{i=1}^n$, $x_i \in \RRR^d$.

As before we want to indentify latent lower-dimensional
representations $z_i\in\RRR^p$ for this data.

~

\item A simple idea: Minimize the stress

\medskip
\eqbox{$S_C(z_{1:n}) = \sum_{i\not= j}( d_{ij}^2 - \norm{z_i-z_j}^2 )^2$}
\medskip

We want distances in high-dimensional space to be equal to distances
in low-dimensional space.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Metric Multidimensional Scaling = (kernel) PCA}{

\item Note the relation:
$$d_{ij}^2 = \norm{x_i-x_j}^2
 = \norm{x_i-\bar x}^2
 + \norm{x_j-\bar x}^2
 - 2 (x_i-\bar x)^\T (x_j-\bar x)
$$

\emph{This translates a distance into a (centered) scalar product}

~\mypause

\item If may we define

%% $$\sum_{i\not= j}( \norm{x_i-x_j}^2 - \norm{z_i-z_j}^2 )^2
%% = \sum_{i\not= j}( x_i^\T x_j - z_i^\T z_j)^2$$

\cen{$\widetilde{K} = (\Id - \frac{1}{n}{\vec 1}{\vec
1}^\T) D (\Id - \frac{1}{n}{\vec 1}{\vec 1}^\T) \comma D_{ij} =
-d_{ij}^2/2$}

then $\widetilde{K_{ij}} = (x_i-\bar x)^\T (x_j-\bar x)$ is the
normal covariance matrix and MDS is equivalent to kernel PCA

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Non-metric Multidimensional Scaling}{

\item We can do this for any data (also non-vectorial or not
  $\in\RRR^d$) as long as we have a data set of comparative
  dissimilarities $d_{ij}$ 
$$S(z_{1:n}) = \sum_{i\not= j}( d_{ij}^2 - |z_i-z_j|^2 )^2$$


\item Minimize $S(z_{1:n})$ w.r.t.\ $z_{1:n}$ \emph{without any
  further constraints}!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{ISOMAP**}
\slide{Example for Non-Metric MDS: ISOMAP**}{

\item  Construct $k$NN graph and label edges with Euclidean distance

-- Between any two $x_i$ and $x_j$, compute ``geodescic'' distance $d_{ij}$

~~ (shortest path along the graph)

-- Then apply MDS

~

\show[.5]{swissRoll}

\tiny\hfill by Tenenbaum et al.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{The zoo of dimensionality reduction methods}{

\item PCA family:

-- kernel PCA, non-neg.\ Matrix Factorization, Factor Analysis

~

\item All of the following can be represented as kernel PCA:

-- Local Linear Embedding

-- Metric Multidimensional Scaling

-- Laplacian Eigenmaps (Spectral Clustering)

~

\cen{They all use different notions of distance/correlation as input
to kernel PCA}

~

{\tiny\hfill see ``Dimensionality Reduction: A Short Tutorial'' by
Ali Ghodsi}

%% ~

%% \item Non-metric Multidimensional Scaling is special and often used.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PCA variants*}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Non-negative matrix factorization**}
\slide{PCA variant: Non-negative Matrix Factorization**}{

\item Assume we have data $D=\{ x_i \}_{i=1}^n$, $x_i \in \RRR^d$.

As for PCA (where we had $x_i \approx V_p z_i + \m$) we search for a
lower-dimensional space with linear relation to $x_i$

~

\item In NMF we require everything is \textbf{non-negative}: the data
  $x_i$, the projection $W$, and latent variables $z_i$

Find $W\in\RRR^{p\times d}$ (the tansposed projection) and $
Z\in\RRR^{n\times p}$ (the latent variables $z_i$) such that
$$X \approx Z W$$

\item Iterative solution: ~ (E-step and M-step like...)
\begin{align*}
z_{ik} &\gets z_{ik}~
\frac{
\sum_{j=1}^d w_{kj} x_{ij}/(Z W)_{ij}
}{
\sum_{j=1}^d w_{kj}
} \\
w_{kj} &\gets w_{kj}~
\frac{
\sum_{i=1}^N z_{ik} x_{ij}/(Z W)_{ij}
}{
\sum_{i=1}^N z_{ik}
}
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{PCA variant: Non-negative Matrix Factorization*}{

\show[.6]{nonNegMatrixFac1}

\show[.6]{nonNegMatrixFac2}

\tiny (from Hastie 14.6)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Factor analysis**}
\slide{PCA variant: Factor Analysis**}{

Another variant of PCA: ~ (Bishop 12.64)

Allows for different noise in each dimension
$P(x_i \| z_i) = \NN(x_i \| V_p z_i + \m, \S)$ (with $\S$ diagonal)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Clustering}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Clustering}{

\item Clustering often involves two steps:

\item First map the data to some embedding that emphasizes clusters
\begin{items}
\item (Feature) PCA
\item Spectral Clustering
\item Kernel PCA
\item ISOMAP
\end{items}

\item Then explicitly analyze clusters
\begin{items}
\item $k$-means clustering
\item Gaussian Mixture Model
\item Agglomerative Clustering
\end{items}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{k-means clustering}
\slide{$k$-means Clustering}{

\item Given data $D=\{ x_i \}_{i=1}^n$, find $K$ centers $\mu_k$, and a
data assignment $c: i \mapsto k$ to minimize
$$\min_{c,\mu} \sum_i (x_i - \mu_{c(i)})^2$$

~

\item $k$-means clustering:
\begin{items}
\item Pick $K$ data points randomly to initialize the centers $\mu_k$
\item Iterate adapting the assignments $c(i)$ and the centers $\mu_k$:
\begin{align*}
\forall_i:~ c(i) &\gets \argmin_{c(i)} \sum_j (x_j - \mu_{c(j)})^2 = \argmin_k (x_i-\mu_k)^2 \\
\forall_k:~ \mu_k &\gets \argmin_{\mu_k} \sum_i (x_i-\mu_{c(i)})^2 =  \frac{1}{|c^\1(k)|} \sum_{i\in c^\1(k)} x_i
\end{align*}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\slide{$k$-means Clustering}{

\show[.6]{kMeansClustering}
{\tiny\hfill from Hastie}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{$k$-means Clustering}{

\item Converges to local minimum $\to$ \textbf{many restarts}

\item Choosing $k$? Plot $L(k) = \min_{c,\mu} \sum_i (x_i - \mu_{c(i)})^2$
for different $k$ -- choose a tradeoff between model complexity (large
$k$) and data fit (small loss $L(k)$)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{$k$-means Clustering for Classification}{

\show[.6]{kMeansClassification}
{\tiny\hfill from Hastie}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Gaussian mixture model}
\slide{Gaussian Mixture Model for Clustering}{

\item GMMs can/should be introduced as \emph{generative} probabilistic
model of the data:
\begin{items}
\item $K$ different Gaussians with parmameters $\m_k,\S_k$
\item Assignment RANDOM VARIABLE $c_i\in\{1,..,K\}$ with $P(c_i\=k) = \pi_k$
\item The observed data point $x_i$ with $P(x_i \| c_i\=k ;\m_k,\S_k)
= \NN(x_i \| \m_k,\S_k)$
\end{items}

\item EM-Algorithm described as a kind of
soft-assignment version of $k$-means
\begin{items}
\item Initialize the centers $\m_{1:K}$ randomly from the data; all
covariances $\S_{1:K}$ to unit and all $\pi_k$ uniformly.

\item \textbf{E-step:} (probabilistic/soft assignment) Compute
$$q(c_i\=k) = P(c_i\=k \| x_i, \m_{1:K}, \S_{1:K})
\propto \NN(x_i \| \m_k, \S_k)~ \pi_k$$

\item \textbf{M-step:} Update parameters (centers AND covariances)
\begin{align*}
\pi_k &= \frac{1}{n} \textstyle\sum_i q(c_i\=k)\\
\m_k &= \frac{1}{n \pi_k}~ \textstyle\sum_i q(c_i\=k)~ x_i\\
\S_k &= \frac{1}{n \pi_k}~ \textstyle\sum_i q(c_i\=k)~ x_ix_i^\T - \m_k\m_k^\T
\end{align*}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Mixture Model}{

EM iterations for Gaussian Mixture model:

~

\show{bishopEM2}

{\tiny\hfill from Bishop}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Agglomerative Hierarchical Clustering**}
\slide{Agglomerative Hierarchical Clustering}{

\item \emph{agglomerative} = bottom-up,~  \emph{divisive} = top-down

\item Merge the two groups with the smallest intergroup dissimilarity

\item Dissimilarity of two groups $G$, $H$ can be measures as
\begin{items}
\item Nearest Neighbor (or ``single linkage''): $d(G,H) = \min_{i\in G,
j\in H} d(x_i,x_j)$
\item Furthest Neighbor (or ``complete linkage''): $d(G,H) = \max_{i\in G,
j\in H} d(x_i,x_j)$
\item Group Average: $d(G,H) = \frac{1}{|G||H|}\sum_{i\in G} \sum_{j\in H} d(x_i,x_j)$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Agglomerative Hierarchical Clustering}{

\show{agglomerativeClustering}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Centering and whitening**}
\slide{Appendix: Centering \& Whitening}{\label{lastpage}

\item Some prefer to \emph{center} (shift to zero mean)
the data before applying methods:
$$x \gets x - \<x\> \comma y \gets y - \<y\>$$
this spares augmenting the bias feature $1$ to the data.

~

\item More interesting: The loss and the best choice of $\l$ depends
on the \emph{scaling} of the data. If we always scale the data in the
same range, we may have better priors about choice of $\l$ and
interpretation of the loss
$$x \gets \frac{1}{\sqrt{\Var{x}}}~ x \comma
y \gets \frac{1}{\sqrt{\Var{y}}}~ y$$

~

\item \textbf{Whitening:}~ Transform the data to remove all
correlations and variances.

{\tiny
Let $A = \Var{x} = \frac{1}{n} X^\T X - \m \m^\T$ with Cholesky decomposition $A = M M^\T$.}
$$x \gets M^\1 x \comma \text{with } \Var{M^\1 x} = \Id_d$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot

