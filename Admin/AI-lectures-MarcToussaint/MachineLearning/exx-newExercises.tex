\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\exnum}{0}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Stochastic Gradient Descent (xx Points)}

We test SDG on a squared function $f(x) = \half x^\T H x$. A Newton
method would need access to the exact Hessian $H$ and directly step to
the optimum $x^*=0$. But SDG only has access to an estimate of the
gradient. Ensuring proper convergence is much more difficult for SGD.

Let $x\in\RRR^d$ for $d=1000$. Generate a sparse\footnote{The simplest
way to represent/initialize a sparse matrix is as a set of triplets
$(i,j,X_{ij})$, where the integers $i,j$ indicate row and column, and
$X_{ij}\in\RRR$ is the matrix entry at this place. TODO: Help on
sparse matrices in python. Or non-sparse?)} random matrix
$J\in\RRR^{n\times d}$, for $n=10^5$ (or $10^6$?) as follows: In each
row, fill in 10 random numbers drawn from $\NN(0,\s^2$) at random
places. Each row either has $\s=1$ or $\s=100$, chosen randomly. We
now define $H = J^\T J$. Note that $H=\sum_{i=1}^n J_i^\T J_i$ is a
sum of rank-1 matrices.

Given this setup, we now simulate a stochastic gradient descent.
\begin{enumerate}
\item Initialize $x=\one_d$.
\item Choose $K=32$ random integers $i_k \in \{1,..,n\}$, where $k=1,..,K$. These indicate which data points we see in this iteration.
\item Compute the stochastic gradient estimate
$$ g = \frac{1}{K} \sum_{k=1}^K (J_{i_k} x) J_{i_k}^\T $$
where $J_{i_k}$ is the $i_k$th row of $J$.
\item For logging: Compute $\ell=\frac{1}{2n} x^\T H x$ and $\hat\ell=\frac{1}{2K} \sum_{k=1}^K (J_{i_k} x)^2$, and write them to a log file for later plotting.
\item Update $x$ based on $g$ useing one of the various methods (see below), and iterate from (ii)
\end{enumerate}




\exsection{NN initialization and neural random features}

(Such an approach was (once) also known as Extreme Learning.)

This exercise could be done independently of tensorflow, as we don't
need learning and the equations are basic. But if you intend to do the
next exercise as well, do it first and then try to do this one
directly in tensorflow.

Generate an $L$-layer NN layers (start with $L=2$), and $h_l$ neurons in layer $l=1,..,L$. $h_L$ is the output dimensionality. Each neuron is a ReLu.

Initialize all neurons such that its $z_i=0$ hyperplane is located randomly, with random orientation and random offset in $\UU(-1,1)^{h_{l\1}}$.

Use this network as feature for linear regression/classification.

* which data set?


\exsection{NNs in tensorflow}

mini tutorial of setting up a full NN in 

* which data set?


\exerfoot
