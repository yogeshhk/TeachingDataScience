\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Regression}
\renewcommand{\keywords}{Linear regression, non-linear features
(polynomial, RBFs, piece-wise), regularization, cross validation,
Ridge/Lasso, kernel trick}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\showh[.4]{codepics/cubicReg-compressed}
\quad
\showh[.4]{codepics/kernelRidgeClass}

~%\mypause

\item Are these linear models? ~~ Linear in \emph{what}?

-- Input: ~ No.

-- Parameters, features: ~ Yes!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\centerline{\emph{Linear Modelling is more powerful than it might seem at
first!}}

~\mypause

{\small
\item Linear Regression on non-linear features $\to$ very powerful
   (polynomials, piece-wise, spline basis, kernels)

\item Regularization (Ridge, Lasso) \& cross-validation for proper
   generalization to test data

\item Gaussian Processes and SVMs are closely related (linear in kernel
   features, but with different optimality criteria)

\item Liquid/Echo State Machines, Extreme Learning, are examples of
linear modelling on many (sort of random) non-linear features

\item Basic insights in model complexity (effective degrees of freedom)

\item Input relevance estimation (z-score) and feature selection (Lasso)

\item Linear regression $\to$ linear classification (logistic regression:
   outputs are likelihood ratios)

}

~

\item[$\To$] Linear modelling is a core of ML

{\small
(We roughly follow Hastie, Tibshirani, Friedman: \emph{Elements of
Statistical Learning})
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Linear regression}
\slide{Linear Regression}{

\item Notation:

-- input vector $x\in\RRR^d$

-- output value $y\in\RRR$

-- parameters $\b=(\b_0,\b_1,..,\b_d)^\T \in \RRR^{d\po}$

-- linear model

\eqbox{$f(x) = \b_0 + \sum_{j=1}^d \b_j x_j$}

~

~\mypause

\item Given training data $D=\{(x_i,y_i)\}_{i=1}^n$ we define
the \emph{least squares} cost (or ``loss'')

\eqbox{$L^{\text{ls}}(\b)
= \sum_{i=1}^n (y_i - f(x_i))^2$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Optimal parameters $\b$}{

\item Augment input vector with a 1 in front:
$\bar x = (1,x) = (1,x_1,..,x_d)^\T \in \RRR^{d\po}$

$\b = (\b_0,\b_1,..,\b_d)^\T \in \RRR^{d\po}$

\eqbox{$f(x) = \b_0 + \sum_{j=1}^n \b_j x_j = \bar x^\T \b$}

~\mypause

\item Rewrite sum of squares as:

\cen{$
L^{\text{ls}}(\b)
 = \sum_{i=1}^n (y_i - \bar x_i^\T\b)^2 = \norm{y - X \b}^2$}
% = (y - X \b)^\T(y - X \b)$}

~%with $X\in\RRR^{n\times d\po}, y\in\RRR^N$:

\cen{\small$X
 = \mat{c}{\bar x_1^\T \\ \vdots \\ \bar x_n^\T}
 = \mat{ccccc}{1 & x_{1,1} & x_{1,2} & \cdots & x_{1,d} \\
\vdots&&&&\vdots\\
1 & x_{n,1} & x_{n,2} & \cdots & x_{n,d}}% \in \RRR^{n\times d\po}
\comma
y = \mat{c}{y_1\\\vdots\\ y_n}% \in \RRR^n
$}

~\mypause

\item Optimum:

$\vec 0_d^\T=\frac{\del L^{\text{ls}}(\b)}{\del \b}
= - 2 (y - X \b)^\T X$
$\iff$
$\vec 0_d=X^\T X \b - X^\T y$

\eqbox{$\hat \b^{\text{ls}} = (X^\T X)^\1 X^\T y$}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Linear Regression}{

%% What if the data is not ``centered'' ($\b_0\not=0$)?

%% \item \emph{Approach 2:} ``centering the data''

%% First translate the input and output data to become ``centered''

%% $x_i' = x_i - \<x_i\>$ \qquad ($\<x_i\> \equiv 1/n \sum_{i=1}^n x_i$)

%% $y_i' = y_i - \<y_i\>$

%% For the centered data $y', X'$ the optimal $\b_0 = 0$.

%% $\to$ solve as above.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Linear Regression}{

%% \begin{algorithmic}[1]
%% \Require input data matrix $X$, output data vector $y$
%% \Ensure $\b_0$, $\b$
%% \State $n = \dim(y)$
%% \State $\bar x = 1/n~ \SUM(X,1)$ \comma
%%        $\bar y = 1/n~ \SUM(y)$ 
%%        \COMMENT{input \& output mean}
%% \State $X \gets X - \vec 1_n \bar x^\T$ \comma
%%        $y \gets y - \bar y \vec 1_n$
%%        \COMMENT{center input \& output data}
%% \State $\b = (X^\T X)^\1 X^\T y$
%% \State $\b_0 = \bar y - \bar x^\T \b$ \COMMENT{$\b_0$ for the
%% non-centered original data}
%% \end{algorithmic}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\showh[.4]{codepics/linReg}
\quad
\showh[.4]{codepics/linRegSin}

~

\hfill\tiny\texttt{./x.exe -mode 1 -dataFeatureType 1 -modelFeatureType 1}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Features}
\slide{Non-linear features}{

\item Replace the inputs $x_i\in\RRR^d$ by some non-linear features $\phi(x_i) \in \RRR^k$

\eqbox{$f(x) = \sum_{j=1}^k \phi_j(x)~ \b_j = \phi(x)^\T \b$}

~

\item The optimal $\b$ is the same

\cen{$\hat \b^{\text{ls}} = (X^\T X)^\1 X^\T y$
\quad but with\quad
$X
 = \mat{c}{\phi(x_1)^\T \\ \vdots \\ \phi(x_n)^\T}\in\RRR^{n\times k}$}

~

\item What are ``features''?

a) Features are an arbitrary set of basis functions 
%that allow us to write $f(x) = \phi(x)^\T \b$

b) Any function \emph{linear in $\b$} can be written as $f(x)
= \phi(x)^\T \b$ \\ ~~~ for some $\phi$, which we denote as ``features''

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Polynomial features}{

\item Linear: $\phi(x) = (1,x_1,..,x_d)~ \in\RRR^{1+d}$

\item Quadratic: $\phi(x) = (1,x_1,..,x_d,x_1^2,x_1x_2,x_1x_3,..,x_d^2)~ \in\RRR^{1 + d + \frac{d(d\po)}{2}}$

\item Cubic: $\phi(x) =
   (..,x_1^3,x_1^2x_2,x_1^2x_3,..,x_d^3)~ \in\RRR^{1 + d + \frac{d(d\po)}{2} + \frac{d(d\po)(d\pt)}{6}}$

~

~

\show[.35]{features}

~

\hfill\tiny\texttt{./x.exe -mode 1 -dataFeatureType 1 -modelFeatureType 1}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Piece-wise features (in 1D)}{

\item Piece-wise constant: $\phi_j(x) = [\xi_j < x \le \xi_{j\po}]$

\item Piece-wise linear: $\phi_j(x) = (1, x)^\T~ [\xi_j < x \le \xi_{j\po}]$

\item Continuous piece-wise linear: $\phi_j(x) = [x-\xi_j]_+$ \quad
(and $\phi_0(x)=x$)

~


\show[.5]{pieceWiseLinear}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Radial Basis Functions (RBF)}{

\item Given a set of centers $\{c_j\}_{j=1}^k$, define

$$\phi_j(x) = b(x,c_j) = e^{-\half \norm{x-c_j}^2} ~\in[0,1]$$

Each $\phi_j(x)$ measures similarity with the center $c_j$

~

\item Special case:

\centerline{\emph{use all training inputs $\{x_i\}_{i=1}^n$ as
centers}}

~

\cen{$\phi(x) = \mat{c}{1\\b(x,x_1)\\\vdots\\b(x,x_n)}$ \quad ($n+1$ dim)}

%% $\To$ \quad $X_{ij} = K(x_i,x_j)$ and $\hat \b^{\text{ls}} = (\vec K^\T \vec K)^\1 \vec K^\T y$
~

{\small This is related to ``kernel methods'' and GPs, but not quite
        the same---we'll discuss this later.}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Features}{

\item Polynomial

\item Piece-wise

\item Radial basis functions (RBF)

\item Splines (see Hastie Ch. 5)

~

\item Linear regression on top of rich features is extremely powerful!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Regularization}
\slide{The need for regularization}{

Noisy $\sin$ data fitted with radial basis functions
\anchor{10,-10}{\showh[.2]{codepics/overfitting}}

{\tiny\texttt{ 
./x.exe -mode 1 -n 40 -modelFeatureType 4 -dataType 2 -rbfWidth .1 -sigma .5 -lambda
1e-10}}

~\mypause

\item \textbf{Overfitting \& generalization:}

The model overfits to the data---and generalizes badly

~

\item \textbf{Estimator variance:}

When you repeat the experiment (keeping the underlying function
fixed), the regression always returns a different model estimate

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Estimator variance}
\slide{Estimator variance}{

\item Assumption:

-- The data was noisy with variance $\Var{y}=\s^2 \Id_n$


%actually sampled from $f(x) = \phi(x)^\T \b$
%$$y \sim \NN(\phi(x)^\T \b, \s), $

~

\item We computed parameters $\hat\b = (X^\T X)^\1 X^\T y$, therefore

\eqbox{$\Var{\hat\b} = (X^\T X)^\1 \s^2$}

-- high data noise $\s$ $\to$ high estimator variance

-- more data $\to$ less estimator variance: ~ $\Var{\hat\b} \propto \frac{1}{n}$

~

\item In practise we don't know $\s$, but we can estimate it based
on the deviation from the learnt model: ~ (with $k = \dim(\b) = \dim(\phi)$)

$$\hat \s^2 = \frac{1}{n-k}~ \sum_{i=1}^n (y_i - f(x_i))^2$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Estimator variance}{

\item ``Overfitting''

-- picking one specific data set
$y\sim \NN(y_\text{mean},\s^2 \Id_n)$

$\oto$ picking one specific $\hat b\sim \NN(\b_\text{mean}, (X^\T X)^\1 \s^2)$

~

\item If we could reduce the variance of the estimator, we could
reduce overfitting---and increase generalization.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

Hastie's section on shrinkage methods is great! Describes several
ideas on reducing estimator variance by reducing model
complexity. We focus on regularization.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Ridge regularization}
\key{Ridge regression}
\slide{Ridge regression: $L_2$-regularization}{

\item We add a \emph{regularization} to the cost:

\medskip
\eqbox{$L^{\text{ridge}}(\b) = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2
+ {\color{red}\l \sum_{j=2}^k \b_j^2}$}

~

NOTE: $\b_1$ is usually \emph{not} regularized!

~\mypause

\item Optimum:

%% $L^{\text{ridge}}(\b) = \norm{y - X \b}^2 + \l \norm{\b}^2$,
%% $\b\equiv \b_{1:d}$

\eqbox{$\hat \b^\text{ridge} = (X^\T X + \l I)^\1 X^\T y$}

~

{\small (where $I = \Id_k$, or with $I_{1,1}=0$ if $\b_1$ is not regularized)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item The objective is now composed of two ``potentials'': The loss,
which depends on the data and jumps around (introduces variance), and
the regularization penalty (sitting steadily at zero). Both are
``pulling'' at the optimal $\b$ $\to$ the regularization reduces
variance.

~

\item The estimator variance becomes less: $\Var{\hat\b} = (X^\T X+\l I)^\1 \s^2$

~

\item The ridge effectively reduces the complexity of the model:

~

\cen{$\text{df}(\l) = \sum_{j=1}^d \frac{d_j^2}{d_j^2+\l}$}

~

where $d_j^2$ is the eigenvalue of $X^\T X = V D^2 V^\T$

(details: Hastie 3.4.1)

%% Choice of $\l$ depends on scaling of the inputs $\to$ standardize
%% inputs (devide each $x_i$ (separately) by is standard deviation)

%% Effective degrees of freedom:

%% Usually: each feature $\phi_j(x)$ corresponds to a parameter $\b_j$ and
%% thereby a DoF; the ridge parameter $\l$ effectively reduces these
%% degrees of freedom. The equation is:


%% If $\l=0$ we have $d$ degrees of freedom (input dimensionality). $\l$
%% reduces these effective degrees of freedom. The $d_j^2$ is the
%% eigenvalue of the PCA (the SVD of $X^\T X$) -- actually this
%% equation is summing over principal component directions and says that
%% each principal component direction looses some degree of freedom depending
%% on its eigenvalue. (Bayesian view: the Gaussian prior on $\b$ has most
%% impact (will over ride the likelihood) on PCA directions with low
%% eigenvalue; whereas PCA directions with strong eigenvalue will
%% ``override'' the prior and contribute full dof)
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \item Generalization:

%% Models with many degrees of freedom (parameters) can fit training data
%% well, but might not generalize to test data.

%% An estimator based only on data (and no prior/regularization) has
%% large variance (esp.\ for few data points and many DoFs).

%% Introducing a regularization/prior might introduce a bias (e.g.,
%% towards smooth functions), but reduces the variance and may generalize
%% better.

%% ~

%% \item Interpretability:

%% If we have a large number of features, we often want to analyze which
%% are really relevant.

%% Feature relevance estimation, feature subset selection, shrinkage.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Cross validation}
\slide{Choosing $\l$: generalization error \& cross validation}{

\item $\l=0$ will always have a lower \emph{training} data error

We need to estimate the \emph{generalization} error on test data

~\mypause

\item \textbf{$k$-fold cross-validation:}

\medskip
\show[.6]{crossValidation}

\begin{algo}
\State Partition data $D$ in $k$ equal sized subsets $D = \{D_1,..,D_k\}$
\For{$i=1,..,k$}
\State compute $\hat \b_i$ on the training data $D \setminus D_i$
leaving out $D_i$
\State compute the error $\ell_i = L^\text{ls}(\hat\b_i,D_i)/|D_i|$ on the validation data $D_i$
\EndFor
\State report mean squared error $\hat \ell =  1/k\sum_i \ell_i$ and variance $1/(k\1)[(\sum_i \ell_i^2) - k\hat \ell^2]$
\end{algo}

~

\item Choose $\l$ for which $\hat \ell$ is smallest

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

quadratic features on sinus data:

\show[.5]{codepics/CV}

~

\tiny

\texttt{./x.exe -mode 4 -n 10 -modelFeatureType 2 -dataType 2 -sigma .1}

%{\texttt ./x.exe -mode 4 -n 10 -modelFeatureType 2 -sigma 1}

\texttt{./x.exe -mode 1 -n 10 -modelFeatureType 2 -dataType 2 -sigma .1}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\key{Lasso regularization}
\key{Feature selection**}
\slide{Lasso: $L_1$-regularization}{

\item We add a $L_1$ regularization to the cost:

\medskip
\eqbox{
$L^{\text{lasso}}(\b)
 = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 + {\color{red}\l \sum_{j=2}^k |\b_j|}$
}

~

NOTE: $\b_1$ is usually not regularized!

~

\item Has no closed form expression for optimum

~

{\tiny (Optimum can be found by solving a quadratic program; see appendix.)}

}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{
Lasso vs.\ Ridge:

~

\show{lassoVsRidge}

~

\item Lasso $\to$ sparsity! ~ feature selection!
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\cen{$L^{q}(\b)
 = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 + \l \sum_{j=2}^k |\b_j|^q$}

~

~


\show{regularizers}

~

\item \emph{Subset selection:} $q=0$ (counting the number of $\b_j \not= 0$)

%% Subset selection: test linear regression of each possible subset of
%% input dimensions

%% Backward and forward selection of features!!

%% FIGURE: Hastie pdf78

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Summary}{

\item \textbf{Representation:} ~ choice of features
$$ f(x) = \phi(x)^\T \b $$

~

\item \textbf{Objective:} ~ squared error ~ + ~ Ridge/Lasso regularization
$$L^{\text{ridge}}(\b) = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 + \l \norm{\b}^2_I$$

~

\item \textbf{Solver:} ~ analytical (or quadratic program for Lasso)
$$\hat \b^\text{ridge} = (X^\T X + \l I)^\1 X^\T y$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Summary}{\label{lastpage}

\item \textbf{Linear models} on non-linear features---extremely powerful

~

\cen{\tiny
\begin{tabular}{|c|c|c|}%p{.3\columnwidth}|p{.3\columnwidth}|p{.3\columnwidth}}
\begin{minipage}{.2\columnwidth}\center
linear\\
polynomial\\
piece-wise linear\\
RBF\\
kernel
\end{minipage}
&
\begin{minipage}{.15\columnwidth}\center
Ridge\\
Lasso
\end{minipage}
&
\begin{minipage}{.2\columnwidth}\center
regression\\
classification*
\end{minipage}
\end{tabular}
}

~

{\hfill\tiny *logistic regression}

~

\item Generalization ~$\oto$~ \textbf{Regularization} ~$\oto$~ complexity/DoF penalty

~

\item \textbf{Cross validation} to estimate generalization empirically $\to$
use to choose regularization parameters

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% Multiple outputs:

%% same as doing separate linear regressions

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Appendix: ~ Fun example is Reservoir Computing}{\label{lastpage}

%% \item General idea: use \emph{many random features} to
%% represent an input -- then use proper regularized linear regression to
%% predict output.

%% ~

%% \item For ordinary regression:

%% {\small
%% Use many random sigmoidal neurons $\phi_j(x) = \s(x^\T w_j)$ with
%% random weights as features.

%% Works well. (Also called ``Extreme Learning Machine'' {\tiny\url{http://www.ntu.edu.sg/home/egbhuang/}})
%% }

%% ~

%% \item For time series prediction:

%% {\small
%% Create a random Recurrent Neural Network with random recurrent weights and all neurons
%% connecting to the inputs with random weights. Let input time series
%% drive the system. Use linear regression to map from all neuron states
%% to desired output.

%% Works well. (Also called echo state network, or liquid state machine
%% {\tiny\url{http://www.scholarpedia.org/article/Echo_state_network}}
%% )
%% }


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Dual formulation of ridge regression**}
\slide{Appendix: Dual formulation of Ridge}{

\item The standard way to write the Ridge regularization:

\cen{$L^{\text{ridge}}(\b) = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2
+ \l \sum_{j=2}^k \b_j^2$}

~

\item Finding $\hat \b^\text{ridge} = \argmin_\b
L^{\text{ridge}}(\b)$ is equivalent to solving
\begin{align*}
\hat \b^\text{ridge}
&= \argmin_\b \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 \\
&\text{subject to } \sum_{j=2}^k \b_j^2 \le t
\end{align*}

~

$\l$ is the Lagrange multiplier for the inequality
constraint


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Dual formulation of Lasso regression**}
\slide{Appendix: Dual formulation of Lasso}{

\item The standard way to write the Lasso regularization:

\cen{
$L^{\text{lasso}}(\b)
 = \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 + \l \sum_{j=2}^k |\b_j|$
}

~

\item Equivalent formulation (via KKT):
\begin{align*}
\hat \b^\text{lasso}
&= \argmin_\b \sum_{i=1}^n (y_i - \phi(x_i)^\T \b)^2 \\
&\text{subject to } \sum_{j=2}^k |\b_j| \le t
\end{align*}

~

\item Decreasing $t$ is called ``shrinkage'': The space of allowed
$\b$ shrinks. Some $\b$ will become zero $\to$ feature selection
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\slidesfoot
