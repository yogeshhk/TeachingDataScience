\input{../shared/shared}

\renewcommand{\course}{Machine Learning}
\renewcommand{\coursepicture}{course_ml}
\renewcommand{\coursedate}{Summer 2019}
\renewcommand{\topic}{Probabilistic Machine Learning}
\renewcommand{\keywords}{learning as inference, Bayesian Kernel Ridge
regression = Gaussian Processes, Bayesian Kernel Logistic
Regression = GP classification, Bayesian Neural Networks}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Learning as Inference}
\slide{Learning as Inference}{

\item The parameteric view
$$P(\b|\text{Data}) = \frac{P(\text{Data}|\b)~ P(\b)}{P(\text{Data})}$$

\item The function space view
$$P(f|\text{Data}) = \frac{P(\text{Data}|f)~ P(f)}{P(\text{Data})}$$

~

\item Today:

-- Bayesian (Kernel) Ridge Regression ~ $\oto$ ~ Gaussian Process (GP)

-- Bayesian (Kernel) Logistic Regression ~ $\oto$ ~ GP classification

-- Bayesian Neural Networks (briefly)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{
\item Beyond learning about specific Bayesian learning methods:

~

Understand relations between

~

\begin{center}
loss/error ~ $\oto$ ~ neg-log likelihood

~

regularization ~ $\oto$ ~ neg-log prior

~

cost (reg.+loss) ~ $\oto$ ~ neg-log posterior
\end{center}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Bayesian [Kernel] Ridge$|$Logistic Regression \& Gaussian Processes}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Process = Bayesian (Kernel) Ridge Regression}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bayesian (kernel) ridge regression}
\slide{Ridge regression as Bayesian inference}{

\item We have random variables $X_{1:n}, Y_{1:n}, \b$

%($\b$ describes the function $f$, which is also a random variable)

~

\item We observe data $D=\{ (x_i, y_i) \}_{i=1}^n$ and want to compute
  $P(\b \| D)$

~

~

~

\item Let's assume:\anchor{170,-20}{\showh[.12]{regression}}


$P(X)$ is arbitrary% ~ (we learn a \emph{conditional} model $P(Y|X)$)

$P(\b)$ is Gaussian: $\b \sim \NN(0,\frac{\s^2}{\lambda}) \propto e^{-\frac{\lambda}{2\s^2}\norm{\b}^2}$

$P(Y \| X,\b)$ is Gaussian: $y = x^\T \b + \e$ \comma $\e \sim \NN(0,\s^2)$
%$y \sim e^{-\half(y-x^\T\b)^2/\s^2}$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Ridge regression as Bayesian inference}{

\item Bayes' Theorem:
$$
P(\b\|D) = \frac{P(D\|\b)~ P(\b)}{P(D)} $$
$$%% P(\b \| x_{1}, y_{1})
%% &= \frac{P(y_1 \| \b , x_1)}{P(y_1 \| x_1)} \underbrace{P(\b \|
%%   x_1)}{=P(\b)} \\
%% P(\b \| x_{1:2}, y_{1:2})
%% &= \frac{P(y_1 \| \b , x_1)}{P(y_1 \| x_1)} P(\b)
%%    \frac{P(y_2 \| \b , x_2)}{P(y_2 \| x_2)} P(\b) \\
P(\b \| x_{1:n}, y_{1:n})
= \frac{\prod_{i=1}^n P(y_i \| \b , x_i)~ P(\b)}{Z}
$$

{\small $P(D\|\b)$ is a \emph{product} of independent likelihoods for each observation $(x_i,y_i)$}

~\mypause

Using the Gaussian expressions:
$$
P(\b \| D)
=\frac{1}{Z'} \prod_{i=1}^n e^{-\frac{1}{2\s^2}(y_i-x_i^\T\b)^2}~
e^{-\frac{\lambda}{2\s^2}\norm{\b}^2}
$$\mypause
$$
-\log P(\b \| D)
=  \frac{1}{2\s^2} \[ \sum_{i=1}^n (y_i-x_i^\T \b)^2 + \l\norm{\b}^2 \] + \log Z'
$$

%\mypause
\eqbox{$-\log P(\b \| D) \propto L^\text{ridge}(\b)$}

~

\textbf{1st insight:} The \emph{neg-log posterior $P(\b\|D)$} is proportional
to the cost function $L^{\text{ridge}}(\b)$!

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Ridge regression as Bayesian inference}{

\item Let us compute $P(\b \| D)$ explicitly:
{\small
\begin{align*}
P(\b \| D)
&=\frac{1}{Z'}~ \prod_{i=1}^n e^{-\frac{1}{2\s^2}~(y_i-x_i^\T\b)^2}~
 e^{-\frac{\lambda}{2\s^2}\norm{\b}^2} \\
&=\frac{1}{Z'}~ e^{-\frac{1}{2\s^2}~\sum_i(y_i-x_i^\T\b)^2}~
 e^{-\frac{\lambda}{2\s^2}\norm{\b}^2} \\
&=\frac{1}{Z'}~ e^{-\frac{1}{2\s^2}[(y - X \b)^\T(y - X \b)+\lambda\b^\T\b]} \\
&=\frac{1}{Z'}~ e^{-\half[\frac{1}{\s^2}y^\T y + \frac{1}{\s^2} \b^\T(X^\T 
X+\lambda \Id)\b - \frac{2}{\s^2} \b^\T X^\T y ]} \\
&= \NN(\b \| \hat\b, \S)
\end{align*}\\}
This is a Gaussian with covariance and mean

\eqbox{$ \S = \s^2~ (X^\T X+\lambda \Id)^\1 \comma
 \hat\b = \frac{1}{\s^2}~ \S X^\T y
 = (X^\T X+\lambda \Id)^\1X^\T y $}

\item \textbf{2nd insight:} The mean $\hat\b$ is exactly the classical
$\argmin_\b L^{\text{ridge}}(\b)$.

\item \textbf{3rd insight:} The Bayesian approach not only
gives a mean/optimal $\hat\b$, but also a variance $\S$ of that estimate. \quad \textbf{(Cp.\ slide 02:13!)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Predictive distribution}
\slide{Predicting with an uncertain {\protect $\b$}}{

%% \item Since we not only have a mean $\hat\b$ but  also an uncertainty
%% desribed by $\S$, we can make a statement about the uncertainty of a
%% prediction:

\item Suppose we want to make a prediction at $x$. We can compute the
\textbf{predictive distribution} over a new observation $y^*$ at $x^*$:
{\small\begin{align*}
P(y^*\|x^*,D)
 &= \textstyle\int_\b P(y^* \| x^*,\b)~ P(\b \| D)~ d\b \\
 &= \textstyle\int_\b \NN(y^* \| \phi(x^*)^\T\b, \s^2)~  \NN(\b \| \hat\b, \S)~ d\b \\
 &= \NN(y^* \| \phi(x^*)^\T\hat\b,~ \s^2 + \phi(x^*)^\T \S \phi(x^*))
\end{align*}}
{\tiny Note, for $f(x)=\phi(x)^\T\b$, we have $P(f(x)\|D)
= \NN(f(x) \| \phi(x)^\T\hat\b,~ \phi(x)^\T \S \phi(x))$ without the $\s^2$}

\item So, $y^*$ is Gaussian distributed around the mean prediction
$\phi(x^*)^\T\hat\b$:

\show[.5]{BayesianPredictiveDistribution}

\hfill\tiny (from Bishop, p176)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Wrapup of Bayesian Ridge regression}{

\item \textbf{1st insight:} The \emph{neg-log posterior $P(\b\|D)$} is equal
to the cost function $L^{\text{ridge}}(\b)$.

\medskip

{\small This is a very very common relation: optimization costs
correspond to neg-log probabilities; probabilities correspond to
exp-neg costs.

}

\medskip

\item \textbf{2nd insight:} The mean $\hat\b$ is exactly the classical
$\argmin_\b L^{\text{ridge}}(\b)$

\medskip

{\small More generally, the most likely parameter $\argmax_\b P(\b|D)$
is also the least-cost parameter $\argmin_\b L(\b)$. In the Gaussian
case, most-likely $\b$ is also the mean.

}

\medskip

\item \textbf{3rd insight:} The Bayesian inference approach not only
gives a mean/optimal $\hat\b$, but also a variance $\S$ of that estimate

\medskip

{\small This is a core benefit of the Bayesian view: It naturally
provides a probability distribution over predictions (\emph{``error
bars''}), not only a single prediction.

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernel Bayesian Ridge Regression}{

\item As in the classical case, we can consider arbitrary features
$\phi(x)$

\item .. or directly use a kernel $k(x,x')$:
{\small\begin{align*}
P(f(x)\|D)
 &= \NN(f(x) \| \phi(x)^\T\hat\b,~ \phi(x)^\T \S \phi(x)) \\
\phi(x)^\T\hat\b
 &= \phi(x)^\T X^\T (X X^\T + \l \Id)^\1 y \\
 &= \k(x) (K + \l \Id)^\1 y \\
\phi(x)^\T \S \phi(x)
 &= \phi(x)^\T \s^2~ (X^\T X+\l \Id)^\1 \phi(x) \\
 &= \frac{\s^2}{\l} \phi(x)^\T \phi(x)
  - \frac{\s^2}{\l} \phi(x)^\T X^\T (X X^\T + \l \Id_k)^\1 X \phi(x) \\
 &= \frac{\s^2}{\l} k(x,x)
  - \frac{\s^2}{\l} \k(x) (K + \l \Id_n)^\1 \k(x)^\T
\end{align*}
{\tiny

3rd line: As on slide 05:2

2nd to last line: Woodbury identity $(A+UBV)^\1 = A^\1 - A^\1 U (B^\1 + VA^\1U)^\1 V A^\1$ with $A=\l \Id$

}

}
~

\item In standard conventions $\l=\s^2$, i.e.\ $P(\b) = \NN(\b|0,1)$

-- Regularization: scale the covariance function (or features)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Processes}{

\textbf{are equivalent to Kernelized Bayesian Ridge Regression}

{\tiny (see also Welling: ``Kernel Ridge Regression'' Lecture Notes;  Rasmussen \& Williams sections 2.1 \& 6.2; Bishop sections 3.3.3 \& 6)\\}

~

\item But it is insightful to introduce them again from the ``function space view'':
GPs define a probability distribution over functions; they are the
infinite dimensional generalization of Gaussian vectors

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Gaussian Process}
\key{Bayesian Kernel Ridge Regression}
\slide{Gaussian Processes -- function prior}{

\item The function space view
$$P(f|D) = \frac{P(D|f)~ P(f)}{P(D)}$$

\item A Gaussian Processes \textbf{prior} $P(f)$ defines a probability distribution over
functions:
\begin{items}
\item A function is an infinite dimensional thing -- how could we
define a Gaussian distribution over functions?
\item For every finite set $\{x_1,..,x_M\}$, the function values
$f(x_1),..,f(x_M)$ are Gaussian distributed with mean and covariance
\begin{align*}
\Exp{f(x_i)} &= \mu(x_i) \qquad\text{(often zero)} \\ 
\cov{f(x_i), f(x_j)} &= k(x_i,x_j)
\end{align*}
Here, $k(\cdot,\cdot)$ is called \textbf{covariance function}
\end{items}

\item Second, for Gaussian Processes we typically have a Gaussian \textbf{data likelihood} $P(D|f)$, namely
$$P(y\|x,f) = \NN(y \| f(x),\s^2)$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Processes -- function posterior}{

\item The \textbf{posterior} $P(f|D)$ is also a Gaussian Process, with the following mean of $f(x)$, covariance of $f(x)$ and $f(x')$: (based on slide 10 (with $\l=\s^2$))
\begin{align*}
\Exp{f(x)\|D}
 &= \k(x) (K + \l \Id)^\1 y ~+~ \mu(x) \\
\cov{f(x), f(x') \| D}
 &= k(x,x') - \k(x') (K + \l \Id_n)^\1 \k(x')^\T
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gaussian Processes}{

~

\show{gaussianProcess1}
{\tiny\hfill(from Rasmussen \& Williams)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{GP: different covariance functions}{

~

\show{gaussianProcess2}
{\tiny\hfill(from Rasmussen \& Williams)}

~

\item These are examples from the $\g$-exponential covariance function

$$k(x,x') = \exp\{-|(x-x')/l|^\g\}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{GP: derivative observations}{

~

\show{gaussianProcess3}
{\tiny\hfill(from Rasmussen \& Williams)}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Bayesian Kernel Ridge Regression = Gaussian Process

~

\item GPs have become a standard regression method

\item If exact GP is not efficient enough, many approximations exist,
e.g.\ sparse and pseudo-input GPs

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{GP classification = Bayesian (Kernel) Logistic Regression}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bayesian (kernel) logistic regression}
\slide{Bayesian Logistic Regression (binary case)}{

\item $f$ now defines a discriminative function:
\begin{align*}
P(X)
 &= \text{arbitrary} \\
P(\b)
 &= \NN(\b | 0,\frac{2}{\lambda}) \propto \exp\{-\l\norm{\b}^2\}\\
P(Y\=1 \| X,\b)
 &=\s(\b^\T\phi(x))
\end{align*}

\item Recall
$$L^\text{logistic}(\b)
= - \sum_{i=1}^n \log p(y_i \| x_i) + {\color{red} \l\norm{\b}^2}$$

\item Again, the parameter posterior is
\begin{align*}
P(\b|D)
 &\propto P(D \| \b)~ P(\b) \propto \exp\{-L^\text{logistic}(\b)\}
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Bayesian Logistic Regression}{

\small

\item Use \textbf{Laplace approximation} (2nd order Taylor for $L$) at
$\b^* = \argmin_\b L(\b)$:
\begin{align*}
L(\b)
 &\approx L(\b^*) + \bar\b^\T \nabla + \half \bar\b^\T H \bar\b
 \comma \bar\b = \b-\b^*
\end{align*}
At $\b^*$ the gradient $\nabla=0$ and $L(\b^*)=\text{const}$. Therefore
\begin{align*}
\tilde P(\b|D)
 &\propto \exp\{ - \half \bar\b^\T H \bar\b\} \\
\To~ P(\b|D)
 &\approx \NN(\b | \b^* , H^\1)
\end{align*}

\item Then the
predictive distribution of the \emph{discriminative function} is also
Gaussian!
\begin{align*}
P(f(x)\|D)
 &= \textstyle\int_\b P(f(x) \| \b)~ P(\b \| D)~ d\b \\
 &\approx \textstyle\int_\b \NN(f(x) \| \phi(x)^\T\b, 0)~  \NN(\b \| \b^*, H^\1)~ d\b \\
 &= \NN(f(x) \| \phi(x)^\T\b^*, \phi(x)^\T H^\1 \phi(x))
 ~=:~ \NN(f(x) \| f^*, s^2)
\end{align*}

\item The predictive distribution over the label $y\in\{0,1\}$:
\begin{align*}
P(y(x)\=1\|D)
 &= \Int_{f(x)} \s(f(x))~ P(f(x)|D)~ df \\
 &\approx \s((1+ s^2 \pi/8)^{\myminus\half} f^*)
\end{align*}
which uses a probit approximation of the convolution.

$\to$ The variance $s^2$ pushes the predictive class probabilities towards $0.5$.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Kernelized Bayesian Logistic Regression}{

\item As with Kernel Logistic Regression, the MAP discriminative
function $f^*$ can be found iterating the Newton method $\oto$
iterating GP estimation on a \emph{re-weighted} data set.

\item The rest is as above.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Gaussian Process Classification}
\key{Bayesian Kernel Logistic Regression}
\slide{Kernel Bayesian Logistic Regression}{

\textbf{is equivalent to Gaussian Process Classification}

~

\item GP classification became a standard classification method, if
the prediction needs to be a meaningful probability that takes
the \emph{model uncertainty} into account.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Bayesian Neural Networks}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bayesian Neural Networks}
\slide{Bayesian Neural Networks}{


\item Simple ways to get uncertainty estimates:
\begin{items}
\item Train ensembles of networks ~ (e.g.\ bootstrap ensembles)
\item Treat the output layer fully probabilistic (treat the trained NN body as feature vector $\phi(x)$, and apply Bayesian Ridge/Logistic Regression on top of that)
\end{items}

~

\item Ways to treat NNs inherently Bayesian:
\begin{items}
\item Infinite single-layer NN $\to$ GP ~ (classical work in 80/90ies)
\item Putting priors over weights ~ (``Bayesian NNs'', Neil, MacKay, 90ies)
\item Dropout ~ (much more recent, see papers below)
\end{items}

~

\item Read

{\small
Gal \& Ghahramani: \emph{Dropout as a bayesian approximation: Representing model uncertainty in deep learning} (ICML'16)

\medskip
Damianou \& Lawrence: \emph{Deep gaussian processes} (AIS 2013)

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Dropout for Uncertainty Prediction}
\slide{Dropout in NNs as Deep GPs}{

\item Deep GPs are essentially a a chaining of Gaussian Processes
\begin{items}
\item The mapping from each layer to the next is a GP
\item Each GP could have a different prior (kernel)
\end{items}

~

\item Dropout in NNs
\begin{items}
\item Dropout leads to randomized prediction
\item One can estimate the mean prediction from $T$ dropout samples (MC estimate)
\item Or one can estimate the mean prediction by averaging the weights of the network (``standard dropout'')
\item Equally one can MC estimate the variance from samples
\item Gal \& Ghahramani show, that a Dropout NN is a Deep GP (with very special kernel), and the ``correct'' predictive variance is this MC estimate plus $\frac{pl^2}{2n\l}$ (kernel length scale $l$, regularization $\l$, dropout prob $p$, and $n$ data points)
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{No Free Lunch**}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{No Free Lunch}
\slide{No Free Lunch}{

\item Averaged over \emph{all} problem instances, any algorithm performs equally. (E.g.\ equal to random.)
\begin{items}
\item ``there is no one model that works best for every problem''
\end{items}
{\tiny

Igel \& Toussaint: \emph{On Classes of Functions for which No Free Lunch Results Hold} (Information Processing Letters 2003)

}

~

\item Rigorous formulations formalize this ``average over \emph{all} problem instances''. E.g.\ by assuming a uniform prior over problems
\begin{items}
\item In black-box optimization, a uniform distribution over underlying objective functions $f(x)$
\item In machine learning, a uniform distribution over the hiddern true function $f(x)$
\end{items}
... and NLF always considers \emph{non-repeating queries}.

~

\item But what does \emph{uniform distribution over functions mean?}

~\pause\small

\item NLF is trivial: when any previous query yields NO information at all about the results of future queries, anything is exactly as good as random guessing

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conclusions}{\label{lastpage}

\item Probabilistic inference is a very powerful concept!

-- Inferring about the world given data

-- Learning, decision making, reasoning can view viewed as forms of
   (probabilistic) inference

~

\item We introduced Bayes' Theorem as the fundamental form of
 probabilistic inference

~

\item Marrying Bayes with (Kernel) Ridge (Logisic) regression yields

-- Gaussian Processes

-- Gaussian Process classification

~

\item We can estimate uncertainty also for NNs

-- Dropout

-- Probabilistic weights and variational approximations; Deep GPs

~

\item No Free Lunch for ML!

%% ~

%% \item Generally the Bayesian framework is a toolbox to design
%%   your own models (which variables, which
%%   dependencies/hierarchies/structure, etc) and derive the respective
%%   learning and/or inference algorithm for this specific model.
%% \quad
%% {$\to$ Graphical Models}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot
