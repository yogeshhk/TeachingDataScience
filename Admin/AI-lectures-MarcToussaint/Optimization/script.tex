\input{../shared/shared}

\renewcommand{\course}{Introduction to Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}

\script

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\slidefont
\fancyhfoffset{0mm}

\input{01-introduction.tex}
\input{02-unconstrainedOpt.tex}
\input{03-constrainedOpt.tex}
\input{04-convexProblems.tex}
\input{05-globalBayesianOptimization.tex}
\input{06-blackBoxOpt.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\fancyhfoffset{0mm}
\parindent 0ex
\parskip 1ex

\section{Exercises}

\input{e01-introduction.tex}
\input{e02-unconstrainedOpt.tex}
\input{e03-newtonMethods.tex}
\input{e04-constraints.tex}
\input{e05-lagrange.tex}
\input{e06-primaldual.tex}
\input{e07-convexOpt.tex}
\input{e08-ILPrelaxation.tex}
\input{e09-globalOptim.tex}
\input{e10-blackBoxOpt.tex}
\input{e11-blackBoxOpt_2.tex}
\input{e12-stochasticSearch.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Bullet points to help learning}

\emph{This is a summary list of core topics in the lecture and
  intended as a guide for preparation for the exam. Test yourself also
  on the bullet points in the table of contents. Going through all
  exercises is equally important.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Optimization Problems in General}

\begin{itemize}
\item Types of optimization problems

-- General constrained optimization problem definition

-- Blackbox, gradient-based, 2nd order

-- Understand the differences

\item Hardly coherent texts that cover all three

-- constrained \& convex optimization

-- local \& adaptive search

-- global/Bayesian optimization

\item In the lecture we usually only consider inequality constraints
  (for simplicity of presentation)

-- Understand in all cases how also equality constraints could be
handled
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Basic Unconstrained Optimization}

\begin{itemize}
\item Plain gradient descent

-- Understand the stepsize problem

-- Stepsize adaptation

-- Backtracking line search (2:21)

\item Steepest descent

-- Is the gradient the steepest direction?

-- Covariance (= invariance under linear transformations) of the
steepest descent direction

\item 2nd-order information

-- 2nd order information can improve direction \& stepsize

-- Hessian needs to be pos-def ($\oto$ $f(x)$ is convex) or
modified/approximated as pos-def (Gauss-Newton, damping)

\item Newton method

-- Definition

-- Adaptive stepsize \& damping

\item Gauss-Newton

-- $f(x)$ is a sum of squared cost terms

-- The approx.\ Hessian $2\na\phi(x)^\T \na\phi(x)$ is always
semi-pos-def!

\item Quasi-Newton

-- Accumulate gradient information to approximate a Hessian

-- BFGS, understand the term $\frac{\d \d^\T}{\d^\T y}$


\item Conjugate gradient

-- New direction $d'$ should be ``orthogonal'' to the previous $d$,
but relative to the local quadratic shape, $d'^\T A d = 0$ (= $d'$ and
$d$ are conjugate)

-- On quadratic functions CG converges in $n$ iterations

\item Rprop

-- Seems awfully hacky

-- Every coordinate is treated separately. No invariance under
rotations/transformations.

-- Change in gradient sign $\to$ reduce stepsize; else increase

-- Works surprisingly well and robust in practice

\item Convergence

-- With perfect line search, the extrem (finite \& positive!) eigenvalues of the
Hessian ensure convergence

-- The Wolfe conditions (acceptance criterion for backtracking line
search) ensure a ``significant'' decrease in $f(x)$, which also leads
to convergence

\item Trust region

-- Alternative to stepsize adaptation and backtracking

\item Evaluating optimization costs

-- Be aware in differences in convention. Sometimes ``1
iteration''=many function evaluations (line search)

-- Best: always report on \# function evaluations

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Constrained Optimization}

\begin{itemize}
\item Overview

-- General problem definition

-- Convert to series of unconstrained problems: penalty, log barrier,
\& Augmented Lagrangian methods

-- Convert to series of QPs and line search: Sequential Quadratic Programming

-- Convert to larger unconstrained problem: primal-dual Newton method

-- Convert to other constrained problem: dual problem


\item Log barrier method

-- Definition

-- Understand how the barrier gets steeper with $\mu\to 0$ (not
$\mu\to\infty$!)

-- Iterativly decreasing $\mu$ generates the \emph{central path}

-- The gradient of the log barrier generates a Lagrange term with
$\l_i = -\frac{\mu}{g_i(x)}$!

$\to$ Each iteration solves the modified (approximate) KKT condition


\item Squared penalty method

-- Definition

-- Motivates the Augmented Lagrangian

\item Augmented Lagrangian

-- Definition

-- Role of the squared penalty: ``measure'' how strong $f$ pushes into
the constraint

-- Role of the Lagrangian term: generate counter force

-- Unstand that the $\l$ update generates the ``desired gradient''


\item The Lagrangian

-- Definition

-- Using the Lagrangian to solve constrained problems on paper (set both,
$\na_x L(x,\l) = 0 $ and $\na_\l L(x,\l)=0$)

-- ``Balance of gradients'' and the first KKT condition

-- Understand in detail the full KKT conditions

-- Optima are necessarily saddle points of the Lagrangian

-- $\min_x L$ $\oto$ first KKT $\oto$ balance of gradients

-- $\max_\l L$ $\oto$ complementarity KKT $\oto$ constraints

\item Lagrange dual problem

-- primal problem: $\min_x \max_{l\ge 0} L(x,\l)$

-- dual problem: $\max_{\l\ge 0} \min_x L(x,\l)$

-- Definition of Lagrange dual

-- Lower bound and strong duality

\item Primal-dual Newton method to solve KKT conditions

-- Definition \& description

\item Phase I optimization

-- Nice trick to find feasible initialization

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Convex Optimization}

\begin{itemize}
\item Definitions

-- Convex, quasi-convex, uni-modal functions

-- Convex optimization problem

\item Linear Programming

-- General and standard form definition

-- Converting into standard form

-- LPs are efficiently solved using 2nd-order log barrier, augmented
Lagrangian or primal-dual methods

-- Simplex Algorithm is classical alternative; walks on the constraint
edges instead of the interior

\item Application of LP:

-- Very important application of LPs: LP-relaxations of integer linear
programs

\item Quadratic Programming

-- Definition

-- QPs are efficiently solved using 2nd-order log barrier, augmented
Lagrangian or dual-primal methods

-- Sequential QP solves general (non-quadratic) problems by defining a
local QP for the step direction followed by a line search in that
direction

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Search methods for Blackbox optimization}

\begin{itemize}

\item Overview

-- Basic downhill running: mostly ignore the collected data

-- Use the data to shape search: stochastic search, EAs, model-based
search

-- Bayesian (global) optimization

\item Basic downhill running

-- Greedy local search: defined by neighborhood $\NN$

-- Stochastic local search: defined by transition probability $q(y|x)$

-- Simulated Annealing: also accepts ``bad'' steps depending on
temperature; theoretically highly relevant, practically less

-- Random restarts of local search can be efficient

-- Iterated local search: use meta-neighborhood $\NN^*$ to restart

-- Coordinate \& Pattern search, Twiddle: use heuristics to walk along
coordinates

-- Nelder-Mead simplex method: reflect, expand, contract, shrink

\item Stochastic Search

-- General scheme: sample from $p_\t(x)$, update $\t$

-- Understand the crucial role of $\t$: $\t$ captures all that is
maintained and updated depending on the data; in EAs, $\t$ is a population;
in ESs, $\t$ are parameters of a Gaussian

-- Categories of EAs: ES, GA, GP, EDA

-- CMA: adapting $C$ and $\s$ based on the path of the mean

\item Model-based Optimization

-- Precursor of Bayesian Optimization

-- Core: smart ways to keep data $D$ healthy

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bayesian Optimization}

\begin{itemize}

\item Multi-armed bandit framework

-- Problem definition

-- Understand the concepts of exploration, exploitation \& belief

-- Optimal Optimization would imply to plan (exactly) through belief
space

-- Upper Confidence Bound (UCB) and confidence interval

-- UCB is optimistic

\item Global optimization

-- Global optimization = infinite bandits

-- Locally correlated bandits $\to$ Gaussian Process beliefs

-- Maximum Probability of Improvement

-- Expected Improvement

-- GP-UCB

\item Potential pitfalls

-- Choice of prior belief (e.g.\ kernel of the GP) is crucial

-- Pure variance-based sampling for radially symmetric kernel
$\approx$ grid sampling

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\addcontentsline{toc}{section}{Index}
\printindex

\end{document}

