\input{../shared/shared}

\renewcommand{\course}{Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}
\renewcommand{\exnum}{6}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Min-max $\not=$ max-min}

Give a function $f(x,y)$ such that
$$\max_y \min_x f(x,y) \not = \min_x \max_y f(x,y)$$

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Lagrangian Method of Multipliers}

We have previously defined the ``hole function'' as $f^c_{\text{hole}}(x) =
1-\exp(-x^\T C x)$, where $C$ is a $n\times n$ diagonal matrix with $C_{ii} =
c^{\frac{i-1}{n-1}}$.  Assume conditioning $c=10$ and use the Lagrangian Method
of Multipliers to solve on paper the following constrained optimization problem
in $2D$.

\begin{align}
\min_x f^c_{\text{hole}}(x) \st& h(x)=0 \\
h(x) = v^\T x - 1
\end{align}

Near the very end, you won't be able to proceed until you have special values
for $v$. Go as far as you can without the need for these values.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\exsection{Primal-dual Newton method}

Slide 03:38 describes the primal-dual Newton method. Implement it to
solve the same constrained problem we considered in the last
exercise.

a) $d=-\na r(x,\l)^\1 r(x,\l)$ defines the search direction. Ideally
one can make a step with factor $\a=1$ in this direction. However,
line search needs to ensure (i) dual feasibility $\l>0$, (ii) primal
feasibility $g(x)\le0$, and (iii) sufficient decrease (the Wolfe
condition). Line search decreases the step factor $\a$ to ensure these
conditions (in this order), where the Wolfe condition here reads
$$|r(z+\a d)| ~\le~ (1-\varrho_\text{ls}\a)|r(z)| \comma
z = \mat{c}{x\\\l} $$

b) Initialize $\mu=1$. In \emph{each} iteration decrease it by some
factor.

c) Optionally, regularize $\na
r(x,\l)$ to robustify inversion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \exsection{Maybe skip: Phase I \& Log Barriers}

%% Consider the the same problem \refeq{eq1}.

%% a) Use the method you implemented above to find a
%% feasible initialization (\emph{Phase I}). Do this by solving the
%% $n+1$-dimensional problem
%% $$\min_{(x,s)\in\RRR^{n\po}} s \st \forall_i:~ g_i(x)\le s,~ s\ge
%% -\e$$ For some very small $\e$. Initialize this with the infeasible
%% point $(1,1)\in\RRR^2$.

%% b) Once you've found a feasible point, use the standard log barrier
%% method to find the solution to the original problem (\ref{eq1}). Start
%% with $\mu=1$, and decrease it by $\mu\gets\mu/2$ in each iteration. In
%% each iteration also report $\l_i := \frac{\mu}{g_i(x)}$ for $i=1,2$.

\exerfoot
