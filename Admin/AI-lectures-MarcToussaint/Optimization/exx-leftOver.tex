\input{../shared/shared}

\renewcommand{\course}{Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}
\renewcommand{\exnum}{1}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Multi-armed bandits \& UCB}

Assume there are $n=10$ bandits. Each bandit is binary (i.e.,
$y_t\in\{0,1\}$) with $P(y_t\=1 | a_t=i) = p_i$. The agent has $T=100$
rounds to play the machines and aimes to maximize $\sum_{t=1}^T y_t$.

For simplicify, in the following assume that $p_i = i/10$ for
$i=1,..,10$. But the agent does not know this, of course.

a) Implement this bandit scenario using a proper (clock) random
seed. (Write a method that receives a $a_t$ and returns a
$y_t\in\{0,1\}$.) Simulate a random agent that chooses actions
$a_t \sim\UU(\{1,..,10\})$ uniformly. Let the agent play 10 games
(each with $T=100$ rounds). What is the random agent's average reward?

b) Implement a UCB agent. For this, the agent needs to keep track how
often he has played a machine ($n_i$) and how often this machine
returned $y=1$ (let's call this $\b_i$) or $y=0$ (let's call this
$\a_i$). What is the agent's average reward?  (Averaged over 10 games, as
above.)

c) (Bonus.) Assume the agent knows that the bandits are binary. He can
exploit this knowledge: His belief can be
$$b_t = P((p_1,..,p_n)|h_t) = \prod_i \text{Beta}(p_i|\a_i,\b_i)$$
where Beta is the so-called Beta-distribution over the Bernoulli
parameter $p_i\in[0,1]$. At Wikipedia you can find information on the
mean and variance (and also the cumulative distribution function, called
regularized incomplete beta function) of a Beta distribution. How
exactly could an agent use this to perhaps become better than the
agent in b)?

\exerfoot
