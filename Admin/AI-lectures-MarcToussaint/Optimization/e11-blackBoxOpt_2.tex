\input{../shared/shared}

\renewcommand{\course}{Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}
\renewcommand{\exnum}{11}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Model Based Optimization}

1) Implement Model-based optimization as described in slide 06:33 and use it to
solve the usual Raistringen and Rosenbrock functions.

Visualize the current estimated model at each step of the optimization procedure.

Hints:
\begin{itemize}
  \item Try out both a quadratic model $\phi_2(x) = [ 1, x_1, x_2, x_1^2, x_2^2, x_1x_2 ]$ and a linear one $\phi_1(x) = [1, x_1, x_2 ]$.
  \item Initialize the model sampling $.5 (n+1)(n+2)$ (in the case of quadratic model) or $n+1$ (in the case of linear model) points around the starting position.
  \item For any given set of datapoints $D$, compute $\beta = (X^\T X)^{-1} X^\T y$, where $X$ contains (row-wise) the data points (either $\phi_1$ or $\phi_2$) in $D$, and $y$ are the respective function evaluations.
  \item Compute $det(D)$ as $det(X^\T X)$.
  \item Solve the local maximization problem in line $12$ directly by inspecting a finite number of points in a grid-like structure around the current point.
  \item The $\Delta$ in line $18$ should just be the current dataset $D$.
  \item Always maintain at least $.5 (n+1)(n+2)$ (quadratic) or $n+1$ (linear) datapoints in D. This almost-surely ensures that the regression parameter $\beta$ is well defined.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{No Free Lunch Theorems}

Broadly speaking, the No Free Lunch Theorems state that an algorithm can be
said to outperform another one only if certain assumptions are made about the
problem which is being solved itself.  In other words, algorithms perform in
average exactly the same, if no restriction or assumption is made on the type
of problem itself.  Algorithms outperform each other only w.r.t specific
classes of problems.

2a) Read the publication ``No Free Lunch Theorems for Optimization'' by Wolpert
and Macready and get a better feel for what the statements are about.

2b, 2c, 2d) You are given an optimization problem where the search space is a
set $X$ with size $100$, and the cost space $Y$ is the set of integers $\{1,
\ldots, 100\}$.  Come up with three different algorithms, and three different
assumptions about the problem-space such that each algorithm outperforms the
others in one of the assumptions.

Try to be creative, or you will all come up with the same ``obvious'' answers.

\exerfoot
