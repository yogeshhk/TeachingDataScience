\input{../shared/shared}

\renewcommand{\course}{Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}
\renewcommand{\exnum}{2}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Quadratics}

Take the quadratic function $f_\text{sq} = x^\T C x$ with diagonal matrix $C$
and entries $C(i, i)=\lambda_i$.

~

a) Which $3$ fundamental shapes does a $2$-dimensional quadratic take?  Plot
the surface of $f_\text{sq}$ for various values of $\lambda_1, \lambda_2$
(big/small, positive/negative/zero).  Could you predict these shapes before
plotting them?

b) For which values of $\lambda_1, \lambda_2$ does $\min_x f_\text{sq}(x)$
\emph{not} have a solution?  For which does it have \emph{infinite} solutions?
For which does it have \emph{exactly} $1$ solution?  Find out empirically
first, if you have to, then analytically.

c) Use the eigen-decomposition of a generic (non-diagonal) matrix $C$ to prove
that the same $3$ basic shapes appear and that the values of $\lambda_1$ and
$\lambda_2$ have the same implications on the existence of one or more
solutions.  (In this scenario, $\lambda_1$ and $\lambda_2$ don't indicate the
diagonal entries of $C$, but its \emph{eigenvalues}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Backtracking}

Consider again the functions:
\begin{align}
f_\text{sq}(x)
 &= x^\T C x \\
f_\text{hole}(x)
 &= 1-\exp(-x^\T C x)
\end{align}
with diagonal matrix $C$ and entries $C(i,i) =
c^{\frac{i-1}{n-1}}$. We choose a conditioning\footnote{The
word ``conditioning'' generally denotes the ratio of the largest and
smallest Eigenvalue of the Hessian.} $c=10$.

~

a) Implement gradient descent with backtracking, as described on slide
02:05 (with default parameters $\r$). Test the algorithm on
$f_\text{sq}(x)$ and $f_\text{hole}(x)$ with start point
$x_0=(1,1)$. To judge the performance, create the following plots:
\begin{itemize}
  \item function value over the number of function evaluations.
  \item number of inner (line search) loops over the number of outer (gradient descent) loops.
  \item function surface, this time including algorithm's search trajectory.
\end{itemize}

b) Test also the \emph{alternative} in step 3. Further, how does the
performance change with $\r_\text{ls}$ (the backtracking stop criterion)?

% c) Implement steepest descent using $C$ as a metric. Perform the same
% evaluations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \exsection{Newton direction}

% a) Derive the Newton direction $d \propto - \he f(x)^\1 \na f(x)$ for
% $f_\text{sq}(x)$ and $f_\text{hole}(x)$.

% b) Observe that the Newton direction diverges (is undefined) in the
% concave part of $f_\text{hole}(x)$. Propose some method/tricks to fix
% this, which at least exploits the efficiency of Newton methods in the
% convex part. Any ideas are allowed.


\exerfoot
