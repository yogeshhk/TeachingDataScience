\input{../shared/shared}

\renewcommand{\course}{Optimization}
\renewcommand{\coursepicture}{optim}
\renewcommand{\coursedate}{Summer 2015}
\renewcommand{\exnum}{3}

\exercises

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Misc}

a) How do you have to choose the ``damping'' $\l$ depending on $\he
f(x)$ in line 3 of the Newton method (slide 02-18) to ensure that the
$d$ is always well defined (i.e., finite)?

b) The Gauss-Newton method uses the ``approximate Hessian''
$2\na\phi(x)^\T \na\phi(x)$. First show that for any vector
$v\in\RRR^n$ the matrix $v v^\T$ is symmetric and
semi-positive-definite.\footnote{ A matrix $A\in\RRR^{n\times n}$ is
semi-positive-definite simply when for any $x\in\RRR^n$ it holds $x^\T
A x \ge 0$. Intuitively: $A$ might be a metric as it ``measures'' the
norm of any $x$ as positive. Or: If $A$ is a Hessian, the function is
(locally) convex.}  From this, how can you argue that
$\na\phi(x)^\T \na\phi(x)$ is also symmetric and
semi-positive-definite?

c) In the context of BFGS, convince yourself that choosing $H^\1
= \frac{\d \d^\T}{\d^\T y}$ indeed fulfills the desired relation $\d =
H^\1 y$, where $\d$ and $y$ are defined as on slide 02-23. Are there other
choices of $H^\1$ that fulfill the relation? Which?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\exsection{Gauss-Newton}

\twocol{.5}{.5}{
\show{func1}
}{
\show{func2}
}

In $x\in\RRR^2$ consider the function
$$f(x) = \phi(x)^\T \phi(x) \comma \phi(x) = \mat{c}{
\sin(a x_1) \\
\sin(a c x_2) \\
2x_1 \\
2c x_2
}$$
The function is plotted above for $a=4$ (left) and $a=5$ (right,
having local minima), and conditioning $c=1$. The function is
non-convex.

a) Extend your backtracking method implemented in the last week's exercise
to a Gauss-Newton method (with constant $\l$) to solve the unconstrained
minimization problem $\min_x f(x)$ for a random start point in
$x\in[-1,1]^2$. Compare the algorithm for $a=4$ and $a=5$ and
conditioning $c=3$ with gradient descent.

b) Optimize the function using your optimization library of choice (If you can,
use a BFGS implementation.)

\exerfoot
