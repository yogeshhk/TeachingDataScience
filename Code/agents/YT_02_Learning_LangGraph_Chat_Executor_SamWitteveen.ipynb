{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Learning LangGraph - Chat Executor"
      ],
      "metadata": {
        "id": "E5TwMbBvpk4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAoQEqlXGrWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02509019-f945-403b-a92a-2d376e592ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.7/173.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --quiet -U langchain langchain_openai langgraph langchainhub langchain_experimental"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "modified from https://github.com/langchain-ai/langgraph/blob/main/examples/chat_agent_executor_with_function_calling/base.ipynb"
      ],
      "metadata": {
        "id": "BL-NMZ7ve3Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph_02\""
      ],
      "metadata": {
        "id": "guac0Zh7Gz4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model"
      ],
      "metadata": {
        "id": "nGkci88EkVwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0, streaming=True)"
      ],
      "metadata": {
        "id": "58MBHiikkQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "2_I3howTkdUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "import random\n",
        "\n",
        "@tool(\"lower_case\", return_direct=True)\n",
        "def to_lower_case(input:str) -> str:\n",
        "  \"\"\"Returns the input as all lower case.\"\"\"\n",
        "  return input.lower()\n",
        "\n",
        "@tool(\"random_number\", return_direct=True)\n",
        "def random_number_maker(input:str) -> str:\n",
        "    \"\"\"Returns a random number between 0-100. input the word 'random'\"\"\"\n",
        "    return random.randint(0, 100)\n",
        "\n",
        "tools = [to_lower_case,random_number_maker]"
      ],
      "metadata": {
        "id": "OLeIVeaEJltj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tools)"
      ],
      "metadata": {
        "id": "IQkcYH78mRmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "\n",
        "functions = [format_tool_to_openai_function(t) for t in tools]\n",
        "model = model.bind_functions(functions)"
      ],
      "metadata": {
        "id": "aqJWD8X1ke5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AgentState"
      ],
      "metadata": {
        "id": "DRyUkY3cktGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]"
      ],
      "metadata": {
        "id": "I1H-xbWNkpSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes"
      ],
      "metadata": {
        "id": "FbQtOmzQk27f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.agents import AgentFinish\n",
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state):\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we finish\n",
        "    if \"function_call\" not in last_message.additional_kwargs:\n",
        "        return \"end\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state):\n",
        "    messages = state['messages']\n",
        "    response = model.invoke(messages)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Define the function to execute tools\n",
        "def call_tool(state):\n",
        "    messages = state['messages']\n",
        "    # Based on the continue condition\n",
        "    # we know the last message involves a function call\n",
        "    last_message = messages[-1]\n",
        "    # We construct an ToolInvocation from the function_call\n",
        "    action = ToolInvocation(\n",
        "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
        "    )\n",
        "    print(f\"The agent action is {action}\")\n",
        "    # We call the tool_executor and get back a response\n",
        "    response = tool_executor.invoke(action)\n",
        "    print(f\"The tool result is: {response}\")\n",
        "    # We use the response to create a FunctionMessage\n",
        "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [function_message]}"
      ],
      "metadata": {
        "id": "2HoxaGZbkvi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph"
      ],
      "metadata": {
        "id": "UvaLZp3jlM9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)\n",
        "\n",
        "# Set the entrypoint as `agent` where we start\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    # First, we define the start node. We use `agent`.\n",
        "    # This means these are the edges taken after the `agent` node is called.\n",
        "    \"agent\",\n",
        "    # Next, we pass in the function that will determine which node is called next.\n",
        "    should_continue,\n",
        "    # Finally we pass in a mapping.\n",
        "    # The keys are strings, and the values are other nodes.\n",
        "    # END is a special node marking that the graph should finish.\n",
        "    # What will happen is we will call `should_continue`, and then the output of that\n",
        "    # will be matched against the keys in this mapping.\n",
        "    # Based on which one it matches, that node will then be called.\n",
        "    {\n",
        "        # If `tools`, then we call the tool node.\n",
        "        \"continue\": \"action\",\n",
        "        # Otherwise we finish.\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# We now add a normal edge from `tools` to `agent`.\n",
        "# This means that after `tools` is called, `agent` node is called next.\n",
        "workflow.add_edge('action', 'agent')\n",
        "\n",
        "# Finally, we compile it!\n",
        "# This compiles it into a LangChain Runnable,\n",
        "# meaning you can use it as you would any other runnable\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "2Vxw2TOClGYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it"
      ],
      "metadata": {
        "id": "AAVFBk3GlY5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELSoxDK_lRO_",
        "outputId": "5882db0a-eeea-4094-ee36-d21ca8d0e46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The agent action is tool='random_number' tool_input={'input': 'random'}\n",
            "The tool result is: 83\n",
            "The agent action is tool='lower_case' tool_input={'input': 'eighty-three'}\n",
            "The tool result is: eighty-three\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content='give me a random number and then write in words and make it lower case'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"random\"\\n}', 'name': 'random_number'}}),\n",
              "  FunctionMessage(content='83', name='random_number'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"eighty-three\"\\n}', 'name': 'lower_case'}}),\n",
              "  FunctionMessage(content='eighty-three', name='lower_case'),\n",
              "  AIMessage(content='The random number is 83. In words, it is \"eighty-three\" and in lower case, it is \"eighty-three\".')]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "# user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "id": "bhW-wd2Olh6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b3aa2b-f74a-4956-ca91-39b5caf8d9ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The agent action is tool='lower_case' tool_input={'input': 'Merlion'}\n",
            "The tool result is: merlion\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content=\"plear write 'Merlion' in lower case\"),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"input\": \"Merlion\"\\n}', 'name': 'lower_case'}}),\n",
              "  FunctionMessage(content='merlion', name='lower_case'),\n",
              "  AIMessage(content='The word \"Merlion\" in lower case is \"merlion\".')]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "# inputs = {\"input\": \"give me a random number and then write in words and make it lower case\", \"chat_history\": []}\n",
        "\n",
        "system_message = SystemMessage(content=\"you are a helpful assistant\")\n",
        "# user_01 = HumanMessage(content=\"give me a random number and then write in words and make it lower case\")\n",
        "# user_01 = HumanMessage(content=\"plear write 'Merlion' in lower case\")\n",
        "user_01 = HumanMessage(content=\"what is a Merlion?\")\n",
        "\n",
        "inputs = {\"messages\": [system_message,user_01]}\n",
        "\n",
        "app.invoke(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdwMgR5Ag6pi",
        "outputId": "f107fe06-29e5-4686-de05-2612956e9fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [SystemMessage(content='you are a helpful assistant'),\n",
              "  HumanMessage(content='what is a Merlion?'),\n",
              "  AIMessage(content='A Merlion is a mythical creature with the head of a lion and the body of a fish. It is a symbol of Singapore and is often used to represent the city-state. The name \"Merlion\" is a combination of the words \"mer\" (meaning sea) and \"lion\". The Merlion is depicted spouting water from its mouth and is a popular tourist attraction in Singapore.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6nGDuI6g-Dg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}