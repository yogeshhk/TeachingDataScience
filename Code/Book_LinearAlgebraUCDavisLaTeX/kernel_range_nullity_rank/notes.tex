

\chapter{\kernelTitle}\label{kernelrank}

Given a linear transformation $$L \colon V \to W\, ,$$ we often want  to know if it has an inverse, {\it i.e.}, if there exists a linear transformation $$M \colon W \to V$$ such that for any vector \(v \in V\), we have $$MLv=v\, ,$$ and for any vector \(w \in W\), we have $$LMw=w\, .$$ A linear transformation is a special kind of function from one vector space to another. So before we discuss which linear transformations have inverses, let us first discuss inverses of arbitrary functions. When we later specialize to linear transformations, we'll also find some nice ways of creating subspaces.

Let \(f \colon S \to T\) be a function from a set \(S\) to a set \(T\). \href{\webworkurl Homework0-Background/3/}{Recall} that \(S\) is called the {\it domain} of \(f\), \(T\) is called the {\it codomain} or {\it target} of \(f\).  
We now formally introduce a term that should be familar to you from many previous courses.

\section{Range}

\begin{definition} 
The {\bf range}  of a function $f:S\to T$ is  the set
\[
{\rm ran}(f):=\left\{ f(s) | s\in S \right\}\subset T\, .
\]
%is called\(f\). 
\end{definition} 
It is the subset of the codomain consisting of elements  to which the function \(f\) maps, {\it i.e.}, the things in \(T\) which you can get to by starting in \(S\) and applying \(f\). 

The range of a matrix is very easy to find; the range of a matrix is the span of its columns. 
Thus, calculation of the range of a matrix is very easy until the last step: simplification. One aught to end by the calculation by writing the vector space as the span of a linearly independent set. 

\begin{example}{ of calculating the range of a matrix.}
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
:= 
\left\{ 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
\colvec{x\\y\\z\\w } | \colvec{x\\y\\z\\w} \in \R^4 \right\}
$$
$$=
\left\{   
x\colvec{1\\1\\0 } +y \colvec{ 2\\2\\0 } +z \colvec{0\\1\\1} + w\colvec{1\\2\\1} \middle| x,y,z,w \in \R \right\}
.$$
That is 
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= 
\spa \left\{   
\colvec{1\\1\\0 } , \colvec{ 2\\2\\0 } , \colvec{0\\1\\1}, \colvec{1\\2\\1} \right\}
$$
but since 
$${\rm RREF} \begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
=\begin{pmatrix} 
1&2&0&1\\
0&0&1&1\\
0&0&0&0
\end{pmatrix} 
$$
the second and fourth columns (which are the non-pivot columns), can be expressed as linear combinations of columns to their left. 
They can then be removed from the set in the span to obtain
$${\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
= \spa \left\{   
\colvec{1\\1\\0 } , \colvec{0\\1\\1} \right\} .
$$
\end{example}

It might occur to you that the range of the $3\times 4$ matrix from the last example can be expressed as the range of a $3\times 2$ matrix;
$$
{\rm ran} 
\begin{pmatrix} 
1&2&0&1\\
1&2&1&2\\
0&0&1&1
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}.$$
Indeed, because the span of a set of vectors does not change when we replace the vectors with another set through an invertible process, we can calculate ranges through strings of equalities of ranges of matrices that differer by Elementary Column Operations, ECOs, ending with the range of a matrix in Column Reduced Echelon Form, CREF, with its zero columns deleted.

\begin{example} Calculating a range with ECOs\\
$$
{\rm ran} 
\begin{pmatrix} 
0&1&1\\
1&3&1\\
1&2&0
\end{pmatrix} 
\stackrel{c_1 \leftrightarrow c_3}{=}
{\rm ran} 
\begin{pmatrix} 
1&1&0\\
1&3&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= c_2-c_1}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&2&1\\
0&2&1
\end{pmatrix} 
\stackrel{c_2'= \frac12 c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&1\\
0&1&1
\end{pmatrix} 
$$
$$\stackrel{c_3'= c_3-c_2}{=}
{\rm ran} 
\begin{pmatrix} 
1&0&0\\
1&1&0\\
0&1&0
\end{pmatrix} 
={\rm ran} 
\begin{pmatrix} 
1&0\\
1&1\\
0&1
\end{pmatrix}. 
$$


\end{example}

\noindent
This is an efficient way to compute and encode the range of a matrix.
%We think this is the most sophisticated and efficient way to calculate the range of a matrix, and encourage students to use this line of thinking.

\section{Image} 

\begin{definition}
For any subset $U$ of the domain $S$ of a function $f:S\to T$  the {\bf image} of $U$ is 
$$f(U)={\rm Im} \,U:= \left\{  f(x) | x\in U \right\} .$$
\end{definition}

\begin{example} 
The image of the  cube
$$U= \left\{  a \colvec{1\\0\\0} +b \colvec{0\\1\\0}+c\colvec{0\\0\\1} \middle| a,b,c \in [0,1] \right\} $$ 
under multiplication by the matrix 
$$ M=
\begin{pmatrix}
1&0&0\\
1&1&1\\
0&0&1
\end{pmatrix}
$$
is the parallelepiped 
$$
{\rm Img}\,U= 
 \left\{  a \colvec{1\\1\\0} +b \colvec{0\\1\\0}+c\colvec{0\\1\\1} \middle| a,b,c \in [0,1] \right\} .
$$
\end{example}

Note that for most subsets $U$ of the domain $S$  of a function  $f$ the image of $U$ is not a vector space. 
The range of a function is the particular case of the image where the subset of the domain is the entire domain; ${\rm ran} f = {\rm Img} S$. 
For this reason, the range of $f$ is also sometimes called the {image} of $f$ and is sometimes denoted ${ \rm im}(f)$ or  $f(S).$  We have seen that the range of a matrix is always a span of vectors, and hence a vector space. 

Note that we prefer the phrase ``range of $f$" to the phrase ``image of f"  
because we wish to avoid confusion between homophones;
 the word 
``image" is also used to describe a single element of the codomain assigned to a single element of the domain. 
For example, 
one might say of 
the function $A:\mathbb{R}\to\mathbb{R}$ with rule of correspondence $A(x=)=2x-1$ for all $x$ in $\mathbb{R}$ that the image of $2$ is $3$ with this second meaning of the word ``image" in mind. 
By contrast, one would never say that the range of $2$ is $3$ since the former is not a function and the latter is not a set.


For thinking about inverses of function we want to think in the oposite direction in a sense. 
\begin{definition} 
The {\bf pre-image}\index{Pre-image} of any subset $U \subset T$ is 
\[
f^{-1}(U):=\{ s\in S | f(s)\in U \}\subset S.
\]
\end{definition}
The pre-image of a set \(U\) is the set of all elements of \(S\) which map to \(U\). 
\begin{example}
The pre-image of the set $U= \left\{ a\colvec{2\\1\\1 } \middle| a\in [0,1]\right\}$ (a line segment) under the matrix 
$$ 
M=\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix}: \R^3\to \R^3
$$
is the set 
\begin{eqnarray*}
M^{-1} U&=& \left\{  x \middle| Mx=v {\rm ~for ~some~ } v\in U \right\} \\[3mm]
&=& \left\{  \colvec{x\\y\\z} \middle| 
\begin{pmatrix}
1&0&1\\
0&1&1\\
0&1&1
\end{pmatrix} \colvec{x\\y\\z} =a\colvec{2\\1\\1}  {\rm ~for ~some~ } a\in [0,1] \right\} .
\end{eqnarray*}
Since
$$
{\rm RREF }
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&1&1&a
\end{amatrix} 
= 
\begin{amatrix}{3}
1&0&1&2a\\
0&1&1&a\\
0&0&0&0
\end{amatrix} 
$$
we have 
$$ M^{-1} U = \left\{   a \colvec{2\\1\\0} +b \colvec{-1\\-1\\ 1}\,  \middle|\,  a\in [0,1] ,b\in \R\right\},$$
a strip from a plane in $\R^3$.
\end{example}


\begin{figure}
\begin{center}
\includegraphics[scale=.25]{functions.jpg}
\end{center}
\caption{For the function $f:S\to T$, $S$ is the domain\index{Domain}, $T$ is the target/codomain\index{Codomain}\index{Target|see{Codomain}}, $f(S)$ is the range and $f^{-1}(U)$ is the
pre-image of $U\subset T$.}
\end{figure}

\subsection{ One-to-one and Onto}
The function \(f\) is {\bf one-to-one} (sometimes denoted 1:1) if different elements in \(S\) always map to different elements in \(T\). That is, \(f\) is one-to-one if for any elements \(x \neq y \in S,\) we have that \(f(x) \neq f(y)\), as pictured below.
\begin{center}
\includegraphics[scale=.25]{121.jpg}
\end{center} 
One-to-one functions are also called {\bf injective} functions (and sometimes called monomorphisms.) Notice that injectivity is a condition on the pre-images of \(f\).

The function \(f\) is {\bf onto} if every element of \(T\) is mapped to by some element of \(S\). That is, \(f\) is onto if for any \(t \in T\), there exists some \(s \in S\) such that \(f(s)=t\). Onto functions are also called {\bf surjective} functions (and sometimes epimorphisms.) Notice that surjectivity is a condition on the range of \(f\).
\begin{center}
\includegraphics[scale=.25]{onto.jpg}
\end{center} 

If \(f\) is both injective and surjective, it is {\bf bijective} (or an isomorphism.)
\begin{center}
\includegraphics[scale=.25]{biject.jpg}
\end{center} 
\begin{theorem}
A function \(f \colon S \to T\) has an inverse function \(g \colon T \to S\) if and only if $f$ is bijective.
\end{theorem}
\begin{proof}
This is an ``if and only if'' statement  so the proof has two parts.
\begin{enumerate}
\item {\it (Existence of an inverse $\Rightarrow$ bijective.)}

Suppose that \(f\) has an inverse function \(g\). We need to show $f$ is bijective, which
we break down into injective and surjective.
\begin{itemize}
\item The function \(f\) is injective: 
Suppose that we have \(s,s' \in S\) such that \(f(s)=f(s')\). We must have that \(g(f(s))=s\) for any \(s \in S\), so in particular \(g(f(s))=s\) and \(g(f(s'))=s'\). But since \(f(s)=f(s'),\) we have \(g(f(s))=g(f(s'))\) so \(s=s'\). Therefore, \(f\) is injective.

\item The function \(f\) is surjective:
Let \(t\) be any element of \(T\). We must have that \(f(g(t))=t\). Thus, \(g(t)\) is an element of \(S\) which maps to \(t\). So \(f\) is surjective.
\end{itemize}

\item {\it (Bijectivity $\Rightarrow$ existence of an inverse.)}
Suppose that \(f\) is bijective. Hence \(f\) is surjective, so every element \(t \in T\) has at least one pre-image. Being bijective, \(f\) is also injective, so every \(t\) has no more than one pre-image. Therefore, to construct an inverse function \(g\), we simply define \(g(t)\) to be the unique pre-image \(f^{-1}(t)\) of \(t\).
\end{enumerate}
\end{proof}

Now let us specialize to functions \(f\) that are linear maps between two vector spaces. Everything we said above for arbitrary functions is exactly the same for linear functions. However, the structure of vector spaces lets us say much more about one-to-one and onto functions whose domains are vector spaces than we can say about functions on general sets.  For example, we know that a linear function always sends $0_V$ to $0_W$, {\it i.e.},
\Shabox{1.1}{$f(0_V)=0_W$}  
In Review Exercise~\ref{injectivekernalprob}, you will show that a linear transformation is one-to-one if and only if $0_V$ is the only vector that is sent to $0_W$. Linear functions are unlike  arbitrary functions between sets in that, by looking at {\it just one} (very special) vector, we can figure out whether $f$ is one-to-one!  
\subsection{Kernel}
Let \(L \colon V \to W\) be a linear transformation. Suppose \(L\) is \emph{not} injective.  Then we can find $v_1 \neq v_2$ such that $Lv_1=Lv_2$.  So $v_1-v_2\neq 0$, but
\[
L(v_1-v_2)=0.
\]

\begin{definition}
If $L \colon V\rightarrow W$ is a linear function  then the set 
\index{Kernel}
\[
\ker L = \{v\in V ~|~ Lv=0_W \}\subset V 
\]
is called the {\bf kernel of $L$}.
\end{definition}


Notice that if $L$ has matrix $M$ in some basis, then finding the kernel of~$L$ is equivalent to solving the homogeneous system 
\[
MX=0.
\]

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$.  Is $L$ one-to-one?

To find out, we can solve the linear system:
\[
\begin{amatrix}{2}
1 & 1 & 0 \\
1 & 2 & 0 \\
0 & 1 & 0 \\
\end{amatrix} \sim
\begin{amatrix}{2}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{amatrix}.
\]
Then all solutions of $MX=0$ are of the form $x=y=0$.  In other words, $\ker L=\{0\}$, and so $L$ is injective.
\end{example}

%\begin{center}\href{\webworkurl ReadingHomework24/1/}{Reading homework: problem \ref{kernelrank}.1}\end{center}
\Reading{KernelRangeNullityRank}{1}

Notice that in the above example we found 
$$\ker 
\begin{pmatrix}
1 & 1  \\
1 & 2  \\
0 & 1 \\
\end{pmatrix} =
\ker
\begin{pmatrix}
1 & 0 \\
0 & 1  \\
0 & 0  \\
\end{pmatrix}.
$$
In general, an efficient way to get the  kernel of a matrix is to write a string of equalities between kernels of matrices which differ by row operations and, once RREF is reached, note that the linear relationships between the columns for a basis for the nullspace.

\begin{example} of calculating the kernel of a matrix.\\
$$
\ker 
\begin{pmatrix}
1&2&0&1 \\
1&2&1&2 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&1&1
\end{pmatrix}
=
\ker 
\begin{pmatrix}
1&2&0&1 \\
0&0&1&1 \\
0&0&0&0
\end{pmatrix}
$$
$$
=\spa \left\{ 
\colvec{-2\\1 \\0\\0}, \colvec{-1\\0\\-1 \\1 } 
 \right\} .
$$
The two column vectors in this last line describe linear relations between the columns $c_1,c_2,c_3,c_4$. 
In particular $-2c_1+1c_2=0$ and $-c_1-c_3 +c_4=0$. 
\end{example}
In general, a description of the kernel of a matrix should be of the form 
$\spa \{ v_1,v_2,\dots,v_n\}$ with one vector $v_i$ for each non-pivot column. 
To agree with the standard procedure, think about how to describe each non-pivot column in terms of columns to its left; this will yield an expression of the form wherein each vector  has a 1 as its last non-zero entry. (Think of Column Reduced Echelon Form, CREF.) 

Thinking again of augmented matrices, 
if a matrix has more than one element in its kernel then it is not invertible since the existence of multiple solutions to $Mx=0$ implies that ${\rm RREF} \,M\neq I$. 
However just because the kernel of a linear function is trivial does not mean that the function is invertible. 

\begin{example}
$\ker 
\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix} =\left\{  \colvec{0\\0} \right\} $
since the matrix has no non-pivot columns. However, 
$\begin{pmatrix}
1&0\\
1&1\\
0&1
\end{pmatrix}:  \R^2 \to \R^3$
is not invertible because there are many things in its codomain that are not in its range, such as  $\colvec{1\\0\\0}$. 
\end{example}

A trivial kernel only gives us half of what is needed for invertibility.




\begin{theorem}
A linear transformation $L\colon V\rightarrow W$ is injective iff $${\rm ker} L=\{0_V\}\, .$$
\end{theorem}

\begin{proof}
The proof of this theorem is Review Exercise~\ref{injectivekernalprob}.
\end{proof}



\begin{theorem}
If $L \colon V\stackrel{\rm linear}{-\!\!\!-\!\!\!-\!\!\!\rightarrow} W$  then $\ker L$ is a subspace of $V$.
\end{theorem}

\begin{proof}
Notice that if $L(v)=0$ and $L(u)=0$, then for any constants $c,d$, $L(cu+dv)=0$.  Then by the \hyperref[subspacetheorem]{subspace theorem}, the kernel of $L$ is a subspace of $V$.
\end{proof}

\begin{example}
Let \(L \colon \Re^3 \to \Re\) be the linear transformation defined by \(L(x,y,z)=(x+y+z)\). Then \(\ker L\) consists of all vectors \((x,y,z) \in \Re^3\) such that \(x+y+z=0\). Therefore, the set
\[
V=\{(x,y,z) \in \Re^3 \mid x+y+z=0\}
\]
is a subspace of \(\Re^3\).
\end{example}

When $L:V\to V$, the above theorem has an interpretation in terms of the eigenspaces of $L$. Suppose $L$ has a zero eigenvalue.  Then the associated eigenspace consists of all vectors $v$ such that $Lv=0v=0$; the $0$-eigenspace of $L$ is exactly the kernel of $L$.  



In the example where $L(x,y)=(x+y,x+2y,y)$, the map $L$ is clearly not surjective, since $L$ maps $\Re^2$ to a plane through the origin in $\Re^3$. But any plane through the origin is a subspace. In general notice 
that if $w=L(v)$ and $w'=L(v')$, then for any constants $c,d$, linearity of $L$ ensures that $$cw+dw' = L(cv+dv')\, .$$  Now the subspace theorem strikes again, and we have the following theorem:

\begin{theorem}
If $L \colon V\rightarrow W$ is linear then the range $L(V)$ is a subspace of~$W$.
\end{theorem}

\begin{example}
Let $L(x,y)=(x+y,x+2y,y)$. The range of $L$ is a plane through the origin and thus a subspace of ${\mathbb R}^3$.
Indeed the matrix of $L$ in the standard basis is 
$$
\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\, .
$$
The columns of this matrix encode the possible outputs of the function $L$ because
$$
L(x,y)=\begin{pmatrix}1&1\\1&2\\0&1\end{pmatrix}\colvec{x\\ y}=x \colvec{1\\1\\0}+y\colvec{1\\2\\1}\, . 
$$
Thus 
$$
L({\mathbb R}^2)=\spa \left\{\colvec{1\\1\\0},\colvec{1\\2\\1}\right\}
$$
Hence, when  bases and a linear transformation is are given, people often refer to its range as the {\it column space}\index{Column space}
of the corresponding matrix.
\end{example}

To find a basis of the range of $L$, we can start with a basis $S=\{v_1, \ldots, v_n\} $ for $V$. Then
the most general input for $L$ is of the form  $\alpha^1 v_1 + \cdots + \alpha^n v_n$. In turn, its most general output looks like
$$
L\big(\alpha^1 v_1 + \cdots + \alpha^n v_n\big)=\alpha^1 Lv_1 + \cdots + \alpha^n Lv_n\in \spa\{Lv_1,\ldots\,Lv_n\}\, .
$$
Thus
\[
L(V)=\spa L(S) = \spa \{Lv_1, \ldots, Lv_n\}\, .
\]
However, the set $\{Lv_1, \ldots, Lv_n\}$ may not be linearly independent; we must solve 
\[
c^1Lv_1+ \cdots + c^nLv_n=0\, ,
\]
to determine whether it is.
By finding relations amongst the elements of $L(S)=\{Lv_1,\ldots ,L v_n\}$, we can discard vectors until a basis is arrived at.  The size of this basis is the dimension of the range of $L$, which is known as the \emph{rank}\index{Rank} of $L$.


\begin{definition}
The {\bf rank} of a linear transformation $L$ is the dimension of its range.
The {\bf nullity}\index{Nullity} of a linear transformation is the dimension of the kernel.
\end{definition}
The notation for these numbers is 
 \Shabox{1}{$\nul L:=\dim \ker L$,}  
 \Shabox{1}{$\rank L:=\dim L(V) = \dim\, \text{ran}\, L$.} 

\begin{theorem}[Dimension Formula]\index{Dimension formula}\label{dimension_formula}
Let $L \colon V\rightarrow W$ be a linear transformation, with $V$ a finite-dimensional vector space\footnote{The formula still makes sense for infinite dimensional vector spaces, such as the space of all polynomials, but the notion of a basis for an infinite dimensional space is more sticky than in the finite-dimensional case.  Furthermore, the dimension formula for infinite dimensional vector spaces isn't useful for computing the rank of a linear transformation, since an equation like $\infty=\infty+x$ cannot be solved for $x$. As such, the proof presented assumes a finite basis for $V$.}.  Then:
\begin{eqnarray*}
\dim V &=& \dim \ker V + \dim L(V)\\
 &=& \nul L + \rank L.
\end{eqnarray*}
\end{theorem}



\begin{proof}
Pick a basis for $V$:
\[
\{ v_1,\ldots,v_p,u_1,\ldots, u_q \},
\]
where $v_1,\ldots,v_p$ is also a basis for $\ker L$.  This can always be done, for example, by finding a basis for the kernel of $L$ and then extending to a basis for $V$.  Then $p=\nul L$ and $p+q=\dim V$.  Then we need to show that $q=\rank L$.  To accomplish this, we show that 
$\{L(u_1),\ldots,L(u_q)\}$ is a basis for $L(V)$.

To see that $\{L(u_1),\ldots,L(u_q)\}$ spans $L(V)$, consider any vector $w$ in $L(V)$.  Then we can find constants $c^i, d^j$ such that:
\begin{eqnarray*}
w &=& L(c^1v_1 + \cdots + c^pv_p+d^1u_1 + \cdots + d^qu_q)\\
  &=& c^1L(v_1) + \cdots + c^pL(v_p)+d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& d^1L(u_1)+\cdots+d^qL(u_q) \text{ since $L(v_i)=0$,}\\
\Rightarrow L(V) &=& \spa \{L(u_1), \ldots, L(u_q)  \}.
\end{eqnarray*}

Now we show that $\{L(u_1),\ldots,L(u_q)\}$ is linearly independent.  We argue by contradiction. Suppose there exist constants $d^j$ (not all zero) such that
\begin{eqnarray*}
0 &=& d^1L(u_1)+\cdots+d^qL(u_q)\\
  &=& L(d^1u_1+\cdots+d^qu_q).\\
\end{eqnarray*}
But since the $u^j$ are linearly independent, then $d^1u_1+\cdots+d^qu_q\neq 0$, and so $d^1u_1+\cdots+d^qu_q$ is in the kernel of $L$.  But then $d^1u_1+\cdots+d^qu_q$ must be in the span of $\{v_1,\ldots, v_p\}$, since this was a basis for the kernel.  This contradicts the assumption that $\{ v_1,\ldots,v_p,u_1,\ldots, u_q \}$ was a basis for $V$, so we are done.
\end{proof}

%\begin{center}\href{\webworkurl ReadingHomework24/2/}{Reading homework: problem \ref{kernelrank}.2}\end{center}
\Reading{KernelRangeNullityRank}{2}

\begin{example} (Row rank equals column rank)\\
Suppose $M$ is an $m\times n$ matrix. The matrix  $M$ itself is a linear transformation $M:{\mathbb R}^n \rightarrow {\mathbb R}^m$ but it  must also  be the matrix of some linear transformation
$$
L:V\stackrel{\rm linear}\longrightarrow W\, .
$$
Here we only know that $\dim V =n$ and $\dim W =m$. The rank of the map $L$ is the dimension of its image and also the number of linearly independent columns of $M$. Hence, this is sometimes called the \index{Rank!column rank}{\it column rank} of $M$. The dimension formula predicts  the dimension of the kernel, {\it i.e.} the nullity:  $ {\rm null}\, L= {\rm dim}V-{\rm rank}L=n-r$. 

To compute the kernel we would study the linear system $$Mx=0\, ,$$ which gives $m$ equations for the $n$-vector~$x$. The \index{Rank!row rank}{\it row rank} of a matrix is the number of linearly independent rows (viewed as vectors).
Each linearly independent row of $M$ gives an independent equation satisfied by the $n$-vector $x$. Every independent equation on $x$ reduces the size of the kernel by one, so if the row rank is $s$, then ${\rm null}\, L+ s = n$.  Thus we have two equations: 
$$
{\rm null}\, L+s=n \mbox{ and } {\rm null } \, L = n-r\, .
$$
From these we conclude the $r=s$. In other words, the row rank of $M$ equals its column rank.
 \end{example}



\section{Summary}\label{thelist}
We have seen that a linear transformation has an inverse if and only if it is bijective ({\it i.e.}, one-to-one and onto). We also know that linear transformations can be represented by matrices, and we have seen many ways to tell whether a matrix is invertible. Here is a list of them:
\begin{theorem}[Invertibility]
\label{theorem:invertibility}
Let $V$  be an $n$-dimensional vector space  
and suppose  $L:V\to V$ is a linear transformation with matrix $M$ in some basis.
Then \(M\) is an \(n \times n\) matrix, and 
%let $$L \colon \Re^n \to \Re^n$$ be the linear transformation defined by \(L(v)=Mv\). Then 
the following statements are equivalent:
\newpage
\begin{enumerate}
\item If $v$ is any vector in \(\Re^n\), then the system \(Mx=v\) has exactly one solution.
\item The matrix \(M\) is row-equivalent to the identity matrix.
\item If \(v\) is any vector in \(V\), then \(L(x)=v\) has exactly one solution.
\item The matrix \(M\) is invertible.
\item The homogeneous system \(Mx=0\) has no non-zero solutions.
\item The determinant of \(M\) is not equal to \(0\).
%\item The matrix \(M\) is a product of elementary matrices of the form \(E_j^i, R^i(\lambda), S_j^i(\gamma)\) with \(\lambda \neq 0\).
\item The transpose matrix \(M^T\) is invertible.
\item The matrix \(M\) does not have \(0\) as an eigenvalue.
\item The linear transformation \(L\) does not have \(0\) as an eigenvalue.
\item The characteristic polynomial \(\det(\lambda I-M)\) does not have \(0\) as a root.
\item The columns (or rows) of \(M\) span \(\Re^n\).
\item The columns (or rows) of \(M\) are linearly independent.
\item The columns (or rows) of \(M\) are a basis for \(\Re^n\).
\item The linear transformation \(L\) is injective.
\item The linear transformation \(L\) is surjective.
\item The linear transformation \(L\) is bijective.
\end{enumerate}
\end{theorem}
Note: it is important that \(M\) be an \(n \times n\) matrix! If \(M\) is not square, then it can't be invertible, and many of the statements above are no longer equivalent to each other.
%I hate it when books do this. "Here is our proof: you prove it or figure out where it has been proven. " I think it is dishonest to write proof: and then not actually give a proof.
%Yes, but some books make proving the above list there raison d'être which I really don't like...plus we have a video handling this!!!!
\begin{proof}
Many of these equivalences were proved earlier in other chapters. Some were left as review questions or sample final questions. The rest are left as exercises for the reader.
\end{proof}

\Videoscriptlink{kernel_range_nullity_rank_inv_cond.mp4}{Invertibility Conditions}{scripts_kernel_range_nullity_rank_inv_cond}

%\section*{References}
%Hefferon, Chapter Three, Section II.2: Rangespace and Nullspace (Recall that ``homomorphism'' is is used instead of ``linear transformation'' in Hefferon.)
%\\
%Beezer, Chapter LT, Sections ILT-IVLT
%\\
%Wikipedia:
%\begin{itemize}
%\item \href{http://en.wikipedia.org/wiki/Rank_(linear_algebra)}{Rank}
%\item \href{http://en.wikipedia.org/wiki/Dimension_theorem}{Dimension Theorem}
%\item \href{http://en.wikipedia.org/wiki/Kernel_(linear_operator)}{Kernel of a Linear Operator}
%\end{itemize}

\section{Review Problems}

{\bf Webwork:} 
\begin{tabular}{|c|c|}
\hline
Reading Problems & 
 \hwrref{KernelRangeRankNullity}{1}, 
 \hwrref{KernelRangeRankNullity}{2}, 
 \\
Elements of kernel &  \hwref{KernelRangeRankNullity}{3}\\
Basis for column space &\hwref{KernelRangeRankNullity}{4}\\
Basis for kernel & \hwref{KernelRangeRankNullity}{5}\\
Basis for kernel and range& \hwref{KernelRangeRankNullity}{6}\\
Orthonomal range basis&\hwref{KernelRangeRankNullity}{7}\\
Orthonomal kernel basis&\hwref{KernelRangeRankNullity}{8}\\
Orthonomal kernel and range bases&\hwref{KernelRangeRankNullity}{9}\\
Orthonomal kernel,  range and row space bases&\hwref{KernelRangeRankNullity}{10}\\
Rank&\hwref{KernelRangeRankNullity}{11}\\
   \hline
\end{tabular}


\input{\kernelPath/problems}

\newpage
