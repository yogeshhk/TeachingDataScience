{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t20b0l3A99b_",
    "outputId": "776994c6-4cb9-4a85-c7e0-d85334038160"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for PyStemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq fastembed==0.3.6 --progress-bar off\n",
    "!pip install -qqq sqlite-vec==0.1.2 --progress-bar off\n",
    "!pip install -qqq groq==0.11.0 --progress-bar off\n",
    "!pip install -qqq langchain-text-splitters==0.3.0 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DBN144k-_89Z"
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from textwrap import dedent\n",
    "from typing import List\n",
    "\n",
    "import sqlite_vec\n",
    "from fastembed import TextEmbedding\n",
    "from google.colab import userdata\n",
    "from groq import Groq\n",
    "from groq.types.chat import ChatCompletionMessage\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sqlite_vec import serialize_float32\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = Groq(api_key=userdata.get(\"GROQ_API_KEY\"))\n",
    "MODEL = \"llama-3.1-70b-versatile\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "db = sqlite3.connect(\"readmes.sqlite3\")\n",
    "db.enable_load_extension(True)\n",
    "sqlite_vec.load(db)\n",
    "db.enable_load_extension(False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext sql\n",
    "%sql sqlite:///readmes.sqlite3"
   ],
   "metadata": {
    "id": "t_ZWIjFHz6KO"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LbENU-yW9Pi7",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "qwen_doc = \"\"\"\n",
    "Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large language models have been upgraded to Qwen2.5. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.\n",
    "Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n",
    "\n",
    "The latest version, Qwen2.5, has the following features:\n",
    "\n",
    "- Dense, easy-to-use, decoder-only language models, available in **0.5B**, **1.5B**, **3B**, **7B**, **14B**, **32B**, and **72B** sizes, and base and instruct variants.\n",
    "- Pretrained on our latest large-scale dataset, encompassing up to **18T** tokens.\n",
    "- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n",
    "- More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
    "- Context length support up to **128K** tokens and can generate up to **8K** tokens.\n",
    "- Multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.\n",
    "\n",
    "The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n",
    "\n",
    "# Quickstart\n",
    "\n",
    "This guide helps you quickly start using Qwen2.5.\n",
    "We provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\n",
    "\n",
    "You can find Qwen2.5 models in the [Qwen2.5 collection](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) at Hugging Face Hub.\n",
    "\n",
    "**This repo contains the instruction-tuned 32B Qwen2.5 model**, which has the following features:\n",
    "- Type: Causal Language Models\n",
    "- Training Stage: Pretraining & Post-training\n",
    "- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n",
    "- Number of Parameters: 32.5B\n",
    "- Number of Paramaters (Non-Embedding): 31.0B\n",
    "- Number of Layers: 64\n",
    "- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n",
    "- Context Length: Full 131,072 tokens and generation 8192 tokens\n",
    "\n",
    "## Hugging Face Transformers & ModelScope\n",
    "\n",
    "To get a quick start with Qwen2.5, we advise you to try with the inference with `transformers` first.\n",
    "Make sure that you have installed `transformers>=4.37.0`.\n",
    "We advise you to use Python 3.10 or higher, and PyTorch 2.3 or higher.\n",
    "\n",
    "* Install with `pip`:\n",
    "\n",
    "```bash\n",
    "pip install transformers -U\n",
    "```\n",
    "\n",
    "* Install with `conda`:\n",
    "\n",
    "```bash\n",
    "conda install conda-forge::transformers\n",
    "```\n",
    "\n",
    "* Install from source:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/huggingface/transformers\n",
    "```\n",
    "\n",
    "The following is a very simple code snippet showing how to run Qwen2.5-7B-Instruct:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "```\n",
    "\n",
    "As you can see, it's just standard usage for casual LMs in `transformers`!\n",
    "\n",
    "### Streaming Generation\n",
    "\n",
    "Streaming mode for model chat is simple with the help of `TextStreamer`.\n",
    "Below we show you an example of how to use it:\n",
    "\n",
    "```python\n",
    "...\n",
    "# Reuse the code before `model.generate()` in the last code snippet\n",
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    "    streamer=streamer,\n",
    ")\n",
    "```\n",
    "\n",
    "It will print the text to the console or the terminal as being generated.\n",
    "\n",
    "### ModelScope\n",
    "\n",
    "To tackle with downloading issues, we advise you to try [ModelScope](https://github.com/modelscope/modelscope).\n",
    "Before starting, you need to install `modelscope` with `pip`.\n",
    "\n",
    "`modelscope` adopts a programmatic interface similar (but not identical) to `transformers`.\n",
    "For basic usage, you can simply change the first line of code above to the following:\n",
    "\n",
    "```python\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "```\n",
    "\n",
    "For more information, please refer to [the documentation of `modelscope`](https://www.modelscope.cn/docs).\n",
    "\n",
    "## Processing Long Texts\n",
    "\n",
    "The current `config.json` is set for context length up to 32,768 tokens.\n",
    "To handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n",
    "\n",
    "For supported frameworks, you could add the following to `config.json` to enable YaRN:\n",
    "```json\n",
    "{\n",
    "  ...,\n",
    "  \"rope_scaling\": {\n",
    "    \"factor\": 4.0,\n",
    "    \"original_max_position_embeddings\": 32768,\n",
    "    \"type\": \"yarn\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## vLLM for Deployment\n",
    "\n",
    "To deploy Qwen2.5, we advise you to use vLLM.\n",
    "vLLM is a fast and easy-to-use framework for LLM inference and serving.\n",
    "In the following, we demonstrate how to build a OpenAI-API compatible API service with vLLM.\n",
    "\n",
    "First, make sure you have installed `vllm>=0.4.0`:\n",
    "\n",
    "```bash\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "Run the following code to build up a vLLM service.\n",
    "Here we take Qwen2.5-7B-Instruct as an example:\n",
    "\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct\n",
    "```\n",
    "\n",
    "with `vllm>=0.5.3`, you can also use\n",
    "\n",
    "```bash\n",
    "vllm serve Qwen/Qwen2.5-7B-Instruct\n",
    "```\n",
    "\n",
    "Then, you can use the [create chat interface](https://platform.openai.com/docs/api-reference/chat/completions/create) to communicate with Qwen:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n",
    "  \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.8,\n",
    "  \"repetition_penalty\": 1.05,\n",
    "  \"max_tokens\": 512\n",
    "}'\n",
    "```\n",
    "\n",
    "or you can use Python client with `openai` Python package as shown below:\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "# Set OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")\n",
    "\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=512,\n",
    "    extra_body={\n",
    "        \"repetition_penalty\": 1.05,\n",
    "    },\n",
    ")\n",
    "print(\"Chat response:\", chat_response)\n",
    "```\n",
    "\n",
    "For more information, please refer to [the documentation of `vllm`](https://docs.vllm.ai/en/stable/).\n",
    "\n",
    "Now, you can have fun with Qwen2.5 models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "40S_3DUWLql9",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "langgraph_doc = \"\"\"\n",
    "# LangGraph\n",
    "\n",
    "⚡ Building language agents as graphs ⚡\n",
    "\n",
    "> [!NOTE]\n",
    "> Looking for the JS version? Click [here](https://github.com/langchain-ai/langgraphjs) ([JS docs](https://langchain-ai.github.io/langgraphjs/)).\n",
    "\n",
    "## Overview\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures, differentiating it from DAG-based solutions. As a very low-level framework, it provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents. Additionally, LangGraph includes built-in persistence, enabling advanced human-in-the-loop and memory features.\n",
    "\n",
    "LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\n",
    "\n",
    "To learn more about LangGraph, check out our first LangChain Academy course, *Introduction to LangGraph*, available for free [here](https://academy.langchain.com/courses/intro-to-langgraph).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Cycles and Branching**: Implement loops and conditionals in your apps.\n",
    "- **Persistence**: Automatically save state after each step in the graph. Pause and resume the graph execution at any point to support error recovery, human-in-the-loop workflows, time travel and more.\n",
    "- **Human-in-the-Loop**: Interrupt graph execution to approve or edit next action planned by the agent.\n",
    "- **Streaming Support**: Stream outputs as they are produced by each node (including token streaming).\n",
    "- **Integration with LangChain**: LangGraph integrates seamlessly with [LangChain](https://github.com/langchain-ai/langchain/) and [LangSmith](https://docs.smith.langchain.com/) (but does not require them).\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "```shell\n",
    "pip install -U langgraph\n",
    "```\n",
    "\n",
    "## Example\n",
    "\n",
    "One of the central concepts of LangGraph is state. Each graph execution creates a state that is passed between nodes in the graph as they execute, and each node updates this internal state with its return value after it executes. The way that the graph updates its internal state is defined by either the type of graph chosen or a custom function.\n",
    "\n",
    "Let's take a look at a simple example of an agent that can use a search tool.\n",
    "\n",
    "```shell\n",
    "pip install langchain-anthropic\n",
    "```\n",
    "\n",
    "```shell\n",
    "export ANTHROPIC_API_KEY=sk-...\n",
    "```\n",
    "\n",
    "Optionally, we can set up [LangSmith](https://docs.smith.langchain.com/) for best-in-class observability.\n",
    "\n",
    "```shell\n",
    "export LANGSMITH_TRACING=true\n",
    "export LANGSMITH_API_KEY=lsv2_sk_...\n",
    "```\n",
    "\n",
    "```python\n",
    "from typing import Annotated, Literal, TypedDict\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "# Define the tools for the agent to use\n",
    "@tool\n",
    "def search(query: str):\n",
    "    # Call to surf the web\n",
    "    # This is a placeholder, but don't tell the LLM that...\n",
    "    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=0).bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", 'agent')\n",
    "\n",
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable.\n",
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Use the Runnable\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content\n",
    "```\n",
    "\n",
    "```\n",
    "\"Based on the search results, I can tell you that the current weather in San Francisco is:\\n\\nTemperature: 60 degrees Fahrenheit\\nConditions: Foggy\\n\\nSan Francisco is known for its microclimates and frequent fog, especially during the summer months. The temperature of 60°F (about 15.5°C) is quite typical for the city, which tends to have mild temperatures year-round. The fog, often referred to as \"Karl the Fog\" by locals, is a characteristic feature of San Francisco\\'s weather, particularly in the mornings and evenings.\\n\\nIs there anything else you\\'d like to know about the weather in San Francisco or any other location?\"\n",
    "```\n",
    "\n",
    "Now when we pass the same `\"thread_id\"`, the conversation context is retained via the saved state (i.e. stored list of messages)\n",
    "\n",
    "```python\n",
    "final_state = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"what about ny\")]},\n",
    "    config={\"configurable\": {\"thread_id\": 42}}\n",
    ")\n",
    "final_state[\"messages\"][-1].content\n",
    "```\n",
    "\n",
    "```\n",
    "\"Based on the search results, I can tell you that the current weather in New York City is:\\n\\nTemperature: 90 degrees Fahrenheit (approximately 32.2 degrees Celsius)\\nConditions: Sunny\\n\\nThis weather is quite different from what we just saw in San Francisco. New York is experiencing much warmer temperatures right now. Here are a few points to note:\\n\\n1. The temperature of 90°F is quite hot, typical of summer weather in New York City.\\n2. The sunny conditions suggest clear skies, which is great for outdoor activities but also means it might feel even hotter due to direct sunlight.\\n3. This kind of weather in New York often comes with high humidity, which can make it feel even warmer than the actual temperature suggests.\\n\\nIt's interesting to see the stark contrast between San Francisco's mild, foggy weather and New York's hot, sunny conditions. This difference illustrates how varied weather can be across different parts of the United States, even on the same day.\\n\\nIs there anything else you'd like to know about the weather in New York or any other location?\"\n",
    "```\n",
    "\n",
    "### Step-by-step Breakdown\n",
    "\n",
    "1. <details>\n",
    "    <summary>Initialize the model and tools.</summary>\n",
    "\n",
    "    - we use `ChatAnthropic` as our LLM. **NOTE:** we need make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI tool calling using the `.bind_tools()` method.\n",
    "    - we define the tools we want to use - a search tool in our case. It is really easy to create your own tools - see documentation here on how to do that [here](https://python.langchain.com/docs/modules/agents/tools/custom_tools).\n",
    "   </details>\n",
    "\n",
    "2. <details>\n",
    "    <summary>Initialize graph with state.</summary>\n",
    "\n",
    "    - we initialize graph (`StateGraph`) by passing state schema (in our case `MessagesState`)\n",
    "    - `MessagesState` is a prebuilt state schema that has one attribute -- a list of LangChain `Message` objects, as well as logic for merging the updates from each node into the state\n",
    "   </details>\n",
    "\n",
    "3. <details>\n",
    "    <summary>Define graph nodes.</summary>\n",
    "\n",
    "    There are two main nodes we need:\n",
    "\n",
    "      - The `agent` node: responsible for deciding what (if any) actions to take.\n",
    "      - The `tools` node that invokes tools: if the agent decides to take an action, this node will then execute that action.\n",
    "   </details>\n",
    "\n",
    "4. <details>\n",
    "    <summary>Define entry point and graph edges.</summary>\n",
    "\n",
    "      First, we need to set the entry point for graph execution - `agent` node.\n",
    "\n",
    "      Then we define one normal and one conditional edge. Conditional edge means that the destination depends on the contents of the graph's state (`MessageState`). In our case, the destination is not known until the agent (LLM) decides.\n",
    "\n",
    "      - Conditional edge: after the agent is called, we should either:\n",
    "        - a. Run tools if the agent said to take an action, OR\n",
    "        - b. Finish (respond to the user) if the agent did not ask to run tools\n",
    "      - Normal edge: after the tools are invoked, the graph should always return to the agent to decide what to do next\n",
    "   </details>\n",
    "\n",
    "5. <details>\n",
    "    <summary>Compile the graph.</summary>\n",
    "\n",
    "    - When we compile the graph, we turn it into a LangChain [Runnable](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface), which automatically enables calling `.invoke()`, `.stream()` and `.batch()` with your inputs\n",
    "    - We can also optionally pass checkpointer object for persisting state between graph runs, and enabling memory, human-in-the-loop workflows, time travel and more. In our case we use `MemorySaver` - a simple in-memory checkpointer\n",
    "    </details>\n",
    "\n",
    "6. <details>\n",
    "   <summary>Execute the graph.</summary>\n",
    "\n",
    "    1. LangGraph adds the input message to the internal state, then passes the state to the entrypoint node, `\"agent\"`.\n",
    "    2. The `\"agent\"` node executes, invoking the chat model.\n",
    "    3. The chat model returns an `AIMessage`. LangGraph adds this to the state.\n",
    "    4. Graph cycles the following steps until there are no more `tool_calls` on `AIMessage`:\n",
    "\n",
    "        - If `AIMessage` has `tool_calls`, `\"tools\"` node executes\n",
    "        - The `\"agent\"` node executes again and returns `AIMessage`\n",
    "\n",
    "    5. Execution progresses to the special `END` value and outputs the final state.\n",
    "    And as a result, we get a list of all our chat messages as output.\n",
    "   </details>\n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "* [Tutorials](https://langchain-ai.github.io/langgraph/tutorials/): Learn to build with LangGraph through guided examples.\n",
    "* [How-to Guides](https://langchain-ai.github.io/langgraph/how-tos/): Accomplish specific things within LangGraph, from streaming, to adding memory & persistence, to common design patterns (branching, subgraphs, etc.), these are the place to go if you want to copy and run a specific code snippet.\n",
    "* [Conceptual Guides](https://langchain-ai.github.io/langgraph/concepts/high_level/): In-depth explanations of the key concepts and principles behind LangGraph, such as nodes, edges, state and more.\n",
    "* [API Reference](https://langchain-ai.github.io/langgraph/reference/graphs/): Review important classes and methods, simple examples of how to use the graph and checkpointing APIs, higher-level prebuilt components and more.\n",
    "* [Cloud (beta)](https://langchain-ai.github.io/langgraph/cloud/): With one click, deploy LangGraph applications to LangGraph Cloud.\n",
    "\n",
    "## Contributing\n",
    "\n",
    "For more information on how to contribute, see [here](https://github.com/langchain-ai/langgraph/blob/main/CONTRIBUTING.md).\n",
    "\n",
    "## Why LangGraph?\n",
    "\n",
    "LLMs are extremely powerful, particularly when connected to other systems such as a retriever or APIs. This is why many LLM applications use a control flow of steps before and / or after LLM calls. As an example [RAG](https://github.com/langchain-ai/rag-from-scratch) performs retrieval of relevant documents to a question, and passes those documents to an LLM in order to ground the response. Often a control flow of steps before and / or after an LLM is called a \"chain.\" Chains are a popular paradigm for programming with LLMs and offer a high degree of reliability; the same set of steps runs with each chain invocation.\n",
    "\n",
    "However, we often want LLM systems that can pick their own control flow! This is one definition of an [agent](https://blog.langchain.dev/what-is-an-agent/): an agent is a system that uses an LLM to decide the control flow of an application. Unlike a chain, an agent given an LLM some degree of control over the sequence of steps in the application. Examples of using an LLM to decide the control of an application:\n",
    "\n",
    "- Using an LLM to route between two potential paths\n",
    "- Using an LLM to decide which of many tools to call\n",
    "- Using an LLM to decide whether the generated answer is sufficient or more work is need\n",
    "\n",
    "There are many different types of [agent architectures](https://blog.langchain.dev/what-is-a-cognitive-architecture/) to consider, which given an LLM varying levels of control. On one extreme, a router allows an LLM to select a single step from a specified set of options and, on the other extreme, a fully autonomous long-running agent may have complete freedom to select any sequence of steps that it wants for a given problem.\n",
    "\n",
    "![Agent Types](img/agent_types.png)\n",
    "\n",
    "Several concepts are utilized in many agent architectures:\n",
    "\n",
    "- [Tool calling](agentic_concepts.md#tool-calling): this is often how LLMs make decisions\n",
    "- Action taking: often times, the LLMs' outputs are used as the input to an action\n",
    "- [Memory](agentic_concepts.md#memory): reliable systems need to have knowledge of things that occurred\n",
    "- [Planning](agentic_concepts.md#planning): planning steps (either explicit or implicit) are useful for ensuring that the LLM, when making decisions, makes them in the highest fidelity way.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "In practice, there is often a trade-off between control and reliability. As we give LLMs more control, the application often become less reliable. This can be due to factors such as LLM non-determinism and / or errors in selecting tools (or steps) that the agent uses (takes).\n",
    "\n",
    "![Agent Challenge](img/challenge.png)\n",
    "\n",
    "## Core Principles\n",
    "\n",
    "The motivation of LangGraph is to help bend the curve, preserving higher reliability as we give the agent more control over the application. We'll outline a few specific pillars of LangGraph that make it well suited for building reliable agents.\n",
    "\n",
    "![Langgraph](img/langgraph.png)\n",
    "\n",
    "**Controllability**\n",
    "\n",
    "LangGraph gives the developer a high degree of [control](../how-tos/index.md#controllability) by expressing the flow of the application as a set of nodes and edges. All nodes can access and modify a common state (memory). The control flow of the application can set using edges that connect nodes, either deterministically or via conditional logic.\n",
    "\n",
    "**Persistence**\n",
    "\n",
    "LangGraph gives the developer many options for [persisting](../how-tos/index.md#persistence) graph state using short-term or long-term (e.g., via a database) memory.\n",
    "\n",
    "**Human-in-the-Loop**\n",
    "\n",
    "The persistence layer enables several different [human-in-the-loop](../how-tos/index.md#human-in-the-loop) interaction patterns with agents; for example, it's possible to pause an agent, review its state, edit it state, and approve a follow-up step.\n",
    "\n",
    "**Streaming**\n",
    "\n",
    "LangGraph comes with first class support for [streaming](../how-tos/index.md#streaming), which can expose state to the user (or developer) over the course of agent execution. LangGraph supports streaming of both events ([like a tool call being taken](../how-tos/stream-updates.ipynb)) as well as of [tokens that an LLM may emit](../how-tos/streaming-tokens.ipynb).\n",
    "\n",
    "## Debugging\n",
    "\n",
    "Once you've built a graph, you often want to test and debug it. [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio?tab=readme-ov-file) is a specialized IDE for visualization and debugging of LangGraph applications.\n",
    "\n",
    "![Langgraph Studio](img/lg_studio.png)\n",
    "\n",
    "## Deployment\n",
    "\n",
    "Once you have confidence in your LangGraph application, many developers want an easy path to deployment. [LangGraph Cloud](../cloud/index.md) is an opinionated, simple way to deploy LangGraph objects from the LangChain team. Of course, you can also use services like [FastAPI](https://fastapi.tiangolo.com/) and call your graph from inside the FastAPI server as you see fit.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the Database"
   ],
   "metadata": {
    "id": "uJJpYlIKYnOV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "db.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE documents(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    text TEXT\n",
    ");\n",
    "\"\"\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCo5joOeYppn",
    "outputId": "af79e456-a512-4a7d-8dd9-ac02b8f48874"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x795c51b7f540>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "db.execute(\n",
    "    \"\"\"\n",
    "CREATE TABLE chunks(\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    document_id INTEGER,\n",
    "    text TEXT,\n",
    "    FOREIGN KEY(document_id) REFERENCES documents(id)\n",
    ");\n",
    "\"\"\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7vkbYpEYmlI",
    "outputId": "e2614ec9-cacd-456c-b088-61ee88535fbb"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x795c51c0c7c0>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_model = TextEmbedding()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "0073dca56c4c43d58beb1fbbcdeb7323",
      "1641af62ba864d34a02cf901dd463c2f",
      "f61dd38d853844998745c6ff51b59fbc",
      "1333d8429ed24f36aeb701bf1f44549a",
      "593e81acb81c4e3db40ac26bbe670af3",
      "1695988fc95441ce85d24e0160c1d9fa",
      "f47b3230b15c4c1e8cf897ab736ad2cd",
      "d366f33a674a44e49dcbf32c75dacdd9",
      "cf6e7dbe0760444b804af90de5c5c88a",
      "5962c71edb87493fa2a91b481e3119f6",
      "0257c98be9d74b90b70336c3dababdc8",
      "2198101a845047118dfd59ee100154a4",
      "312b70fa6ac34924a2c6e05ad8003820",
      "e3be7a7cd31442a7b63e207649ce1973",
      "79d1a82b8b0246a78268d5792e886a63",
      "b630548d76364f8ea81020dbe2be2538",
      "b2203b51385c433391a3571617989796",
      "0e137eee3bea4ed3b5869530ad45a9be",
      "e63f7112756a4b689298f2e8fcfd8bb8",
      "0779ecc7cbc44ca19685f5c015911eae",
      "ef441eaeb6444c50b528e4dc902c64ae",
      "d2e78934decc408da04ae5b8364087e6",
      "604e1353b11848818fa5e2b3aa2a4f61",
      "832fd2cd55864d68955ab2fc96cd48cf",
      "599ceb0e86254efaa5533c685b0bb820",
      "fb7ff48a04774984896b7d6bc19891ea",
      "f12a1713950540479a00c1c47586f5cd",
      "724ddddb67c74dd782fc3082f77a1ce0",
      "f02d9035d15d4a71bb83220946331c00",
      "071e7894d4234433bf2b8f7c95baa4fc",
      "9465e90598a24f378b8f685d39b3fd4c",
      "f2692b690a564ec99f9ead2bf712acef",
      "53fac0c2b3744dc5bf8269045df9758b",
      "b71a63c1e3b3481396a97fd43cf1edf2",
      "622fde9be6434b7ba29d5a574f439013",
      "382d1e24b4774680bfc3ff7d1c29376b",
      "0b160d4e2bf54332aa68da8e51dbd87e",
      "f84e17628c01450496fac38787aaa86d",
      "5c74260ee8aa46afa0dc878f5b9f8763",
      "2a57ae16ae1f4547838ee3dca4c8f59c",
      "cafb651d7f584bb7ac3aed898a9de32e",
      "f401eea1d90048f8b497fd7e5cc310c1",
      "c8c5253b36884c80badfaaa267164d03",
      "1146155d12f24d4fa9e29133e4e534fa",
      "9c67248dd2424e9fa17c1c34a0521c2a",
      "93122e1e56ef45cf8d763c4c9502138a",
      "636178d5cc48461f9f4e8fb641f06f25",
      "9b1207c2c9124c63b70818b12248e563",
      "153f3dbf3ecd4d1d82d06b9af86ba4c2",
      "5bc73836c151423d978e771e75b6d8c0",
      "0738c07314a64231abb49a1e6a0dd350",
      "40edf4dbaf804802ba5be7b67f26b159",
      "2dc667e18c034340ba68cb8afac85f7c",
      "e9b48b26647b4a0a8c6c0581b086995c",
      "1097e43f2e0a4249aa7fb39a01ca3a07",
      "b80a2853cbc24de3ad6f8f1d2da14dd3",
      "fa2c97c08d7a4b05abf3413e29dec64b",
      "2e02ba91bc8741d0a504ae3e93128e08",
      "714d42a25b0f464ca6d261a9a7a341df",
      "9655ab59a91545baac1297792a1bdec8",
      "707c223b9bfa44a49cb15b88ea905230",
      "b7ab334df49c4f68b89b7c306b8b2918",
      "2d900c2a23984017b98f87f749a3bd5b",
      "84c58b0fdb2d4f26993c2235a08829a2",
      "deae40f420cb4616a64582b63001a4f6",
      "3a479e5672ef4cf396981179dca92bcb"
     ]
    },
    "id": "B5hq_5NL2FyZ",
    "outputId": "5e580ca9-2671-41aa-961b-2ac02aca294e"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0073dca56c4c43d58beb1fbbcdeb7323"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2198101a845047118dfd59ee100154a4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "604e1353b11848818fa5e2b3aa2a4f61"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71a63c1e3b3481396a97fd43cf1edf2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c67248dd2424e9fa17c1c34a0521c2a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b80a2853cbc24de3ad6f8f1d2da14dd3"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "documents = [qwen_doc, langgraph_doc]\n",
    "document_embeddings = list(embedding_model.embed(documents[0]))\n",
    "len(document_embeddings), document_embeddings[0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "or2Ziz9iZSNt",
    "outputId": "5f56c4e9-6973-45c5-f9ce-fc6c5fcbba52"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, (384,))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "db.execute(\n",
    "    f\"\"\"\n",
    "        CREATE VIRTUAL TABLE chunk_embeddings USING vec0(\n",
    "          id INTEGER PRIMARY KEY,\n",
    "          embedding FLOAT[{document_embeddings[0].shape[0]}],\n",
    "        );\n",
    "    \"\"\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJYVeV5yZPEn",
    "outputId": "05cc8ee3-489f-453b-f6c1-15a77a9983e1"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x795c501be140>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add Documents to DB"
   ],
   "metadata": {
    "id": "msStYKzlvr21"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with db:\n",
    "    for doc in documents:\n",
    "        db.execute(\"INSERT INTO documents(text) VALUES(?)\", [doc])"
   ],
   "metadata": {
    "id": "q0hEecWBx6ds"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    id,\n",
    "    substr(text, 1, 120)\n",
    "FROM\n",
    "    documents"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "spkixCn_zsaE",
    "outputId": "944b4135-869b-4be4-b359-a943ec42f14f"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * sqlite:///readmes.sqlite3\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(1, '\\nQwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large langu'),\n",
       " (2, '\\n# LangGraph\\n\\n⚡ Building language agents as graphs ⚡\\n\\n> [!NOTE]\\n> Looking for the JS version? Click [here](https://githu')]"
      ],
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>id</th>\n",
       "            <th>substr(text, 1, 120)</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td><br>Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large langu</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td><br># LangGraph<br><br>⚡ Building language agents as graphs ⚡<br><br>&gt; [!NOTE]<br>&gt; Looking for the JS version? Click [here](https://githu</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding Documents"
   ],
   "metadata": {
    "id": "4S780rMH2EK2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def call_model(prompt: str, messages=[]) -> ChatCompletionMessage:\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "metadata": {
    "id": "F94IKpH_DElO"
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=128)"
   ],
   "metadata": {
    "id": "cXhdEGd_6Vvx"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "CONTEXTUAL_EMBEDDING_PROMPT = \"\"\"\n",
    "Here is the chunk we want to situate within the whole document\n",
    "<chunk>\n",
    "{chunk}\n",
    "</chunk>\n",
    "\n",
    "Here is the content of the whole document\n",
    "<document>\n",
    "{document}\n",
    "</document>\n",
    "\n",
    "Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "Answer only with the succinct context and nothing else.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "3RcTrTw0-X1D"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_contextual_chunks(chunks: List[str], document: str) -> List[str]:\n",
    "    contextual_chunks = []\n",
    "    for chunk in chunks:\n",
    "        prompt = CONTEXTUAL_EMBEDDING_PROMPT.format(chunk=chunk, document=document)\n",
    "        chunk_context = call_model(prompt)\n",
    "        contextual_chunks.append(f\"{chunk}\\n{chunk_context}\")\n",
    "    return contextual_chunks"
   ],
   "metadata": {
    "id": "MwD2kOpOXepr"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def save_chunks(chunks: List[str]):\n",
    "    chunk_embeddings = list(embedding_model.embed(chunks))\n",
    "    for chunk, embedding in zip(chunks, chunk_embeddings):\n",
    "        result = db.execute(\n",
    "            \"INSERT INTO chunks(document_id, text) VALUES(?, ?)\", [doc_id, chunk]\n",
    "        )\n",
    "        chunk_id = result.lastrowid\n",
    "        db.execute(\n",
    "            \"INSERT INTO chunk_embeddings(id, embedding) VALUES (?, ?)\",\n",
    "            [chunk_id, serialize_float32(embedding)],\n",
    "        )"
   ],
   "metadata": {
    "id": "0JI5djfGyWFn"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with db:\n",
    "    document_rows = db.execute(\"SELECT id, text FROM documents\").fetchall()\n",
    "    row = document_rows[0]\n",
    "    doc_id, doc_text = row\n",
    "    chunks = text_splitter.split_text(doc_text)\n",
    "    print(len(chunks))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quvJm55t1-61",
    "outputId": "5ba14583-6a57-4a78-868b-58569afe52dd"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(chunks[5])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OM90LQR2KNW",
    "outputId": "bed1329c-ad3d-46c1-bf08-0b9ccf44e5cd"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For more information, please refer to [the documentation of `vllm`](https://docs.vllm.ai/en/stable/).\n",
      "\n",
      "Now, you can have fun with Qwen2.5 models.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "contextual_chunks = create_contextual_chunks([chunks[5]], doc_text)"
   ],
   "metadata": {
    "id": "TKAA0uov28El"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(contextual_chunks[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teGBaFCx3Ncg",
    "outputId": "a9ccc570-ffe0-4bf2-d51e-e8301d098f38"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For more information, please refer to [the documentation of `vllm`](https://docs.vllm.ai/en/stable/).\n",
      "\n",
      "Now, you can have fun with Qwen2.5 models.\n",
      "The chunk is situated at the end of the document, following the section on deploying Qwen2.5 models with vLLM, and serves as a concluding remark encouraging users to explore the capabilities of Qwen2.5 models.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "with db:\n",
    "    document_rows = db.execute(\"SELECT id, text FROM documents\").fetchall()\n",
    "    for row in document_rows:\n",
    "        doc_id, doc_text = row\n",
    "        chunks = text_splitter.split_text(doc_text)\n",
    "        contextual_chunks = create_contextual_chunks(chunks, doc_text)\n",
    "        save_chunks(contextual_chunks)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_y1CtxMGW1dj",
    "outputId": "3f5aa3a7-0970-44ba-d1f5-ef3a4ae4d1ff"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 11.5 s, sys: 273 ms, total: 11.8 s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find Similar Documents"
   ],
   "metadata": {
    "id": "yAuohEwh5nOE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"How many parameters does Qwen have?\"\n",
    "query_embeddings = list(embedding_model.embed([query]))\n",
    "query_embedding = query_embeddings[0]"
   ],
   "metadata": {
    "id": "koreX8RR5pCQ"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = db.execute(\n",
    "    \"\"\"\n",
    "      SELECT\n",
    "        chunk_embeddings.id,\n",
    "        distance,\n",
    "        text\n",
    "      FROM chunk_embeddings\n",
    "      LEFT JOIN chunks ON chunks.id = chunk_embeddings.id\n",
    "      WHERE embedding MATCH ?\n",
    "        AND k = 3\n",
    "      ORDER BY distance\n",
    "    \"\"\",\n",
    "    [serialize_float32(query_embedding)],\n",
    ").fetchall()"
   ],
   "metadata": {
    "id": "YdVQZWIo53Hg"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOHC48On6EX0",
    "outputId": "dc95a272-e1c5-4f13-a33a-5050395220b8"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(1,\n",
       "  0.689344584941864,\n",
       "  'Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large language models have been upgraded to Qwen2.5. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.\\nQwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\\n\\nThe latest version, Qwen2.5, has the following features:\\n\\n- Dense, easy-to-use, decoder-only language models, available in **0.5B**, **1.5B**, **3B**, **7B**, **14B**, **32B**, and **72B** sizes, and base and instruct variants.\\n- Pretrained on our latest large-scale dataset, encompassing up to **18T** tokens.\\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\\n- More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\\n- Context length support up to **128K** tokens and can generate up to **8K** tokens.\\n- Multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\\n\\n# Takeaways\\nOverview of Qwen2.5, a large language model series developed by the Qwen Team, Alibaba Group, highlighting its capabilities, features, and improvements.'),\n",
       " (3,\n",
       "  0.7151840925216675,\n",
       "  'You can find Qwen2.5 models in the [Qwen2.5 collection](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) at Hugging Face Hub.\\n\\n**This repo contains the instruction-tuned 32B Qwen2.5 model**, which has the following features:\\n- Type: Causal Language Models\\n- Training Stage: Pretraining & Post-training\\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\\n- Number of Parameters: 32.5B\\n- Number of Paramaters (Non-Embedding): 31.0B\\n- Number of Layers: 64\\n- Number of Attention Heads (GQA): 40 for Q and 8 for KV\\n- Context Length: Full 131,072 tokens and generation 8192 tokens\\n\\n## Hugging Face Transformers & ModelScope\\n\\nTo get a quick start with Qwen2.5, we advise you to try with the inference with `transformers` first.\\nMake sure that you have installed `transformers>=4.37.0`.\\nWe advise you to use Python 3.10 or higher, and PyTorch 2.3 or higher.\\n\\n* Install with `pip`:\\n\\n```bash\\npip install transformers -U\\n```\\n\\n* Install with `conda`:\\n\\n```bash\\nconda install conda-forge::transformers\\n```\\n\\n* Install from source:\\n\\n```bash\\npip install git+https://github.com/huggingface/transformers\\n```\\n\\nThe following is a very simple code snippet showing how to run Qwen2.5-7B-Instruct:\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"Give me a short introduction to large language model.\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\\n    {\"role\": \"user\", \"content\": prompt},\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True,\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\nThis chunk is part of the \"Quickstart\" section of the document, specifically focusing on using Hugging Face Transformers and ModelScope to get started with the Qwen2.5 model.'),\n",
       " (2,\n",
       "  0.7328726649284363,\n",
       "  '# Takeaways\\n\\nIn terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.\\n\\nThe specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\\n\\n# Quickstart\\n\\nThis guide helps you quickly start using Qwen2.5.\\nWe provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\\nThe chunk is situated in the introductory section of the document, immediately following the overview of the Qwen2.5 language model series and its features, and preceding the quickstart guide for using Qwen2.5.')]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrieve Context"
   ],
   "metadata": {
    "id": "b-ZFBCPz6ss4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def retrieve_context(\n",
    "    query: str, k: int = 3, embedding_model: TextEmbedding = embedding_model\n",
    ") -> str:\n",
    "    query_embedding = list(embedding_model.embed([query]))[0]\n",
    "    results = db.execute(\n",
    "        \"\"\"\n",
    "    SELECT\n",
    "        chunk_embeddings.id,\n",
    "        distance,\n",
    "        text\n",
    "    FROM chunk_embeddings\n",
    "    LEFT JOIN chunks ON chunks.id = chunk_embeddings.id\n",
    "    WHERE embedding MATCH ? AND k = ?\n",
    "    ORDER BY distance\n",
    "        \"\"\",\n",
    "        [serialize_float32(query_embedding), k],\n",
    "    ).fetchall()\n",
    "    return \"\\n-----\\n\".join([item[2] for item in results])"
   ],
   "metadata": {
    "id": "RAkEwLWr6tN5"
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(retrieve_context(query))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVicvN7k7AT4",
    "outputId": "fea9da00-6731-4a67-f875-b9cb9568a6c3"
   },
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large language models have been upgraded to Qwen2.5. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.\n",
      "Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n",
      "\n",
      "The latest version, Qwen2.5, has the following features:\n",
      "\n",
      "- Dense, easy-to-use, decoder-only language models, available in **0.5B**, **1.5B**, **3B**, **7B**, **14B**, **32B**, and **72B** sizes, and base and instruct variants.\n",
      "- Pretrained on our latest large-scale dataset, encompassing up to **18T** tokens.\n",
      "- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n",
      "- More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
      "- Context length support up to **128K** tokens and can generate up to **8K** tokens.\n",
      "- Multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
      "\n",
      "# Takeaways\n",
      "Overview of Qwen2.5, a large language model series developed by the Qwen Team, Alibaba Group, highlighting its capabilities, features, and improvements.\n",
      "-----\n",
      "You can find Qwen2.5 models in the [Qwen2.5 collection](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) at Hugging Face Hub.\n",
      "\n",
      "**This repo contains the instruction-tuned 32B Qwen2.5 model**, which has the following features:\n",
      "- Type: Causal Language Models\n",
      "- Training Stage: Pretraining & Post-training\n",
      "- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n",
      "- Number of Parameters: 32.5B\n",
      "- Number of Paramaters (Non-Embedding): 31.0B\n",
      "- Number of Layers: 64\n",
      "- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n",
      "- Context Length: Full 131,072 tokens and generation 8192 tokens\n",
      "\n",
      "## Hugging Face Transformers & ModelScope\n",
      "\n",
      "To get a quick start with Qwen2.5, we advise you to try with the inference with `transformers` first.\n",
      "Make sure that you have installed `transformers>=4.37.0`.\n",
      "We advise you to use Python 3.10 or higher, and PyTorch 2.3 or higher.\n",
      "\n",
      "* Install with `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers -U\n",
      "```\n",
      "\n",
      "* Install with `conda`:\n",
      "\n",
      "```bash\n",
      "conda install conda-forge::transformers\n",
      "```\n",
      "\n",
      "* Install from source:\n",
      "\n",
      "```bash\n",
      "pip install git+https://github.com/huggingface/transformers\n",
      "```\n",
      "\n",
      "The following is a very simple code snippet showing how to run Qwen2.5-7B-Instruct:\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_name,\n",
      "    torch_dtype=\"auto\",\n",
      "    device_map=\"auto\"\n",
      ")\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\n",
      "prompt = \"Give me a short introduction to large language model.\"\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
      "    {\"role\": \"user\", \"content\": prompt},\n",
      "]\n",
      "text = tokenizer.apply_chat_template(\n",
      "    messages,\n",
      "    tokenize=False,\n",
      "    add_generation_prompt=True,\n",
      ")\n",
      "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
      "This chunk is part of the \"Quickstart\" section of the document, specifically focusing on using Hugging Face Transformers and ModelScope to get started with the Qwen2.5 model.\n",
      "-----\n",
      "# Takeaways\n",
      "\n",
      "In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.\n",
      "\n",
      "The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n",
      "\n",
      "# Quickstart\n",
      "\n",
      "This guide helps you quickly start using Qwen2.5.\n",
      "We provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\n",
      "The chunk is situated in the introductory section of the document, immediately following the overview of the Qwen2.5 language model series and its features, and preceding the quickstart guide for using Qwen2.5.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ask Questions"
   ],
   "metadata": {
    "id": "Ko3ZzYeL_ZYJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You're an expert AI/ML engineer with a background in software development. You're\n",
    "answering questions about technical topics and projects.\n",
    "If you don't know the answer - just reply with an excuse that you\n",
    "don't know. Keep your answers brief and to the point. Be kind and respectful.\n",
    "\n",
    "Use the provided context for your answers. The most relevant information is\n",
    "at the top. Each piece of information is separated by ---.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "SbR8ERv27J7O"
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def ask_question(query: str) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "    ]\n",
    "    context = retrieve_context(query)\n",
    "    prompt = dedent(\n",
    "        f\"\"\"\n",
    "Use the following information:\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "to answer the question:\n",
    "{query}\n",
    "    \"\"\"\n",
    "    )\n",
    "    return call_model(prompt, messages), context"
   ],
   "metadata": {
    "id": "T1h-MEIM8oqr"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"How many parameters does Qwen have?\"\n",
    "response, context = ask_question(query)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ8OJ2MI8sK7",
    "outputId": "ba86e18c-a120-46aa-e185-eb585d131aa7"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Qwen2.5 models are available in various sizes, with the number of parameters ranging from 0.5B to 72B. The specific model mentioned in the text has 32.5B parameters, with 31.0B non-embedding parameters.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(context)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBKtPVG284Nc",
    "outputId": "8d15ba5b-a01a-47ae-a4da-a4bcdd5f94a4"
   },
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Now the large language models have been upgraded to Qwen2.5. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences.\n",
      "Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc.\n",
      "\n",
      "The latest version, Qwen2.5, has the following features:\n",
      "\n",
      "- Dense, easy-to-use, decoder-only language models, available in **0.5B**, **1.5B**, **3B**, **7B**, **14B**, **32B**, and **72B** sizes, and base and instruct variants.\n",
      "- Pretrained on our latest large-scale dataset, encompassing up to **18T** tokens.\n",
      "- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.\n",
      "- More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n",
      "- Context length support up to **128K** tokens and can generate up to **8K** tokens.\n",
      "- Multilingual support for over **29** languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.\n",
      "\n",
      "# Takeaways\n",
      "Overview of Qwen2.5, a large language model series developed by the Qwen Team, Alibaba Group, highlighting its capabilities, features, and improvements.\n",
      "-----\n",
      "You can find Qwen2.5 models in the [Qwen2.5 collection](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) at Hugging Face Hub.\n",
      "\n",
      "**This repo contains the instruction-tuned 32B Qwen2.5 model**, which has the following features:\n",
      "- Type: Causal Language Models\n",
      "- Training Stage: Pretraining & Post-training\n",
      "- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n",
      "- Number of Parameters: 32.5B\n",
      "- Number of Paramaters (Non-Embedding): 31.0B\n",
      "- Number of Layers: 64\n",
      "- Number of Attention Heads (GQA): 40 for Q and 8 for KV\n",
      "- Context Length: Full 131,072 tokens and generation 8192 tokens\n",
      "\n",
      "## Hugging Face Transformers & ModelScope\n",
      "\n",
      "To get a quick start with Qwen2.5, we advise you to try with the inference with `transformers` first.\n",
      "Make sure that you have installed `transformers>=4.37.0`.\n",
      "We advise you to use Python 3.10 or higher, and PyTorch 2.3 or higher.\n",
      "\n",
      "* Install with `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers -U\n",
      "```\n",
      "\n",
      "* Install with `conda`:\n",
      "\n",
      "```bash\n",
      "conda install conda-forge::transformers\n",
      "```\n",
      "\n",
      "* Install from source:\n",
      "\n",
      "```bash\n",
      "pip install git+https://github.com/huggingface/transformers\n",
      "```\n",
      "\n",
      "The following is a very simple code snippet showing how to run Qwen2.5-7B-Instruct:\n",
      "\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_name,\n",
      "    torch_dtype=\"auto\",\n",
      "    device_map=\"auto\"\n",
      ")\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\n",
      "prompt = \"Give me a short introduction to large language model.\"\n",
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
      "    {\"role\": \"user\", \"content\": prompt},\n",
      "]\n",
      "text = tokenizer.apply_chat_template(\n",
      "    messages,\n",
      "    tokenize=False,\n",
      "    add_generation_prompt=True,\n",
      ")\n",
      "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
      "This chunk is part of the \"Quickstart\" section of the document, specifically focusing on using Hugging Face Transformers and ModelScope to get started with the Qwen2.5 model.\n",
      "-----\n",
      "# Takeaways\n",
      "\n",
      "In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages.\n",
      "\n",
      "The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR).\n",
      "\n",
      "# Quickstart\n",
      "\n",
      "This guide helps you quickly start using Qwen2.5.\n",
      "We provide examples of [Hugging Face Transformers](https://github.com/huggingface/transformers) as well as [ModelScope](https://github.com/modelscope/modelscope), and [vLLM](https://github.com/vllm-project/vllm) for deployment.\n",
      "The chunk is situated in the introductory section of the document, immediately following the overview of the Qwen2.5 language model series and its features, and preceding the quickstart guide for using Qwen2.5.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"How should one deploy Qwen model on a private server?\"\n",
    "response, context = ask_question(query)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtpZJkGB-eSv",
    "outputId": "4f60d8c0-cb7e-4b64-aafc-23e32b8218b8"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "To deploy Qwen2.5 on a private server, you can use vLLM, a fast and easy-to-use framework for LLM inference and serving. First, install `vllm>=0.4.0` using pip. Then, run the following command to build up a vLLM service:\n",
      "\n",
      "```bash\n",
      "python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2.5-7B-Instruct\n",
      "```\n",
      "\n",
      "Alternatively, with `vllm>=0.5.3`, you can use:\n",
      "\n",
      "```bash\n",
      "vllm serve Qwen/Qwen2.5-7B-Instruct\n",
      "```\n",
      "\n",
      "This will start a service that you can interact with using the OpenAI API.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"I have a RTX 4090 (24GB). Which version of the model can I run with good inference speed?\"\n",
    "response, context = ask_question(query)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KEvyEuk-nMZ",
    "outputId": "837287ac-0553-4ccc-e153-449c85a4c9c4"
   },
   "execution_count": 59,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Based on the provided information, the model sizes available for Qwen2.5 are 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B. \n",
      "\n",
      "Considering your RTX 4090 has 24GB of memory, you can likely run the 7B or 14B models with good inference speed. However, the 14B model might be pushing the limits of your GPU's memory, so the 7B model would be a safer choice.\n",
      "\n",
      "Keep in mind that the actual performance will also depend on other factors such as your system's CPU, RAM, and the specific use case.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Can you use Qwen with LangGraph? How one could do that?\"\n",
    "response, context = ask_question(query)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5co0lLYc_NJA",
    "outputId": "f3e66cfd-4a43-49a2-b5e2-a7c44795c0ec"
   },
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yes, you can use Qwen with LangGraph. LangGraph is a library for building stateful, multi-actor applications with LLMs (Large Language Models), and Qwen is a large language model series. \n",
      "\n",
      "To use Qwen with LangGraph, you would need to integrate the Qwen model into your LangGraph application. This would involve using the Qwen model as the LLM component in your LangGraph workflow.\n",
      "\n",
      "Here's a high-level overview of the steps:\n",
      "\n",
      "1. Choose a Qwen model variant (e.g., Qwen2.5-1.5B) and load it into your application.\n",
      "2. Define a LangGraph workflow that incorporates the Qwen model as the LLM component.\n",
      "3. Use the LangGraph API to interact with the Qwen model and build your application.\n",
      "\n",
      "Note that the specific implementation details would depend on the LangGraph version and the Qwen model variant you choose. You may need to consult the LangGraph documentation and the Qwen documentation for more information on how to integrate the two.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"Can you use human input in a LangGraph app?\"\n",
    "response, context = ask_question(query)\n",
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aYP0KpXcBXsr",
    "outputId": "7408a1f2-36a1-4399-b9aa-a78f683e2fef"
   },
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yes, LangGraph supports human-in-the-loop interaction patterns, allowing you to pause an agent, review its state, edit its state, and approve a follow-up step.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"Can you use human input in a LangGraph app?\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7ZQ4nj3CULT",
    "outputId": "ec892273-ca85-4387-ef3d-e0ed92adb615"
   },
   "execution_count": 64,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yes, you can use human input in a LangGraph app. LangGraph is a visual programming tool that allows you to create interactive applications by combining visual elements and code. To incorporate human input into your LangGraph app, you can use various input elements, such as:\n",
      "\n",
      "1. **Text Input**: This element allows users to enter text, which can be used as input for your application.\n",
      "2. **Button**: You can use buttons to capture user clicks, which can trigger specific actions or events in your application.\n",
      "3. **Slider**: Sliders enable users to input numerical values within a specified range.\n",
      "4. **Dropdown**: Dropdown menus allow users to select from a list of predefined options.\n",
      "5. **Checkbox**: Checkboxes enable users to select or deselect specific options.\n",
      "\n",
      "To use human input in your LangGraph app, follow these general steps:\n",
      "\n",
      "1. **Add an input element**: Drag and drop the desired input element (e.g., Text Input, Button, Slider, etc.) onto your LangGraph canvas.\n",
      "2. **Configure the input element**: Set the properties of the input element, such as its label, default value, or range (for sliders).\n",
      "3. **Connect the input element to your application logic**: Use LangGraph's visual programming paradigm to connect the input element to your application's logic. This might involve creating a flowchart or using conditional statements to process the user input.\n",
      "4. **Use the input data**: Once the user provides input, you can use the data in your application logic to perform calculations, update visualizations, or trigger events.\n",
      "\n",
      "Here's an example of how you might use human input in a LangGraph app:\n",
      "\n",
      "Suppose you're building a simple calculator app that takes two numbers as input and displays their sum. You could use two Text Input elements to capture the user's input, and then connect these elements to a calculation node that adds the two numbers together. The result would be displayed in a Label element.\n",
      "\n",
      "By incorporating human input into your LangGraph app, you can create interactive and engaging applications that respond to user input.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## References\n",
    "\n",
    "- [Introducing Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval)\n",
    "- [Anthropic Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/skills/contextual-embeddings/guide.ipynb)"
   ],
   "metadata": {
    "id": "OlKhr-DD_ePA"
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0073dca56c4c43d58beb1fbbcdeb7323": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1641af62ba864d34a02cf901dd463c2f",
       "IPY_MODEL_f61dd38d853844998745c6ff51b59fbc",
       "IPY_MODEL_1333d8429ed24f36aeb701bf1f44549a"
      ],
      "layout": "IPY_MODEL_593e81acb81c4e3db40ac26bbe670af3"
     }
    },
    "1641af62ba864d34a02cf901dd463c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1695988fc95441ce85d24e0160c1d9fa",
      "placeholder": "​",
      "style": "IPY_MODEL_f47b3230b15c4c1e8cf897ab736ad2cd",
      "value": "Fetching 5 files: 100%"
     }
    },
    "f61dd38d853844998745c6ff51b59fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d366f33a674a44e49dcbf32c75dacdd9",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf6e7dbe0760444b804af90de5c5c88a",
      "value": 5
     }
    },
    "1333d8429ed24f36aeb701bf1f44549a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5962c71edb87493fa2a91b481e3119f6",
      "placeholder": "​",
      "style": "IPY_MODEL_0257c98be9d74b90b70336c3dababdc8",
      "value": " 5/5 [00:03&lt;00:00,  1.86s/it]"
     }
    },
    "593e81acb81c4e3db40ac26bbe670af3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1695988fc95441ce85d24e0160c1d9fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47b3230b15c4c1e8cf897ab736ad2cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d366f33a674a44e49dcbf32c75dacdd9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6e7dbe0760444b804af90de5c5c88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5962c71edb87493fa2a91b481e3119f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0257c98be9d74b90b70336c3dababdc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2198101a845047118dfd59ee100154a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_312b70fa6ac34924a2c6e05ad8003820",
       "IPY_MODEL_e3be7a7cd31442a7b63e207649ce1973",
       "IPY_MODEL_79d1a82b8b0246a78268d5792e886a63"
      ],
      "layout": "IPY_MODEL_b630548d76364f8ea81020dbe2be2538"
     }
    },
    "312b70fa6ac34924a2c6e05ad8003820": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2203b51385c433391a3571617989796",
      "placeholder": "​",
      "style": "IPY_MODEL_0e137eee3bea4ed3b5869530ad45a9be",
      "value": "config.json: 100%"
     }
    },
    "e3be7a7cd31442a7b63e207649ce1973": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e63f7112756a4b689298f2e8fcfd8bb8",
      "max": 706,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0779ecc7cbc44ca19685f5c015911eae",
      "value": 706
     }
    },
    "79d1a82b8b0246a78268d5792e886a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef441eaeb6444c50b528e4dc902c64ae",
      "placeholder": "​",
      "style": "IPY_MODEL_d2e78934decc408da04ae5b8364087e6",
      "value": " 706/706 [00:00&lt;00:00, 5.08kB/s]"
     }
    },
    "b630548d76364f8ea81020dbe2be2538": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2203b51385c433391a3571617989796": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e137eee3bea4ed3b5869530ad45a9be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e63f7112756a4b689298f2e8fcfd8bb8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0779ecc7cbc44ca19685f5c015911eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef441eaeb6444c50b528e4dc902c64ae": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2e78934decc408da04ae5b8364087e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "604e1353b11848818fa5e2b3aa2a4f61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_832fd2cd55864d68955ab2fc96cd48cf",
       "IPY_MODEL_599ceb0e86254efaa5533c685b0bb820",
       "IPY_MODEL_fb7ff48a04774984896b7d6bc19891ea"
      ],
      "layout": "IPY_MODEL_f12a1713950540479a00c1c47586f5cd"
     }
    },
    "832fd2cd55864d68955ab2fc96cd48cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_724ddddb67c74dd782fc3082f77a1ce0",
      "placeholder": "​",
      "style": "IPY_MODEL_f02d9035d15d4a71bb83220946331c00",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "599ceb0e86254efaa5533c685b0bb820": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_071e7894d4234433bf2b8f7c95baa4fc",
      "max": 1242,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9465e90598a24f378b8f685d39b3fd4c",
      "value": 1242
     }
    },
    "fb7ff48a04774984896b7d6bc19891ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2692b690a564ec99f9ead2bf712acef",
      "placeholder": "​",
      "style": "IPY_MODEL_53fac0c2b3744dc5bf8269045df9758b",
      "value": " 1.24k/1.24k [00:00&lt;00:00, 5.72kB/s]"
     }
    },
    "f12a1713950540479a00c1c47586f5cd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "724ddddb67c74dd782fc3082f77a1ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f02d9035d15d4a71bb83220946331c00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "071e7894d4234433bf2b8f7c95baa4fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9465e90598a24f378b8f685d39b3fd4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2692b690a564ec99f9ead2bf712acef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53fac0c2b3744dc5bf8269045df9758b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b71a63c1e3b3481396a97fd43cf1edf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_622fde9be6434b7ba29d5a574f439013",
       "IPY_MODEL_382d1e24b4774680bfc3ff7d1c29376b",
       "IPY_MODEL_0b160d4e2bf54332aa68da8e51dbd87e"
      ],
      "layout": "IPY_MODEL_f84e17628c01450496fac38787aaa86d"
     }
    },
    "622fde9be6434b7ba29d5a574f439013": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c74260ee8aa46afa0dc878f5b9f8763",
      "placeholder": "​",
      "style": "IPY_MODEL_2a57ae16ae1f4547838ee3dca4c8f59c",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "382d1e24b4774680bfc3ff7d1c29376b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cafb651d7f584bb7ac3aed898a9de32e",
      "max": 695,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f401eea1d90048f8b497fd7e5cc310c1",
      "value": 695
     }
    },
    "0b160d4e2bf54332aa68da8e51dbd87e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8c5253b36884c80badfaaa267164d03",
      "placeholder": "​",
      "style": "IPY_MODEL_1146155d12f24d4fa9e29133e4e534fa",
      "value": " 695/695 [00:00&lt;00:00, 3.64kB/s]"
     }
    },
    "f84e17628c01450496fac38787aaa86d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c74260ee8aa46afa0dc878f5b9f8763": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a57ae16ae1f4547838ee3dca4c8f59c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cafb651d7f584bb7ac3aed898a9de32e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f401eea1d90048f8b497fd7e5cc310c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8c5253b36884c80badfaaa267164d03": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1146155d12f24d4fa9e29133e4e534fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c67248dd2424e9fa17c1c34a0521c2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93122e1e56ef45cf8d763c4c9502138a",
       "IPY_MODEL_636178d5cc48461f9f4e8fb641f06f25",
       "IPY_MODEL_9b1207c2c9124c63b70818b12248e563"
      ],
      "layout": "IPY_MODEL_153f3dbf3ecd4d1d82d06b9af86ba4c2"
     }
    },
    "93122e1e56ef45cf8d763c4c9502138a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5bc73836c151423d978e771e75b6d8c0",
      "placeholder": "​",
      "style": "IPY_MODEL_0738c07314a64231abb49a1e6a0dd350",
      "value": "tokenizer.json: 100%"
     }
    },
    "636178d5cc48461f9f4e8fb641f06f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40edf4dbaf804802ba5be7b67f26b159",
      "max": 711396,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2dc667e18c034340ba68cb8afac85f7c",
      "value": 711396
     }
    },
    "9b1207c2c9124c63b70818b12248e563": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9b48b26647b4a0a8c6c0581b086995c",
      "placeholder": "​",
      "style": "IPY_MODEL_1097e43f2e0a4249aa7fb39a01ca3a07",
      "value": " 711k/711k [00:00&lt;00:00, 2.23MB/s]"
     }
    },
    "153f3dbf3ecd4d1d82d06b9af86ba4c2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5bc73836c151423d978e771e75b6d8c0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0738c07314a64231abb49a1e6a0dd350": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40edf4dbaf804802ba5be7b67f26b159": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2dc667e18c034340ba68cb8afac85f7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9b48b26647b4a0a8c6c0581b086995c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1097e43f2e0a4249aa7fb39a01ca3a07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b80a2853cbc24de3ad6f8f1d2da14dd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fa2c97c08d7a4b05abf3413e29dec64b",
       "IPY_MODEL_2e02ba91bc8741d0a504ae3e93128e08",
       "IPY_MODEL_714d42a25b0f464ca6d261a9a7a341df"
      ],
      "layout": "IPY_MODEL_9655ab59a91545baac1297792a1bdec8"
     }
    },
    "fa2c97c08d7a4b05abf3413e29dec64b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_707c223b9bfa44a49cb15b88ea905230",
      "placeholder": "​",
      "style": "IPY_MODEL_b7ab334df49c4f68b89b7c306b8b2918",
      "value": "model_optimized.onnx: 100%"
     }
    },
    "2e02ba91bc8741d0a504ae3e93128e08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d900c2a23984017b98f87f749a3bd5b",
      "max": 66465124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_84c58b0fdb2d4f26993c2235a08829a2",
      "value": 66465124
     }
    },
    "714d42a25b0f464ca6d261a9a7a341df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_deae40f420cb4616a64582b63001a4f6",
      "placeholder": "​",
      "style": "IPY_MODEL_3a479e5672ef4cf396981179dca92bcb",
      "value": " 66.5M/66.5M [00:02&lt;00:00, 36.9MB/s]"
     }
    },
    "9655ab59a91545baac1297792a1bdec8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "707c223b9bfa44a49cb15b88ea905230": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7ab334df49c4f68b89b7c306b8b2918": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d900c2a23984017b98f87f749a3bd5b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84c58b0fdb2d4f26993c2235a08829a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "deae40f420cb4616a64582b63001a4f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a479e5672ef4cf396981179dca92bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}