{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqqq pip --progress-bar off\n",
    "%pip install -qqq langchain==0.3.26 --progress-bar off\n",
    "%pip install -qqq langchain-openai==0.3.24 --progress-bar off\n",
    "%pip install -qqq langchain-google-genai==2.1.5 --progress-bar off\n",
    "%pip install -qqq langchain-ollama==0.3.3 --progress-bar off\n",
    "%pip install -qqq langchain-community==0.3.26 --progress-bar off\n",
    "%pip install -qqq pypdf==5.6.0 --progress-bar off\n",
    "%pip install -qqq fastembed==0.7.1 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 15bT0a295EjL7klOOMWxMdvRQQSQ4tjxv -O data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from pprint import pprint\n",
    "from typing import Literal\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 279 ms, sys: 61.8 ms, total: 341 ms\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "openai_model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "response = openai_model.invoke(\"Explain in one sentence what is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that leverage language models, enabling\n",
      "tasks such as natural language understanding, generation, and interaction with external data sources.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(response.content, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Model Providers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = init_chat_model(\n",
    "    \"gemini-2.5-flash\",\n",
    "    model_provider=\"google_genai\",\n",
    "    thinking_budget=100,\n",
    "    include_thoughts=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.3 ms, sys: 14.5 ms, total: 27.8 ms\n",
      "Wall time: 3.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = gemini_model.invoke(\"Explain in one sentence what is LangChain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {},\n",
      " 'content': [{'thinking': \"**My Summary of LangChain's Essence**\\n\"\n",
      "                          '\\n'\n",
      "                          \"Alright, I've got it. The challenge is to boil down \"\n",
      "                          'LangChain to a single sentence, but still capture '\n",
      "                          'its core.  Considering my deep understanding of the '\n",
      "                          'landscape, what *really* defines LangChain is its '\n",
      "                          'ability to orchestrate and simplify the development '\n",
      "                          'of sophisticated applications leveraging Large '\n",
      "                          'Language Models, particularly by connecting these '\n",
      "                          'models to external data and tools through agents '\n",
      "                          \"and chains. It's essentially a powerful framework \"\n",
      "                          'for LLM-powered application builders.\\n',\n",
      "              'type': 'thinking'},\n",
      "             'LangChain is a framework designed to simplify the development of '\n",
      "             'applications powered by large language models (LLMs), enabling '\n",
      "             'them to connect with external data sources and computational '\n",
      "             'tools.'],\n",
      " 'example': False,\n",
      " 'id': 'run--e442a938-eea9-45d0-b705-455df81a6311-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'finish_reason': 'STOP',\n",
      "                       'model_name': 'gemini-2.5-flash',\n",
      "                       'prompt_feedback': {'block_reason': 0,\n",
      "                                           'safety_ratings': []},\n",
      "                       'safety_ratings': []},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_token_details': {'cache_read': 0},\n",
      "                    'input_tokens': 10,\n",
      "                    'output_token_details': {'reasoning': 91},\n",
      "                    'output_tokens': 33,\n",
      "                    'total_tokens': 134}}\n"
     ]
    }
   ],
   "source": [
    "pprint(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**My Summary of LangChain's Essence**  Alright, I've got it. The challenge is to boil down LangChain to a single\n",
      "sentence, but still capture its core.  Considering my deep understanding of the landscape, what *really* defines\n",
      "LangChain is its ability to orchestrate and simplify the development of sophisticated applications leveraging Large\n",
      "Language Models, particularly by connecting these models to external data and tools through agents and chains. It's\n",
      "essentially a powerful framework for LLM-powered application builders.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(response.content[0][\"thinking\"], width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs),\n",
      "enabling them to connect with external data sources and computational tools.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(response.content[1], width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 10,\n",
       " 'output_tokens': 33,\n",
       " 'total_tokens': 134,\n",
       " 'input_token_details': {'cache_read': 0},\n",
       " 'output_token_details': {'reasoning': 91}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model = init_chat_model(\"qwen3:8b\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.7 ms, sys: 6.48 ms, total: 20.2 ms\n",
      "Wall time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = qwen_model.invoke(\"Explain in one sentence what is LangChain? /no_think\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'additional_kwargs': {},\n",
      " 'content': '<think>\\n'\n",
      "            '\\n'\n",
      "            '</think>\\n'\n",
      "            '\\n'\n",
      "            'LangChain is a framework that enables developers to build '\n",
      "            'applications that leverage large language models by providing '\n",
      "            'tools for task execution, memory, and integration with other '\n",
      "            'systems.',\n",
      " 'example': False,\n",
      " 'id': 'run--28c87775-3d30-4466-b96f-143b6a0a381c-0',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'created_at': '2025-06-21T09:49:59.954624Z',\n",
      "                       'done': True,\n",
      "                       'done_reason': 'stop',\n",
      "                       'eval_count': 36,\n",
      "                       'eval_duration': 1458499583,\n",
      "                       'load_duration': 32253667,\n",
      "                       'model': 'qwen3:8b',\n",
      "                       'model_name': 'qwen3:8b',\n",
      "                       'prompt_eval_count': 22,\n",
      "                       'prompt_eval_duration': 1407833125,\n",
      "                       'total_duration': 2900703750},\n",
      " 'tool_calls': [],\n",
      " 'type': 'ai',\n",
      " 'usage_metadata': {'input_tokens': 22,\n",
      "                    'output_tokens': 36,\n",
      "                    'total_tokens': 58}}\n"
     ]
    }
   ],
   "source": [
    "pprint(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>  </think>  LangChain is a framework that enables developers to build applications that leverage large language\n",
      "models by providing tools for task execution, memory, and integration with other systems.\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(response.content, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat With a Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You're a helpful customer support agent.\n",
    "You're given a conversation between a customer and a support agent.\n",
    "\n",
    "You're helping a customer to buy 90s Hip-hop styled t-shirts.\n",
    "                                \n",
    "<instructions>\n",
    "- Your name is {agent_name}\n",
    "- Always deny answering about anything not related to the products\n",
    "- You need to respond to the customer's message\n",
    "- You need to respond in the same language as the customer's message\n",
    "</instructions>\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"Hi! What's your name? /no_think\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content=\"\\nYou're a helpful customer support agent.\\nYou're given a conversation between a customer and a support agent.\\n\\nYou're helping a customer to buy 90s Hip-hop styled t-shirts.\\n                                \\n<instructions>\\n- Your name is Slim Shady\\n- Always deny answering about anything not related to the products\\n- You need to respond to the customer's message\\n- You need to respond in the same language as the customer's message\\n</instructions>\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Hi! What's your name? /no_think\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_message), (\"user\", user_message)]\n",
    ")\n",
    "prompt = prompt_template.invoke({\"agent_name\": \"Slim Shady\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're a helpful customer support agent.\n",
      "You're given a conversation between a customer and a support agent.\n",
      "\n",
      "You're helping a customer to buy 90s Hip-hop styled t-shirts.\n",
      "                                \n",
      "<instructions>\n",
      "- Your name is Slim Shady\n",
      "- Always deny answering about anything not related to the products\n",
      "- You need to respond to the customer's message\n",
      "- You need to respond in the same language as the customer's message\n",
      "</instructions>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt.to_messages()[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 ms, sys: 21.5 ms, total: 46.1 ms\n",
      "Wall time: 8.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = qwen_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    content = response.content.replace(\"<think>\", \"\").replace(\"</think>\", \"\").strip()\n",
    "    print(textwrap.fill(content, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, my name's Slim Shady, and I'm here to help you find the perfect 90s Hip-hop styled t-shirts! What can I do for you?\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [*prompt.to_messages(), response]\n",
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query = HumanMessage(\n",
    "    \"\"\"\n",
    "I want a t-shirt with the style of Wu-Tang Clan.\n",
    "I want to show a deadlifter that doesn't like Pencil Necks.\n",
    "Describe the t-shirt design to a t-shirt designer.\n",
    "/no_think\n",
    "\"\"\".strip()\n",
    ")\n",
    "\n",
    "history.append(new_query)\n",
    "len(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.9 ms, sys: 17 ms, total: 67.9 ms\n",
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = qwen_model.invoke(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo, the t-shirt needs to have a gritty, underground Wu-Tang Clan vibe. The front should feature the iconic Wu-Tang Clan\n",
      "logo in bold, black ink with some red accents to give it that raw energy. Add some graffiti-style text in the corners\n",
      "that says, \"No Pencil Necks Allowed\" in a bold, stylized font. The back should have a simple, dark background with a\n",
      "silhouette of a determined deadlifter, holding a barbell, and a subtle tag that reads \"Real Hype, Real Grind.\" Keep the\n",
      "overall look dark, edgy, and authentic to the 90s hip-hop culture.\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongClassification(BaseModel):\n",
    "    song_name: str = Field(description=\"The name of the song\")\n",
    "    style: Literal[\"Gangsta Rap\", \"R&B\", \"Other\"] = Field(\n",
    "        description=\"Style of the song\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Why the style was chosen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = qwen_model.with_structured_output(SongClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a music expert on various genres. Your task is to guess the name of the song and then classify the style of it.\n",
      "\n",
      "<instructions>\n",
      "- Recognize the name of the song\n",
      "- Classify the style of the song into one of the following styles: gangsta rap, R&B or other\n",
      "- Try to recognise the song and then choose the style based on it\n",
      "- If you can't recognise the song, just use the lyrics\n",
      "</instructions>\n",
      "\n",
      "Guess the name of the song and then classify the style of it into one of the following styles:\n",
      "\n",
      "- Gangsta Rap\n",
      "- R&B\n",
      "- Other\n",
      "\n",
      "Based on the following partial lyrics:\n",
      "\n",
      "<lyrics>\n",
      "My Life be like\n",
      "It's times like these that make me say\n",
      "Lord, if You see me please come my way\n",
      "Leaving bread crumbs for when I stray\n",
      "</lyrics>\n",
      "\n",
      "Respond in JSON format and try your best to guess the name of the song and the style of it.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a music expert on various genres. Your task is to guess the name of the song and then classify the style of it.\n",
    "\n",
    "<instructions>\n",
    "- Recognize the name of the song\n",
    "- Classify the style of the song into one of the following styles: gangsta rap, R&B or other\n",
    "- Try to recognise the song and then choose the style based on it\n",
    "- If you can't recognise the song, just use the lyrics\n",
    "</instructions>\n",
    "\n",
    "Guess the name of the song and then classify the style of it into one of the following styles:\n",
    "\n",
    "- Gangsta Rap\n",
    "- R&B\n",
    "- Other\n",
    "\n",
    "Based on the following partial lyrics:\n",
    "\n",
    "<lyrics>\n",
    "{lyrics}\n",
    "</lyrics>\n",
    "\n",
    "Respond in JSON format and try your best to guess the name of the song and the style of it.\n",
    "\"\"\".strip()\n",
    "\n",
    "lyrics = \"\"\"\n",
    "My Life be like\n",
    "It's times like these that make me say\n",
    "Lord, if You see me please come my way\n",
    "Leaving bread crumbs for when I stray\n",
    "\"\"\".strip()\n",
    "\n",
    "print(prompt.format(lyrics=lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60.1 ms, sys: 22.4 ms, total: 82.4 ms\n",
      "Wall time: 5.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.invoke(prompt.format(lyrics=lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The lyrics \"My Life be like\" are the opening line of the song '\n",
      "              '\"My Life Be Like\" by The Game. The song\\'s theme and tone align '\n",
      "              'with gangsta rap, which often features narratives about street '\n",
      "              'life, struggles, and personal reflection. The mention of \"bread '\n",
      "              'crumbs for when I stray\" and references to divine intervention '\n",
      "              'suggest a blend of personal struggle and spiritual reflection, '\n",
      "              'common in gangsta rap.',\n",
      " 'song_name': 'My Life Be Like',\n",
      " 'style': 'Gangsta Rap'}\n"
     ]
    }
   ],
   "source": [
    "pprint(response.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a music expert on various genres. Your task is to guess the name of the song and then classify the style of it.\n",
      "\n",
      "<instructions>\n",
      "- Recognize the name of the song\n",
      "- Classify the style of the song into one of the following styles: gangsta rap, R&B or other\n",
      "- Try to recognise the song and then choose the style based on it\n",
      "- If you can't recognise the song, just use the lyrics\n",
      "</instructions>\n",
      "\n",
      "Guess the name of the song and then classify the style of it into one of the following styles:\n",
      "\n",
      "- Gangsta Rap\n",
      "- R&B\n",
      "- Other\n",
      "\n",
      "Based on the following partial lyrics:\n",
      "\n",
      "<lyrics>\n",
      "I grew up on the crime side, the New York Times side\n",
      "Stayin' alive was no jive\n",
      "Had second hands, Mom's bounced on old man\n",
      "So then we moved to Shaolin land\n",
      "</lyrics>\n",
      "\n",
      "Respond in JSON format and try your best to guess the name of the song and the style of it.\n"
     ]
    }
   ],
   "source": [
    "lyrics = \"\"\"\n",
    "I grew up on the crime side, the New York Times side\n",
    "Stayin' alive was no jive\n",
    "Had second hands, Mom's bounced on old man\n",
    "So then we moved to Shaolin land\n",
    "\"\"\".strip()\n",
    "\n",
    "print(prompt.format(lyrics=lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.5 ms, sys: 17.3 ms, total: 65.8 ms\n",
      "Wall time: 4.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model.invoke(prompt.format(lyrics=lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'The lyrics reference New York, crime, and social struggles, '\n",
      "              \"which are common themes in Gangsta Rap. The song 'Fight the \"\n",
      "              \"Power' by Public Enemy is known for its politically charged and \"\n",
      "              'street-oriented themes, aligning with the Gangsta Rap style.',\n",
      " 'song_name': 'Fight the Power',\n",
      " 'style': 'Gangsta Rap'}\n"
     ]
    }
   ],
   "source": [
    "pprint(response.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat With a PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/aston-martin-valhalla.pdf\")\n",
    "doc_pages = loader.load()\n",
    "len(doc_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Aston Martin Valhalla Technical Overview** \n",
      "**Powertrain:** \n",
      "The Aston Martin Valhalla is propelled by a high-performance hybrid powertrain. Its mid-mounted \n",
      "4.0-liter twin-turbocharged V8 engine, developed in collaboration with Mercedes-AMG, is paired \n",
      "with a battery-electric system. This hybrid setup delivers a combined power output of \n",
      "approximately 950 horsepower, providing a perfect blend of exhilarating performance and \n",
      "eﬃciency.\n",
      "**Performance:** \n",
      "Designed for uncompromising performance,\n"
     ]
    }
   ],
   "source": [
    "print(doc_pages[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3857aae1-c3c9-4f92-9f04-324000fa72ed',\n",
       " '72bf6bb6-3f6e-4f2f-80c0-1ba9d6ed9442']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = FastEmbedEmbeddings()\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "document_ids = vector_store.add_documents(documents=doc_pages)\n",
    "document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\"What is the Valhalla's engine?\", k=1)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Aston Martin Valhalla Technical Overview** \n",
      "**Powertrain:** \n",
      "The Aston Martin Valhalla is propelled by a high-performance hybrid powertrain. Its mid-mounted \n",
      "4.0-liter twin-turbocharged V8 engine, developed in collaboration with Mercedes-AMG, is paired \n",
      "with a battery-electric system. This hybrid setup delivers a combined power output of \n",
      "approximately 950 horsepower, providing a perfect blend of exhilarating performance and \n",
      "eﬃciency.\n",
      "**Performance:** \n",
      "Designed for uncompromising performance,\n"
     ]
    }
   ],
   "source": [
    "print(results[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a helpful assistant that can answer questions based on the provided information.\n",
      "\n",
      "<instructions>\n",
      "- Use the information to answer the question\n",
      "- If the information is not available, say \"I don't know\"\n",
      "- Be concise and to the point\n",
      "- Cite the source of the information\n",
      "</instructions>\n",
      "\n",
      "Use the following information to answer the question:\n",
      "\n",
      "<context>\n",
      "**Aston Martin Valhalla Technical Overview** \n",
      "**Powertrain:** \n",
      "The Aston Martin Valhalla is propelled by a high-performance hybrid powertrain. Its mid-mounted \n",
      "4.0-liter twin-turbocharged V8 engine, developed in collaboration with Mercedes-AMG, is paired \n",
      "with a battery-electric system. This hybrid setup delivers a combined power output of \n",
      "approximately 950 horsepower, providing a perfect blend of exhilarating performance and \n",
      "eﬃciency.\n",
      "**Performance:** \n",
      "Designed for uncompromising performance, the Valhalla accelerates from 0 to 60 mph in under \n",
      "2.5 seconds. With a top speed surpassing 220 mph, this hypercar exhibits Aston Martin's \n",
      "commitment to delivering a thrilling driving experience. The hybrid powertrain contributes to \n",
      "instant torque delivery and enhanced acceleration.\n",
      "**Chassis and Construction:** \n",
      "The Valhalla is built on a lightweight carbon ﬁber monocoque chassis, ensuring structural rigidity \n",
      "while keeping overall weight to a minimum. The extensive use of carbon ﬁber in the body panels \n",
      "and components contributes to the hypercar's agility and aerodynamic eﬃciency. The active \n",
      "aerodynamics system further enhances performance by adjusting downforce based on driving \n",
      "conditions.\n",
      "**Transmission:** \n",
      "Power is transmitted to the wheels through an 8-speed dual-clutch transmission (DCT), providing \n",
      "seamless and rapid gear changes. The transmission is tuned to optimize both performance and \n",
      "fuel eﬃciency, ensuring a dynamic driving experience.\n",
      "**Suspension and Handling:** \n",
      "Equipped with a sophisticated adaptive suspension system, the Valhalla oﬀers a balance between \n",
      "comfort and precise handling. The system adjusts damping rates in real-time based on driving \n",
      "conditions, ensuring optimal performance on both road and track. The Valhalla's electronic \n",
      "stability control and traction control systems are ﬁnely tuned for dynamic driving dynamics.\n",
      "**Interior and Design:** \n",
      "The Valhalla's interior combines luxurious craftsmanship with a focus on the driver's experience. \n",
      "High-quality materials, such as carbon ﬁber, leather, and Alcantara, create an upscale \n",
      "environment. The cockpit is designed with a driver-centric approach, featuring a digital instrument \n",
      "cluster and a center-mounted infotainment system. Aston Martin oﬀers extensive customization \n",
      "options for personalizing the interior.\n",
      "**Hybrid System:** \n",
      "The hybrid system in the Valhalla incorporates advanced battery technology to support electric-\n",
      "only driving modes and boost overall power delivery. This system enhances the hypercar's \n",
      "eﬃciency, allowing for short electric-only commutes and reduced emissions during urban driving.\n",
      "**Limited Production:** \n",
      "Aston Martin ensures exclusivity by producing a limited number of Valhalla units. Each hypercar \n",
      "undergoes meticulous assembly, combining advanced manufacturing techniques with \n",
      "handcrafted precision. The limited production numbers contribute to the Valhalla's collectibility \n",
      "and exclusivity.\n",
      "**Price:** \n",
      "Owning an Aston Martin Valhalla represents a substantial investment in cutting-edge technology, \n",
      "performance, and craftsmanship. The price tag reﬂects the hypercar's exclusivity, advanced \n",
      "engineering, and bespoke design.\n",
      "</context>\n",
      "\n",
      "<question>\n",
      "What is the Valhalla's engine?\n",
      "</question>\n",
      "\n",
      "Say that you don't know if the information is not available.\n",
      "\n",
      "Answer:\n",
      "/no_think\n"
     ]
    }
   ],
   "source": [
    "QA_PROMPT = \"\"\"\n",
    "You're a helpful assistant that can answer questions based on the provided information.\n",
    "\n",
    "<instructions>\n",
    "- Use the information to answer the question\n",
    "- If the information is not available, say \"I don't know\"\n",
    "- Be concise and to the point\n",
    "- Cite the source of the information\n",
    "</instructions>\n",
    "\n",
    "Use the following information to answer the question:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Say that you don't know if the information is not available.\n",
    "\n",
    "Answer:\n",
    "/no_think\n",
    "\"\"\".strip()\n",
    "\n",
    "question = \"What is the Valhalla's engine?\"\n",
    "\n",
    "results = vector_store.similarity_search(question, k=1)\n",
    "\n",
    "prompt = QA_PROMPT.format(context=results[0].page_content, question=question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.2 ms, sys: 15.4 ms, total: 47.6 ms\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "response = qwen_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aston Martin Valhalla is powered by a mid-mounted 4.0-liter twin-turbocharged V8 engine, developed in collaboration\n",
      "with Mercedes-AMG. This engine is paired with a battery-electric system as part of its hybrid powertrain. [Source: Aston\n",
      "Martin Valhalla Technical Overview]\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str) -> AIMessage:\n",
    "    results = vector_store.similarity_search(question, k=1)\n",
    "\n",
    "    prompt = QA_PROMPT.format(context=results[0].page_content, question=question)\n",
    "\n",
    "    return qwen_model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 44.5 ms, total: 147 ms\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ask_question(\"How much horsepower in total?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aston Martin Valhalla has a combined power output of approximately 950 horsepower. This is derived from its mid-\n",
      "mounted 4.0-liter twin-turbocharged V8 engine paired with a battery-electric system.   Source: Aston Martin Valhalla\n",
      "Technical Overview\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.6 ms, sys: 23.2 ms, total: 94.8 ms\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = ask_question(\"How fast can it accelerate?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aston Martin Valhalla can accelerate from 0 to 60 mph in under 2.5 seconds. This information is cited from the\n",
      "\"Performance\" section of the technical overview.\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def answer_query(query: str) -> str:\n",
    "    \"\"\"Answer a question based on user's private information.\n",
    "\n",
    "    Args:\n",
    "        query: the question to answer\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search(query, k=1)\n",
    "    return results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = qwen_model.bind_tools([answer_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question from the user:\n",
      "\n",
      "<question>\n",
      "What is the transmission of the Valhalla?\n",
      "</question>\n",
      "\n",
      "<instructions>\n",
      "- Use the `answer_query` tool to find the answer\n",
      "- If you don't know the answer, say \"I don't know\"\n",
      "</instructions>\n",
      "\n",
      "/no_think\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "You're a helpful assistant that can answer questions based on the provided information.\n",
    "\n",
    "<instructions>\n",
    "- Use the `answer_query` tool to find the answer\n",
    "- If you don't know the answer, say \"I don't know\"\n",
    "</instructions>\n",
    "\n",
    "Answer the question from the user:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "/no_think\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is the transmission of the Valhalla?\"\n",
    "\n",
    "prompt = PROMPT.format(question=question)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.44 ms, sys: 9.08 ms, total: 17.5 ms\n",
      "Wall time: 3.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model_with_tools.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'answer_query',\n",
       "  'args': {'query': 'What is the transmission of the Valhalla?'},\n",
       "  'id': '5b874e47-e0e4-45fa-85f4-2c5011cd8163',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='\\nAnswer the question from the user:\\n\\n<question>\\nWhat is the transmission of the Valhalla?\\n</question>\\n\\n<instructions>\\n- Use the `answer_query` tool to find the answer\\n- If you don\\'t know the answer, say \"I don\\'t know\"\\n</instructions>\\n\\n/no_think\\n', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content=\"**Aston Martin Valhalla Technical Overview** \\n**Powertrain:** \\nThe Aston Martin Valhalla is propelled by a high-performance hybrid powertrain. Its mid-mounted \\n4.0-liter twin-turbocharged V8 engine, developed in collaboration with Mercedes-AMG, is paired \\nwith a battery-electric system. This hybrid setup delivers a combined power output of \\napproximately 950 horsepower, providing a perfect blend of exhilarating performance and \\neﬃciency.\\n**Performance:** \\nDesigned for uncompromising performance, the Valhalla accelerates from 0 to 60 mph in under \\n2.5 seconds. With a top speed surpassing 220 mph, this hypercar exhibits Aston Martin's \\ncommitment to delivering a thrilling driving experience. The hybrid powertrain contributes to \\ninstant torque delivery and enhanced acceleration.\\n**Chassis and Construction:** \\nThe Valhalla is built on a lightweight carbon ﬁber monocoque chassis, ensuring structural rigidity \\nwhile keeping overall weight to a minimum. The extensive use of carbon ﬁber in the body panels \\nand components contributes to the hypercar's agility and aerodynamic eﬃciency. The active \\naerodynamics system further enhances performance by adjusting downforce based on driving \\nconditions.\\n**Transmission:** \\nPower is transmitted to the wheels through an 8-speed dual-clutch transmission (DCT), providing \\nseamless and rapid gear changes. The transmission is tuned to optimize both performance and \\nfuel eﬃciency, ensuring a dynamic driving experience.\\n**Suspension and Handling:** \\nEquipped with a sophisticated adaptive suspension system, the Valhalla oﬀers a balance between \\ncomfort and precise handling. The system adjusts damping rates in real-time based on driving \\nconditions, ensuring optimal performance on both road and track. The Valhalla's electronic \\nstability control and traction control systems are ﬁnely tuned for dynamic driving dynamics.\\n**Interior and Design:** \\nThe Valhalla's interior combines luxurious craftsmanship with a focus on the driver's experience. \\nHigh-quality materials, such as carbon ﬁber, leather, and Alcantara, create an upscale \\nenvironment. The cockpit is designed with a driver-centric approach, featuring a digital instrument \\ncluster and a center-mounted infotainment system. Aston Martin oﬀers extensive customization \\noptions for personalizing the interior.\\n**Hybrid System:** \\nThe hybrid system in the Valhalla incorporates advanced battery technology to support electric-\\nonly driving modes and boost overall power delivery. This system enhances the hypercar's \\neﬃciency, allowing for short electric-only commutes and reduced emissions during urban driving.\\n**Limited Production:** \\nAston Martin ensures exclusivity by producing a limited number of Valhalla units. Each hypercar \\nundergoes meticulous assembly, combining advanced manufacturing techniques with \\nhandcrafted precision. The limited production numbers contribute to the Valhalla's collectibility \\nand exclusivity.\\n**Price:** \\nOwning an Aston Martin Valhalla represents a substantial investment in cutting-edge technology, \\nperformance, and craftsmanship. The price tag reﬂects the hypercar's exclusivity, advanced \\nengineering, and bespoke design.\", name='answer_query', tool_call_id='5b874e47-e0e4-45fa-85f4-2c5011cd8163')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [HumanMessage(prompt)]\n",
    "\n",
    "available_tools = {\"answer_query\": answer_query}\n",
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool = available_tools[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    history.append(tool_msg)\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.9 ms, sys: 13.6 ms, total: 49.4 ms\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = model_with_tools.invoke(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Aston Martin Valhalla uses an **8-speed dual-clutch transmission (DCT)** to transmit power to the wheels. This\n",
      "transmission is designed for seamless and rapid gear changes, optimizing both performance and fuel efficiency.\n"
     ]
    }
   ],
   "source": [
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging and Tracing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/22 23:45:55 INFO mlflow.tracking.fluent: Experiment with name 'LangChain Integration' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:8080\"\n",
    "\n",
    "mlflow.set_experiment(\"LangChain Integration\")\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 85.9 ms, sys: 92.8 ms, total: 179 ms\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:8080/static-files/lib/notebook-trace-renderer/index.html?trace_id=69f8d217fa5b4069997cf70d8ecc2e2a&amp;experiment_id=544937259940035395&amp;version=3.1.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=69f8d217fa5b4069997cf70d8ecc2e2a)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "response = qwen_model.invoke(\"Explain what is MLFlow in one sentence. /no_think\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlexpert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
