1
00:00:00,000 --> 00:00:02,700
We take the results of our initial iteration through

2
00:00:02,700 --> 00:00:07,019
the design lifecycle and use the results to return to the need-finding process.

3
00:00:07,019 --> 00:00:10,605
Now, that's not to say we need to redo everything from scratch.

4
00:00:10,605 --> 00:00:16,020
But our prototypes and evaluation have now increased our understanding of the problem.

5
00:00:16,020 --> 00:00:20,570
There are things we learn by prototyping and evaluating about the task itself.

6
00:00:20,570 --> 00:00:25,050
In this case, we could have learned that even for exercises with our hands free,

7
00:00:25,050 --> 00:00:28,230
gestures are still tough because they're moving around so much.

8
00:00:28,230 --> 00:00:30,660
The evaluation process may have also given us

9
00:00:30,660 --> 00:00:34,425
new questions we want to ask users to understand the task better.

10
00:00:34,425 --> 00:00:38,790
So, for example, Maureen could have mentioned that she needed the ability to rewind.

11
00:00:38,790 --> 00:00:41,970
We might want to know how common a problem is that.

12
00:00:41,970 --> 00:00:45,200
So, in many ways, synthesizing our experiences with

13
00:00:45,200 --> 00:00:48,320
the evaluation is our next need-finding process.

14
00:00:48,320 --> 00:00:51,920
So, then, we move on to design alternative stage.

15
00:00:51,920 --> 00:00:56,380
Again, that doesn't mean we start from scratch and come up with all new ideas.

16
00:00:56,380 --> 00:01:00,650
Here it means expanding on our current ideas, flushing them out of it,

17
00:01:00,650 --> 00:01:02,600
and brainstorming them in the context of

18
00:01:02,600 --> 00:01:05,555
those personas and scenarios that we used previously.

19
00:01:05,555 --> 00:01:08,450
We might also come up with whole new ideas here based on

20
00:01:08,450 --> 00:01:12,170
our first iteration, then more prototyping.

21
00:01:12,170 --> 00:01:14,660
At this point, we might discover that as we

22
00:01:14,660 --> 00:01:17,330
try to increase the fidelity of some of our prototypes,

23
00:01:17,330 --> 00:01:21,235
the technology or the resource is just aren't quite there yet.

24
00:01:21,235 --> 00:01:24,350
For example, while the gesture interface might've been promising,

25
00:01:24,350 --> 00:01:26,585
in the Wizard of Oz prototype, we could've done for that,

26
00:01:26,585 --> 00:01:31,135
we don't yet have the technology to recognize gestures that way on the go,

27
00:01:31,135 --> 00:01:34,879
or we might have found that the expense related to the prototype is unfeasible,

28
00:01:34,879 --> 00:01:36,950
or we could have realized that the prototype would

29
00:01:36,950 --> 00:01:39,890
require violating some of our other user needs.

30
00:01:39,890 --> 00:01:42,860
So, for example, we could do gesture recognition if we

31
00:01:42,860 --> 00:01:45,980
had users hold a physical device that could recognize certain gestures,

32
00:01:45,980 --> 00:01:48,890
but that might be too expensive to produce or it might

33
00:01:48,890 --> 00:01:52,400
conflict with our audiences need for a hands-free system.

34
00:01:52,400 --> 00:01:56,090
So, we move on with the prototypes that we can build,

35
00:01:56,090 --> 00:01:59,755
with the goal of getting to the feedback stage as quickly as possible.

36
00:01:59,755 --> 00:02:01,425
For our voice recognition,

37
00:02:01,425 --> 00:02:05,010
instead of trying to build a full voice recognition system,

38
00:02:05,010 --> 00:02:09,420
maybe we just build a system that can recognize very simplistic voice commands.

39
00:02:09,420 --> 00:02:11,475
Instead of recognizing words,

40
00:02:11,475 --> 00:02:15,485
maybe it just recognizes the number of utterances if that's easier to build.

41
00:02:15,485 --> 00:02:17,705
For the screen, maybe we build

42
00:02:17,705 --> 00:02:20,950
a wireframe prototype that moves between different screens on a phone,

43
00:02:20,950 --> 00:02:22,880
but we don't connect it to a real system.

44
00:02:22,880 --> 00:02:24,890
We still have someone run alongside

45
00:02:24,890 --> 00:02:28,195
the exerciser and play the book according to their commands.

46
00:02:28,195 --> 00:02:31,730
That way, we focus on usability instead of things like

47
00:02:31,730 --> 00:02:35,389
integration with audiobook apps or voice-to-text transcription,

48
00:02:35,389 --> 00:02:38,405
things that take a lot of work to get right and might end up

49
00:02:38,405 --> 00:02:41,930
unnecessary if we find that the prototype isn't actually useful,

50
00:02:41,930 --> 00:02:44,550
and then we evaluate again.

51
00:02:44,550 --> 00:02:47,800
This time, we probably get a little bit more objective.

52
00:02:47,800 --> 00:02:51,024
We still want data on the qualitative user experience,

53
00:02:51,024 --> 00:02:55,210
but we also want data on things like how long does it take the user to perform

54
00:02:55,210 --> 00:02:57,145
the desired actions in the interface or

55
00:02:57,145 --> 00:02:59,665
what prevents them from working with the interface.

56
00:02:59,665 --> 00:03:01,300
Imagine that we found, for instance,

57
00:03:01,300 --> 00:03:02,755
that for many exercisers,

58
00:03:02,755 --> 00:03:05,925
they go through places that are too loud for voice commands to work,

59
00:03:05,925 --> 00:03:07,360
or we could have found,

60
00:03:07,360 --> 00:03:09,280
that the time it takes to pull out the interface and

61
00:03:09,280 --> 00:03:12,010
interact is just too distracting and limiting.

62
00:03:12,010 --> 00:03:16,370
That information is once again useful to our ongoing iteration.

63
00:03:16,370 --> 00:03:18,035
At the end of that process,

64
00:03:18,035 --> 00:03:20,439
we again have some higher fidelity prototypes,

65
00:03:20,439 --> 00:03:22,285
but no product yet.

66
00:03:22,285 --> 00:03:24,610
So, we go again.

