1
00:00:00,190 --> 00:00:03,660
If you said yes, there are several
reasons you might have stated.

2
00:00:03,660 --> 00:00:06,480
You might agree that because
the terms of service covered it,

3
00:00:06,480 --> 00:00:07,960
it was technically ethical research.

4
00:00:07,960 --> 00:00:10,910
The users did agree to things like this.

5
00:00:10,910 --> 00:00:14,170
You may have actually read the article
or read other publications about it and

6
00:00:14,170 --> 00:00:17,550
noted that Facebook actually has
an internal IRB that reviews things

7
00:00:17,550 --> 00:00:18,600
like this.

8
00:00:18,600 --> 00:00:22,970
And in this case,
an external IRB did review the study.

9
00:00:22,970 --> 00:00:26,030
If you said no, the reason you
gave may have been that we know

10
00:00:26,030 --> 00:00:28,570
users are not aware of
what's in terms of use.

11
00:00:28,570 --> 00:00:31,707
We have plenty of studies that indicate
that users really don't spend any time

12
00:00:31,707 --> 00:00:33,930
reading what they're agreeing to.

13
00:00:33,930 --> 00:00:37,236
And while technically, it's true
that they're still agreeing to it,

14
00:00:37,236 --> 00:00:40,370
what we're interested in here
are participants' rights.

15
00:00:40,370 --> 00:00:43,740
If we know that users aren't
reading what they're agreeing to,

16
00:00:43,740 --> 00:00:46,958
don't we have an ethical obligation to
make sure they're aware before we go

17
00:00:46,958 --> 00:00:47,840
ahead with it.

18
00:00:47,840 --> 00:00:51,600
We also might say no because users
couldn't opt out of this study.

19
00:00:51,600 --> 00:00:54,710
Part of that is because opting out of
the study alone means deactivating your

20
00:00:54,710 --> 00:00:57,760
entire Facebook account or
just stopping using the tool.

21
00:00:57,760 --> 00:01:00,420
But part of it is that users also
weren't aware that a study was

22
00:01:00,420 --> 00:01:01,610
now going on.

23
00:01:01,610 --> 00:01:04,370
They couldn't opt out of
the study specifically, nor

24
00:01:04,370 --> 00:01:07,230
could they even opt out of it by closing
down their entire Facebook account

25
00:01:07,230 --> 00:01:09,630
because they didn't know
when the study had started.

26
00:01:09,630 --> 00:01:11,020
That ties into the other issue.

27
00:01:11,020 --> 00:01:13,910
Users weren't notified that they
were participants in an experiment.

28
00:01:13,910 --> 00:01:16,730
So even though they technically agreed
to it when they agreed to Facebook's

29
00:01:16,730 --> 00:01:20,100
terms of service, one could argue
the fact they weren't notified when

30
00:01:20,100 --> 00:01:23,400
the study was beginning and ending
means that it wasn't ethical research.

31
00:01:23,400 --> 00:01:25,420
I'm not going to give you a right or
wrong answer to this.

32
00:01:25,420 --> 00:01:28,410
There's a very interesting
conversation to have about this.

33
00:01:28,410 --> 00:01:32,000
But what's most important here are the
interesting questions that it brings up.

34
00:01:32,000 --> 00:01:35,780
Especially in regard to companies
doing human subjects research

35
00:01:35,780 --> 00:01:38,870
that doesn't have any over sight
from the federal government.

36
00:01:38,870 --> 00:01:41,560
If you agreed with these
reasons why it wasn't ethical,

37
00:01:41,560 --> 00:01:43,220
what could they have done to fix it?

38
00:01:43,220 --> 00:01:45,600
Maybe they could have separated
out the consent process for

39
00:01:45,600 --> 00:01:48,870
research studies from the rest
of Facebook as a whole.

40
00:01:48,870 --> 00:01:52,460
Maybe they could have specifically
requested that individual users opt-in,

41
00:01:52,460 --> 00:01:54,360
and alert them when the study was done,
but

42
00:01:54,360 --> 00:01:56,780
not tell them what's
actually being manipulated.

43
00:01:56,780 --> 00:01:58,820
And even if the original
study was ethical,

44
00:01:58,820 --> 00:02:01,809
there were likely things that
could have reduced the backlash.

45
00:02:01,809 --> 00:02:05,530
At the same time, those things
might have affected the results.

46
00:02:05,530 --> 00:02:07,050
These are the tradeoffs
that we deal with.

