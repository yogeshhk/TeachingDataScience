1
00:00:00,270 --> 00:00:03,660
Regardless of the type of evaluation
you're planning to perform, there's

2
00:00:03,660 --> 00:00:07,990
a series of steps to perform to ensure
that the evaluation is actually useful.

3
00:00:07,990 --> 00:00:11,071
First, we want to clearly define
the task that we're examining.

4
00:00:11,071 --> 00:00:14,300
Depending on your place in the design
process this can be very large or

5
00:00:14,300 --> 00:00:15,380
very small.

6
00:00:15,380 --> 00:00:18,980
If we were designing Facebook, it can be
as simple as posting a status update, or

7
00:00:18,980 --> 00:00:22,550
as complicated as navigating amongst and
using several different pages.

8
00:00:22,550 --> 00:00:25,620
It could involve context and constraints
like taking notes while running, or

9
00:00:25,620 --> 00:00:28,630
looking up a restaurant address
without touching the screen.

10
00:00:28,630 --> 00:00:29,580
Whatever it is,

11
00:00:29,580 --> 00:00:33,910
we want to start by clearly identifying
what task we're going to investigate.

12
00:00:33,910 --> 00:00:37,095
Second, we want to define
our performance measures.

13
00:00:37,095 --> 00:00:39,835
How are we going to evaluate
the user's performance?

14
00:00:39,835 --> 00:00:41,865
Qualitatively, it could be
based on their spoken or

15
00:00:41,865 --> 00:00:44,055
written feedback about the experience.

16
00:00:44,055 --> 00:00:47,365
Quantitatively, we can measure
efficiency in certain activities or

17
00:00:47,365 --> 00:00:49,135
count the number of mistakes.

18
00:00:49,135 --> 00:00:52,885
Defining performance measures
helps us avoid confirmation bias.

19
00:00:52,885 --> 00:00:55,515
It makes sure we don't just pick
out whatever observations or

20
00:00:55,515 --> 00:00:59,320
data confirm our hypotheses, or
say that we have a good interface.

21
00:00:59,320 --> 00:01:01,380
It forces us to look at it objectively.

22
00:01:02,520 --> 00:01:05,019
Third, we develop the experiment.

23
00:01:05,019 --> 00:01:08,040
How will we find user's performance
on the performance measures?

24
00:01:08,040 --> 00:01:10,940
If we're looking qualitatively will
we have them think out loud while

25
00:01:10,940 --> 00:01:11,740
they're using the tool?

26
00:01:11,740 --> 00:01:14,400
Or will we have them do
a survey after they're done?

27
00:01:14,400 --> 00:01:17,420
If we're looking quantitatively what
will we measure, what will we control,

28
00:01:17,420 --> 00:01:18,910
and what will we vary?

29
00:01:18,910 --> 00:01:21,590
This is also where we ask questions
about whether our assessment

30
00:01:21,590 --> 00:01:23,750
measures are reliable and valid.

31
00:01:23,750 --> 00:01:26,580
And whether the users we're
testing are generalizable.

32
00:01:26,580 --> 00:01:29,140
Fourth, we recruit the participants.

33
00:01:29,140 --> 00:01:32,310
As part of the ethics process, we make
sure we're recruiting participants

34
00:01:32,310 --> 00:01:35,510
who are aware of their rights and
contributing willingly.

35
00:01:35,510 --> 00:01:37,500
Then fifth, we do the experiment.

36
00:01:37,500 --> 00:01:41,590
We have them walk-through what we
outline when we develop the experiment.

37
00:01:41,590 --> 00:01:43,580
Sixth, we analyze the data.

38
00:01:43,580 --> 00:01:47,820
We focus on what the data tells us
about our performance measures.

39
00:01:47,820 --> 00:01:50,730
It's important that we stay close
to what we outlined initially.

40
00:01:50,730 --> 00:01:53,740
It can be tempting to just look for
whatever supports are design but

41
00:01:53,740 --> 00:01:55,680
we want to be impartial.

42
00:01:55,680 --> 00:01:58,940
If we find some evidence that suggests
our interface is good in ways we didn't

43
00:01:58,940 --> 00:02:02,255
anticipate, we can always do a follow
up experiment to test if we're right.

44
00:02:03,315 --> 00:02:06,345
Seventh, we summarize the data
in a way that informs our on

45
00:02:06,345 --> 00:02:08,264
going design process.

46
00:02:08,264 --> 00:02:09,485
What did our data say was working?

47
00:02:09,485 --> 00:02:11,115
What could be improved?

48
00:02:11,115 --> 00:02:13,345
How can we take the results
of this experiment and

49
00:02:13,345 --> 00:02:16,125
use it to then revise our interface?

50
00:02:16,125 --> 00:02:19,385
The results of this experiment then
become part of our design life cycle.

51
00:02:20,435 --> 00:02:25,480
We investigated user needs, develop
alternatives, made a prototype and

52
00:02:25,480 --> 00:02:27,810
put the prototype in front of users.

53
00:02:27,810 --> 00:02:30,450
To put the prototype in front of users,

54
00:02:30,450 --> 00:02:32,320
we walked through this
experimental method.

55
00:02:32,320 --> 00:02:34,470
We defined the task,
defined the performance measures,

56
00:02:34,470 --> 00:02:37,960
developed the experiment,
recruited them, did the experiment,

57
00:02:37,960 --> 00:02:39,970
analyzed our data and
summarized our data.

58
00:02:41,250 --> 00:02:44,980
Based on the experience, we now have
the data necessary to develop a better

59
00:02:44,980 --> 00:02:49,300
understanding of the user's needs, to
revisit our earlier design alternatives

60
00:02:49,300 --> 00:02:52,710
and to either improve our prototypes
by increasing their fidelity or

61
00:02:52,710 --> 00:02:55,270
by revising them based
on what we just learned.

62
00:02:55,270 --> 00:02:57,800
Regardless of whether we're
doing qualitative, empirical, or

63
00:02:57,800 --> 00:03:02,030
predictive evaluation,
these steps remain largely the same.

64
00:03:02,030 --> 00:03:06,130
Those different types of evaluation just
fill in the experiment that we develop,

65
00:03:06,130 --> 00:03:10,000
and they inform our performance measure,
data analysis, and summaries.

