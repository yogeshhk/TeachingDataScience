\input{../shared/shared}

\renewcommand{\course}{Artificial Intelligence}
\renewcommand{\coursepicture}{course_ai}
\renewcommand{\coursedate}{Winter 2019}
\renewcommand{\topic}{Graphical Models}
\renewcommand{\keywords}{}

\slides

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\story{

Graphical models are a generic language to express ``structured''
probabilistic models. Structured simply means that we talk about many
random variables and many coupling terms, where each coupling term
concerns only a (usually small) subset of random variables. so,
structurally they are very similar to CSPs. But the coupling terms are
not boolean functions but real-valued functions, called factors. And
that defines a probability distribution over all RVs. The problem then
is either to find the most probable value assignment to all RVs
(called MAP inference problem), or to find the probabilities over the
values of a single variable that arises from the couplings (called
marginal inference).

There are so many applications of graphical models that is it hard to
pick some to list: Modelling gene networks (e.g.\ to understand genetic
diseases), structured text models (e.g.\ to cluster text into topics),
modelling dynamic processes like music or human activities (like
cooking or so), modelling more structured Markov Decision Processes
(hierarchical RL, POMDPs, etc), modelling multi-agent systems,
localization and mapping of mobile robots, and also
many of the core ML methods can be expressed as graphical models,
e.g.\ Bayesian (kernel) logistic/ridge regression, Gaussian mixture
models, clustering methods, many unsupervised learning methods, ICA,
PCA, etc. It is though fair to say that these methods do
not \emph{have} to be expressed as graphical models; but they can be
and I think it is very helpful to see the underlying principles of
these methods when expressing them in terms of graphical models. And
graphical models then allow you to invent variants/combinations of such methods
specifically for your particular data domain.

In this lecture we introduce Bayesian networks and factor graphs and  discuss
probabilistic inference methods. Exact inference amounts to summing
over variables in a certain order. This can be automated in a way that
exploits the graph structure, leading to what is called variable
elimination and message passing on trees. The latter is perfectly
analogous to constraint propagation to exactly solve tree CSPs. For
non-trees, message passing becomes loopy belief propagation, which
approximates a solution. Monte-Carlo sampling methods are also important tools for
approximate inference, which are beyond this lecture though.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Bayes Nets and Conditional Independence}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Outline}{

\item A.~ Introduction

{\tiny
-- Motivation and definition of Bayes Nets

-- Conditional independence in Bayes Nets

-- Examples

}

\item B.~ Inference in Graphical Models

{\tiny
-- Variable Elimination \& Factor Graphs

-- Message passing, Loopy Belief Propagation

-- Sampling methods (Rejection, Importance,
Gibbs)

}

%% \item C.~ Learning in Graphical Models

%% {\tiny
%% -- Maximum likelihood learning

%% -- Expectation Maximization, Gaussian Mixture Models, Hidden Markov
%% Models, Free Energy formulation

%% -- Discriminative Learning

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Graphical Models}{

\item The core difficulty in modelling is specifying

\cen{\emph{What are the relevant variables?}}

\cen{\emph{How do they depend on each other?}}

\cen{(Or how \emph{could} they depend on each other $\to$ learning)}

~%\mypause

\item \textbf{Graphical models} are a graphical notation for

~~ 1) which random variables exist

~~ 2) which random variables are directly coupled, and how

Thereby they \emph{describe a joint probability distribution}
$P(X_1,..,X_n)$ over
$n$ random variables.

~%\mypause

\item 2 basic variants:
\begin{items}
\item Bayesian Networks ~~ (aka.\ directed model, belief network)

\item Factor Graphs ~~ (aka.\ undirected model, Markov Random Field)
\end{items}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example}{

\cen{drinking red wine $\to$ longevity?}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Bayesian Network}
\slide{Bayesian Networks}{

\item A \textbf{Bayesian Network} is a
\begin{items}
\item directed acyclic graph (DAG)

\item where each node represents a random variable $X_i$

\item for each node we have a conditional probability distribution

   \cen{$P(X_i \| \text{Parents}(X_i))$}
\end{items}

~

\item In the simplest case (discrete RVs), the conditional 
   distribution is represented as a conditional probability table
   (\textbf{CPT})


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Bayesian Networks}{

\item DAG $\to$ we can sort the RVs; edges only go from lower to
higher index

\item  \textbf{The joint distribution can be factored as}
\begin{align*}
P(X_{1:n})
 &= \prod_{i=1}^n P(X_i \| \text{Parents}(X_i))
\end{align*}
%(notation: $X_{\pi(i)} = (X_a,..,X_b)$ if $\pi(i)=(a,..,b)$ )

\item Missing links imply conditional independence

\item Forward sampling from joint distribution


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example}{

\hfill{\tiny (Heckermann 1995)}
\show[.6]{vl2-car}

$
\iff ~ P(S,T,G,F,B) = P(B)~ P(F)~ P(G|F,B)~ P(T|B)~ P(S|T,F)
$

~

\item Table sizes: ~ LHS = $2^5-1=31$ ~ RHS = $1 + 1 + 4 + 2 + 4=12$



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Constructing a Bayes Net}{

%% \item[1.] Choose a relevant set of variables $X_i$ that describe the
%% domain

%% \item[2.] Choose an ordering for the variables

%% \item[3.] While there are variables left

%%  (a) Pick a variable $X_i$ and add it to the network

%%  (b) Set $\text{Parents}(X_i)$ to some minimal set of nodes already in
%%  the net

%%  (c) Define the CPT for $X_i$

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{}{

%% \item This procedure is guaranteed to produce a DAG

%% \item Different orderings may lead to \textbf{different Bayes
%% nets representing the same joint distribution}:

%% \item To ensure maximum sparsity choose a wise order (``root
%% causes''first). Counter example: construct DAG for the car example
%% using the ordering S, T, G, F, B

%% \item ``Wrong'' ordering will give same
%% joint distribution, but will require the specification of more numbers
%% than otherwise necessary

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Conditional independence in a Bayes Net}
\slide{Bayes Nets \& conditional independence}{

\item Independence: ~ $Indep(X,Y) \iff P(X,Y) = P(X)~ P(Y)$

\item Conditional independence: ~

\cen{$Indep(X,Y|Z) \iff P(X,Y | Z) = P(X|Z)~ P(Y|Z)$}

%(This implies $P(X,Y,Z) = P(X,Y | Z)~ P(Z) = P(X|Z)~ P(Y|Z)~ P(Z)$.)

~

~\mypause

\hspace*{-8mm}\threecol{.33}{.33}{.33}{\center
\show[.7]{vl2-indep1}

(head-to-head)

~

$Indep(X,Y)$

$\neg Indep(X,Y|Z)$
}{\center
\show[.7]{vl2-indep2}

(tail-to-tail)

~

$\neg Indep(X,Y)$

$Indep(X,Y|Z)$
}{\center
\show[.7]{vl2-indep3}

(head-to-tail)

~

$\neg Indep(X,Y)$

$Indep(X,Y|Z)$
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item Head-to-head: $Indep(X, Y)$
 
$P(X, Y, Z) = P(X)~ P(Y)~ P(Z|X, Y)$

$P(X, Y) = P(X)~ P(Y)~ \sum_Z P(Z|X, Y) = P(X)~ P(Y)$

~

\item Tail-to-tail: $Indep(X, Y|Z)$

$P(X, Y, Z) = P(Z)~ P(X|Z)~ P(Y|Z)$

$P(X, Y|Z) = P(X, Y, Z) / P(Z) = P(X|Z)~ P(Y|Z)$

~

\item Head-to-tail: $Indep(X, Y|Z)$

$P(X, Y, Z) = P(X)~ P(Z|X)~ P(Y|Z)$

$P(X, Y|Z) = \frac{P(X, Y, Z)}{P(Z)} = \frac{P(X, Z)~ P(Y|Z)}{P(Z)} = P(X|Z)~
P(Y|Z)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\textbf{General rules for determining conditional independence in a Bayes net:}

~

\item Given three groups of random variables $X,Y,Z$

~

\cen{$Indep(X,Y|Z) \iff$ every path from $X$ to $Y$ is ``blocked by $Z$''}

~

\item A path is ``blocked by $Z$'' $\iff$ on this path...
\begin{items}
\item $\exists$ a node in $Z$ that is head-to-tail w.r.t.\ the path, or

\item $\exists$ a node in $Z$ that is tail-to-tail w.r.t.\ the path, or

\item $\exists$ another node $A$ which is head-to-head w.r.t.\ the
   path\\ ~~ and neither $A$ nor any of its descendants are in $Z$
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example}{

\hfill{\tiny (Heckermann 1995)}
\show[.6]{vl2-car}

~

\cen{ $Indep(T,F)$? ~~ $Indep(B,F|S)$? ~~ $Indep(B,S|T)$? }

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Causality?}{

%% \item Bayes nets are also called causal networks (Pearl, 1988)

%% \item We know that any joint distribution $P(X,Y)$ can be factorized
%% as
%% \twocol{.4}{.4}{$P(X,Y) = P(X|Y)~ P(Y)$}{}

%% \twocol{.4}{.4}{$P(X,Y) = P(Y|X)~ P(X)$}{}

%% Both Bayes nets are ``correct''

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What can we do with Bayes nets?}{

\item \textbf{Inference:} Given some pieces of information (prior,
  observed variabes) what is the implication (the implied information,
  the posterior) on a non-observed variable

~

\item \textbf{Decision Making:} If utilities and decision variables
  are defined $\to$ compute optimal decisions in probabilistic domains

~

\item \textbf{Learning:}
\begin{items}
\item Fully Bayesian Learning: Inference over parameters (e.g., $\b$)

\item Maximum likelihood training: Optimizing parameters
\end{items}

~

\item \textbf{Structure Learning} (Learning/Inferring the graph structure itself):
Decide which model (which graph structure) fits the data best; thereby
uncovering conditional independencies in the data.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Inference: general meaning}
\slide{Inference}{

\item Inference: Given some pieces of information (prior,
  observed variabes) what is the implication (the implied
  information, the posterior) on a non-observed variable

~

\item In a Bayes Nets: Assume there is three groups of RVs:
\begin{items}
\item $Z$ are observed random variables

\item $X$ and $Y$ are hidden random variables

\item We want to do inference about $X$, not $Y$
\end{items}

~

Given some observed variables $Z$, compute
the\\ \textbf{posterior marginal} $P(X \| Z)$ for some hidden
variable $X$.
\begin{align*}
P(X \| Z)
 &= \frac{P(X, Z)}{P(Z)}
  = \frac{1}{P(Z)}~ \sum_Y P(X,Y,Z)
\end{align*}
where $Y$ are all hidden random variables except for $X$

~

\item Inference requires summing over \emph{(eliminating)} hidden variables.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Holmes \& Watson}{

\item Mr. Holmes lives in Los Angeles. One morning when Holmes leaves
his house, he realizes that his grass is wet. Is it due to rain, or
has he forgotten to turn off his sprinkler?
\begin{items}
\item Calculate $P(R|H)$, $P(S|H)$ and compare these values to the prior probabilities.

\item Calculate $P(R, S|H)$.

   Note: $R$ and $S$ are marginally independent, but conditionally dependent
\end{items}

~

\item Holmes checks Watson's grass, and finds it is also wet.
\begin{items}
\item Calculate $P(R|H,W)$, $P(S|H,W)$

\item This effect is called explaining away
\end{items}

~

{\small JavaBayes: run it from the html page\\
\url{http://www.cs.cmu.edu/~javabayes/Home/applet.html} }

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Holmes \& Watson}{

\show[0.5]{vl2-rain}
\cen{$P(H,W,S,R) = P(H|S,R)~ P(W|R)~ P(S)~ P(R)$}

\tiny
\begin{align*}
P(R|H)
 &= \sum_{W,S} \frac{P(R,W,S,H)}{P(H)}
  = \frac{1}{P(H)} \sum_{W,S} P(H|S,R)~ P(W|R)~ P(S)~ P(R) \\
 &= \frac{1}{P(H)} \sum_S P(H|S,R)~ P(S)~ P(R) \\
\hspace*{-10mm}
P(R\=1 \| H\=1)
 &= \frac{1}{P(H\=1)} (1.0 \cdot 0.2 \cdot 0.1 + 1.0 \cdot 0.2 \cdot
 0.9)
  = \frac{1}{P(H\=1)} 0.2\\
\hspace*{-10mm}
P(R\=0 \| H\=1)
 &= \frac{1}{P(H\=1)} (0.9 \cdot 0.8 \cdot 0.1 + 0.0 \cdot 0.8 \cdot
 0.9)
  = \frac{1}{P(H\=1)} 0.072
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{}{

\item These types of calculations can be automated

$\to$ Variable Elimination Algorithm

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Bavarian dialect}{

~

\show[.05]{bavarian}

\item Two binary random variables(RVs): $B$ (bavarian) and $D$ (dialect)

\item Given:

$P(D,B) = P(D \| B)~ P(B)$

$P(D\=1 \| B\=1) = 0.4$, $P(D\=1 \| B\=0) = 0.01$, $P(B\=1) = 0.15$

~

\item \textbf{Notation:} Grey shading usually indicates ``observed''

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Coin flipping}{

~

\show[.35]{coinFlipping}

\item  One binary RV $H$ (hypothesis), 5 RVs for the coin tosses $d_1,..,d_5$

\item Given:

$P(D,H) = \prod_i P(d_i \| H)~ P(H)$

$P(H\=1)=\frac{999}{1000}$, 
$P(d_i\=\texttt{H} \| H\=1) = \half$, $P(d_i\=\texttt{H} \| H\=2) = 1$

}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Ridge regression**}{

~

\show[.2]{regression}

\item One multi-variate RV $\b$, $2n$ RVs $x_{1:n}$, $y_{1:n}$ (observed
   data)

\item Given:

$P(D ,\b) = \prod_i \[P(y_i \| x_i,\b)~ P(x_i)\]~ P(\b)$

 $P(\b) =  \NN(\b \| 0,\frac{\s^2}{\lambda})$, $P(y_i \| x_i,\b) = \NN(y_i \| x_i^\T \b, \s^2)$

~

\item \textbf{Plate notation:} Plates (boxes with index ranges) mean
``copy $n$-times''

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Example: Gaussian Mixture Model**}{

\show[.3]{gaussianMixture}

\item Discrete latent RVs $c_{1:n}$ indicating mixture component,
cont.\ RVs $x_{1:n}$ (observed data)

\item Model: ~ $P(x_i \| \m_{1:K},\S_{1:K}) = \sum_{k=1}^K \NN(x_i \| \m_k,\S_k)~ P(c_i\=k)$

}



%% \slide{Number of parameters}{

%% \item Representing $P(H,W,S,R)$ as big array of probabilities

%% $\to$ ~ $2^4-1 = 15$ parameters

%% ~

%% \item Representing $P(H|S,R)~ P(W|R)~ P(S)~ P(R)$ as 4 separate
%% CPTs

%% $\to$ ~ $2^2 + 2 + 1 + 1 = 8$ parameters

%% ~\mypause

%% \emph{Conditional independences reduces the number of
%% parameters $\to$ this is reflected directly also in the Bayes Net.}

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Inference Methods in Graphical Models}{
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Inference in graphical models: overview}
\slide{Inference methods in graphical models}{

\item \textbf{Message passing:}
\begin{items}
\item Exact inference on trees (includes the Junction Tree Algorithm)

\item Belief propagation
\end{items}

~

\item \textbf{Sampling:}
\begin{items}
\item Rejection samping, importance sampling, Gibbs sampling

\item More generally, Markov-Chain Monte Carlo (MCMC) methods
\end{items}

~

\item \textbf{Other approximations/variational methods}
\begin{items}
\item Expectation propagation

\item Specialized variational methods depending on the model
\end{items}

~

\item \textbf{Reductions:}
\begin{items}
\item Mathematical Programming (e.g.\ LP relaxations of MAP)

\item Compilation into Arithmetic Circuits (Darwiche at al.)
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% \slide{Probabilistic Inference \& Optimization}{

%% ~

%% \show[.5]{inferenceAndOptimization}

%% ~

%% \item Note the relation of the Boltzmann distribution $p(x) \propto
%% e^{-f(x)/T}$ with the cost function (energy) $f(x)$ and temperature $T$.

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Variable elimination}
\slide{Variable Elimination}{

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination example}{

~

\onslide<1>{\anchor{200,-10}{\showhs[.7]{vl-elim1}}}%
\onslide<2>{\anchor{200,-10}{\showhs[.7]{vl-elim2}}}%
\onslide<3>{\anchor{200,-10}{\showhs[.7]{vl-elim3}}}%
\onslide<4>{\anchor{200,-10}{\showhs[.7]{vl-elim4}}}%
\onslide<5>{\anchor{200,-10}{\showhs[.7]{vl-elim5}}}%
\onslide<6>{\anchor{200,-10}{\showhs[.7]{vl-elim6}}}%
\onslide<7>{\anchor{200,-10}{\showhs[.7]{vl-elim7}}}%
\onslide<8>{\anchor{200,-10}{\showhs[.7]{vl-elim8}}}%
\onslide<9>{\anchor{200,-10}{\showhs[.7]{vl-elim9}}}%
\onslide<10>{\anchor{200,-10}{\showhs[.7]{vl-elim10}}}%
\onslide<11>{\anchor{200,-10}{\showhs[.7]{vl-elim11}}}%

{\small
$P(x_5)$

$= \sum_{x_1,x_2,x_3,x_4,x_6}
 P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_4|x_2)~ P(x_5|x_3)~
 P(x_6|x_2,x_5)$\hspace*{-10mm}

\mypause

$= \sum_{x_1,x_2,x_3,x_6}
 P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_5|x_3)~
 P(x_6|x_2,x_5)~ \sum_{x_4} \underbrace{P(x_4|x_2)}_{\Fc_1(x_2,x_4)}$\hspace*{-10mm}

\mypause

$= \sum_{x_1,x_2,x_3,x_6}
 P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_5|x_3)~
 P(x_6|x_2,x_5)~ {\muc_1(x_2)}$\hspace*{-10mm}

\mypause

$= \sum_{x_1,x_2,x_3}
 P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_5|x_3)~ {\muc_1(x_2)}~
 \sum_{x_6} \underbrace{P(x_6|x_2,x_5)}_{\Fc_2(x_2,x_5,x_6)}$\hspace*{-10mm}

\mypause

$= \sum_{x_1,x_2,x_3}
 P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_5|x_3)~
 {\muc_1(x_2)~ \muc_2(x_2,x_5)}$\hspace*{-10mm}

\mypause

$= \sum_{x_2,x_3}
 P(x_5|x_3)~
 {\muc_1(x_2)~ \muc_2(x_2,x_5)}~ \sum_{x_1} \underbrace{P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)}_{\Fc_3(x_1,x_2,x_3)}$\hspace*{-10mm}

\mypause

$= \sum_{x_2,x_3}
 P(x_5|x_3)~
 {\muc_1(x_2)~ \muc_2(x_2,x_5)~ \muc_3(x_2,x_3)}$

\mypause

$= \sum_{x_3}
 P(x_5|x_3)~
 \sum_{x_2} \underbrace{\muc_1(x_2)~ \muc_2(x_2,x_5)~ \muc_3(x_2,x_3)}_{\Fc_4(x_2,x_3,x_5)}$

\mypause

$= \sum_{x_3}
 P(x_5|x_3)~
 {\muc_4(x_3,x_5)}$

\mypause

$= \sum_{x_3}
 \underbrace{P(x_5|x_3)~ {\muc_4(x_3,x_5)}}_{\Fc_5(x_3,x_5)}$

\mypause

$= {\muc_5(x_5)}$
}



}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination example -- lessons learnt}{

\item There is a dynamic programming principle behind Variable
Elimination:
\begin{items}
\item For eliminating $X_{5,4,6}$ we use the solution of eliminating
   $X_{4,6}$

\item The ``sub-problems'' are represented by the $F$ terms, their
   solutions by the \emph{remaining $\mu$ terms}

\item We'll continue to discuss this 4 slides later!
\end{items}

~

\item The factorization of the joint
\begin{items}
\item determines in which order Variable Elimination is efficient

\item determines what the terms $F(...)$ and $\mu(...)$ depend
   on
\end{items}

~

\item We can automate Variable Elimination. For the automation, all
that matters is the factorization of the joint.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Factor graph}
\slide{Factor graphs}{

\item In the previous slides we introduces the box $\blbox$ notation to
indicate \emph{terms} that depend on some variables. That's exactly
what factor graphs represent.

~

\item A \textbf{Factor graph} is a
\begin{items}
\item bipartite graph

\item where each circle node represents a random variable $X_i$

\item each box node represents a \textbf{factor} $f_k$, which is a function
   $f_k(X_{\del k})$

\item the joint
probability distribution is given as

$$P(X_{1:n}) = \prod_{k=1}^K f_k(X_{\del k})$$
\end{items}

~

\textbf{Notation:} $\del k$ is shorthand for $\text{Neighbors}(k)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Bayes Net $\to$ factor graph}{

\item Bayesian Network:

\show[.35]{vl3-Jordan-BNet}

\cen{\small$
P(x_{1:6})
 = P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_4|x_2)~ P(x_5|x_3)~ P(x_6|x_2,x_5)
$}

~

\item Factor Graph:

\show[.35]{vl3-Jordan-FG}

\cen{\small$
P(x_{1:6})
 = f_1(x_1,x_2)~ f_2(x_3,x_1)~ f_3(x_2,x_4)~ f_4(x_3,x_5)~ f_5(x_2,x_5,x_6)
$}

~

$\to$ each CPT in the Bayes Net is just a factor (we neglect the
special semantics of a CPT)

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Elimination Algorithm}{

%% ... same example as above -- in terms of a factor graph

%% ~

%% \item Factor Graph:
%% \begin{center}
%% \shows[.5]{vl3-Jordan-FG}
%% \end{center}
%% $\iff
%% P(x_{1:6})
%%  = f(x_1,x_2)~ f(x_3,x_1)~ f(x_2,x_4)~ f(x_3,x_5)~ f(x_2,x_5,x_6)
%% $

%% ~

%% problem: compute $P(x_1,x_6)$

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Elimination Algorithm}{
%% {\small\vspace*{-10mm}
%% \begin{align*}
%% &P(x_1,x_6) \\
%%  &= \sum_{x_2} \sum_{x_3}\sum_{x_4}\sum_{x_5}
%%  f(x_1,x_2)~ f(x_3,x_1)~ f(x_2,x_4)~ f(x_3,x_5)~ f(x_2,x_5,x_6) \\
%%  &= \sum_{x_2} \sum_{x_3}\sum_{x_4}
%%  f(x_1,x_2)~ f(x_3,x_1)~ f(x_2,x_4) \sum_{x_5} \underbrace{\cl_1(x_2,x_3,x_5,x_6)}_{f(x_3,x_5)~ f(x_2,x_5,x_6)} \\
%%  &= \sum_{x_2} \sum_{x_3}\sum_{x_4}
%%  f(x_1,x_2)~ f(x_3,x_1)~ f(x_2,x_4)~ \tmp_1(x_2,x_3,x_6) \\
%%  &= \sum_{x_2} \sum_{x_3}
%%  f(x_1,x_2)~ f(x_3,x_1)~ \tmp_1(x_2,x_3,x_6) \sum_{x_4} \underbrace{\cl_2(x_2,x_4)}_{f(x_2,x_4)} \\
%%  &= \sum_{x_2} \sum_{x_3}
%%  f(x_1,x_2)~ f(x_3,x_1)~ \tmp_1(x_2,x_3,x_6)~ \tmp_2(x_2) \\
%%  &= \sum_{x_2} 
%%  f(x_1,x_2)~  \tmp_2(x_2) \sum_{x_3} \underbrace{\cl_3(x_1,x_2,x_3,x_6)}_{f(x_3,x_1)~ \tmp_1(x_2,x_3,x_6)}  \\
%%  &= \sum_{x_2} 
%%  \underbrace{\cl_4(x_1,x_2,x_6)}_{f(x_1,x_2)~  \tmp_2(x_2)~ \tmp_3(x_1,x_2,x_6)}  \\
%%  &= \tmp_4(x_1,x_6)
%% \end{align*}
%% }
%% \item we can automate this!

%% }


%% \begin{align*}
%% P&(x_1,x_6) \\
%%  &=
%% \sum_{x_2}
%% \sum_{x_3}
%% \sum_{x_4}
%% \sum_{x_5}
%% \sum_{x_6} f(x_1,x_2)~ f(x_1,x_3)~ f(x_2,x_4)~ f(x_3,x_5)~ f(x_2,x_5,x_6) \\
%%  &= 
%% \sum_{x_2} f(x_1,x_2)
%% \sum_{x_3} f(x_1,x_3)
%% \sum_{x_4} f(x_2,x_4)
%% \sum_{x_5} f(x_3,x_5)
%% \sum_{x_6} f(x_2,x_5,x_6) \\
%%  &= 
%% \sum_{x_2} f(x_1,x_2)
%% \sum_{x_3} f(x_1,x_3)
%% \sum_{x_4} f(x_2,x_4)
%% \sum_{x_5} f(x_3,x_5)~ {\color{red}t_6}(x_2,x_5) \\
%%  &= 
%% \sum_{x_2} f(x_1,x_2)
%% \sum_{x_3} f(x_1,x_3)~ {\color{red}t_5}(x_2,x_3)
%% \sum_{x_4} f(x_2,x_4) \\
%%  &= 
%% \sum_{x_2} f(x_1,x_2)~ {\color{red}t_4}(x_2)
%% \sum_{x_3} f(x_1,x_3)~ {\color{red}t_5}(x_2,x_3) \\
%%  &= 
%% \sum_{x_2} f(x_1,x_2)~ {\color{red}t_4}(x_2)~ {\color{red}t_3}(x_1,x_2) \\
%%  &= {\color{red}t_2} (x_1)
%% \end{align*}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination Algorithm}{

\item \texttt{eliminate\_single\_variable}$(F,i)$

\begin{algo}
   \State {\bfseries Input:} list $F$ of factors, variable id $i$
   \State {\bfseries Output:} list $F$ of factors
   \State find relevant subset $\hat F \subseteq F$ of factors coupled to $i$:~
   $\hat F = \{k: i \in \del k\}$
   \State create new factor $\hat k$ with neighborhood $\del \hat k$ = all variables in $\hat F$ except $i$
   \State compute $\mu_{\hat k}(X_{\del \hat k})
   = \sum_{X_i} \prod_{k\in \hat F} f_k(X_{\del k})$
   \State remove old factors $\hat F$ and append new factor $\mu_{\hat k}$ to $F$
   \State return $F$
\end{algo}

~

\item \texttt{elimination\_algorithm}$(F,M)$

\begin{algo}
   \State {\bfseries Input:} list $F$ of factors, tuple $M$ of
   desired output variables ids
   \State {\bfseries Output:} single factor $\m$ over variables $X_M$
   \State define all variables present in $F$:~ $V = \text{vars}(F)$
   \State define variables to be eliminated:~ $E = V \setminus M$
   \State for all $i \in E$:~ \texttt{eliminate\_single\_variable}$(F,i)$
   \State for all remaining factors, compute the product $\m = \prod_{f\in F} f$
   \State return $\m$
\end{algo}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Conditioning on Observed Variables}{

\item Before, we defined the general problem of inference as computing
\begin{align*}
P(X \| Z)
 &= \frac{P(X, Z)}{P(Z)}
  = \frac{1}{P(Z)}~ \sum_Y P(X,Y,Z)
\end{align*}

\item How can we account for conditioning on $Z$?

~

\item Answer: Don't sum over $Z$!

\item More practical answer: Add additional ``one-hot factors'' $\r(Z)$ which is zero for all values of $Z$, except for the observed value, where it is 1.
\begin{items}
\item Add such 'conditioning factors' to the list of factors before calling variable elimination -- the rest is unchanged
\item Adding such 'conditioning factors' makes the overall factor graph non-normalized, exactly by the evidence $P(Z)$
\end{items}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination on trees}{

~

\show[.5]{vl-BP-tree2}

~

The subtrees w.r.t.\ $X$ can be described as

\quad $F_1(Y_{1,8},X) = f_1(Y_8,Y_1)~ f_2(Y_1,X)$

\quad $F_2(Y_{2,6,7},X) = f_3(X,Y_2)~ f_4(Y_2,Y_6)~ f_5(Y_2,Y_7)$

\quad $F_3(Y_{3,4,5},X) = f_6(X,Y_3,Y_4)~ f_7(Y_4,Y_5)$

The joint distribution is:

\quad $P(Y_{1:8},X) = F_1(Y_{1,8},X)~ F_2(Y_{2,6,7},X)~ F_3(Y_{3,4,5},X)$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination on trees}{

~

\show[.5]{vl-BP-tree3}

~

We can eliminate each tree independently. The remaining terms (\textbf{messages}) are:

\quad $\mu_{F_1\to X}(X) = \sum_{Y_{1,8}} F_1(Y_{1,8},X)$

\quad $\mu_{F_2\to X}(X) = \sum_{Y_{2,6,7}} F_2(Y_{2,6,7},X)$

\quad $\mu_{F_3\to X}(X) = \sum_{Y_{3,4,5}} F_3(Y_{3,4,5},X)$

The marginal $P(X)$ is the \textbf{product of subtree messages}

\eqbox{$P(X) = \mu_{F_1\to X}(X)~ \mu_{F_2\to X}(X)~ \mu_{F_3\to
X}(X)$}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Variable Elimination on trees -- lessons learnt}{

\item The ``remaining terms'' $\mu$'s are called \textbf{messages}

Intuitively, \textbf{messages subsume information from a subtree}

~\mypause

\item Marginal = product of messages, $P(X) = \prod_k \mu_{F_k \to
X}$, is very intuitive:
\begin{items}
\item \emph{Fusion of independent information} from the different
   subtrees

\item Fusing independent information $\oto$ multiplying
   probability tables
\end{items}

~\mypause

\item Along a (sub-) tree, messages can be computed recursively


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Belief propagation}
\key{Message passing}
\slide{Message passing}{

\item General equations (\textbf{belief propagation (BP)}) for
recursive message computation ~ (writing $\mu_{k\to i}(X_i)$ instead
of $\mu_{F_k \to X}(X)$):
\begin{empheq}[box=\eqbox]{align*}
\m_{k\to i}(X_i)
% &= \frac{1}{\m_{i\to C}(X_i)} \sum_{X_C\setminus X_i} b_C(X_C) \label{eqBP1}\\
 &= \sum_{X_{\del k\setminus i}}
\underbrace{f_k(X_{\del k}) \prod_{j\in \del k\setminus i}
\quad
\overbrace{\prod_{k' \in \del j\setminus k} \m_{k'\to j}(X_j)}^{\bar\m_{j\to k}(X_j)}}_{\Fc(\text{subtree})}
%%  && \text{(factor-to-variable)}\\
%% \text{where~~}
%% &\bar\m_{j\to k}(X_j)
%% % &= \frac{1}{\m_{C\to i}(X_i)}~ b_i(X_i) \label{eqBP2} \\
%%  = 
%% && \text{(variable-to-factor)}
\end{empheq}
$\prod_{j\in \del k\setminus i}$: branching at factor $k$,
prod.\ over adjacent variables $j$ excl.\ $i$

$\prod_{k' \in \del j\setminus k}$: branching at variable $j$,
prod.\ over adjacent factors $k'$ excl.\ $k$

{\tiny $\bar\m_{j\to k}(X_j)$ are called ``variable-to-factor
messages'': store them for efficiency}


~

\hspace*{-10mm}\twocol{.5}{.6}{
\showh[.9]{vl-BP-tree4}}{\tiny

Example messages:

$\mu_{2\to X} = \sum_{Y_1} f_2(Y_1,X)~ \m_{1\to Y_1}(Y_1)$

$\mu_{6\to X} = \sum_{Y_3,Y_4} f_6(Y_3,Y_4,X)~ \m_{7\to Y_4}(Y_4)~ \m_{8\to Y_4}(Y_4)$

$\mu_{3\to X}$=$\sum_{Y_2} f_3(Y_2,X)~ \m_{4\to Y_2}(Y_2)~ \m_{5\to Y_2}(Y_2)$
}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Constraint propagation is 'boolean' message passing}{

\item Assume all factors are binary $\to$ boolean constraint functions
as for CSP

\item All messages are binary vectors

\item The product of incoming messages indicates the variable's remaining
domain $D_i$ ~ (analogous to the marginal $P(X_i)$)

\item The message passing equations do constraint propagation

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Message passing remarks}{

\item Computing these messages recursively on a tree does nothing
else than Variable Elimination

\cen{$\To P(X_i) = \prod_{k \in \del i} \m_{k\to i}(X_i)$
is the correct posterior marginal}

~

\item However, since it stores all ``intermediate terms'', we can
 compute ANY marginal $P(X_i)$ for any $i$

~

\item Message passing exemplifies how to exploit the
factorization structure of the joint distribution for the algorithmic
implementation

~

\item Note: These are recursive equations. They can be resolved
exactly if and only if the dependency structure (factor graph) is
a tree. If the factor graph had loops, this would be a ``loopy
recursive equation system''...

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Message passing variants}{

%% \item In comparison to sampling, message passing algorithmically
%%  manipulates whole ``local distributions'' instead of only
%%  samples. (Compare esp.\ to Gibbs sampling, where a local sample is
%%  redrawn based on neighboring variables; here a local message is
%%  recomputed based on neighboring messages.)

\item Message passing has many important applications:

~

\begin{items}
\item Many models are actually trees: In particular chains esp.\ \emph{Hidden Markov Models}

~

\item Message passing can also be applied on non-trees ($\oto$ loopy
   graphs) $\to$ approximate inference \emph{(Loopy Belief Propagation)}

~

\item Bayesian Networks can be ``squeezed'' to become
   trees $\to$ exact inference in Bayes Nets! \emph{(Junction Tree Algorithm)}
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Loopy belief propagation}
\slide{Loopy Belief Propagation}{

\renewcommand{\old}{\text{\color{red}old}}
\renewcommand{\new}{\text{\color{red}new}}

\item If the graphical model is not a tree (=has loops):
\begin{items}
\item The recursive message equations cannot be resolved.

\item However, we could try to just iterate them as update equations...
\end{items}

~

\item Loopy BP update equations: \qquad (initialize with $\m_{k\to i}=1$)

\begin{align*}
\m_{k\to i}^\new(X_i)
 &= \sum_{X_{\del k\setminus i}}
f_k(X_{\del k})~
\prod_{j\in \del k\setminus i}
\quad
\prod_{k' \in \del j\setminus k} \m_{k'\to j}^\old(X_j)
\end{align*}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Loopy BP remarks}{

\tiny

\item Problem of loops intuitively:

\cen{loops $\To$ branches of a node to not represent independent information!}

-- BP is multiplying (=fusing) messages from dependent sources of
   information

~

\item No convergence guarantee, but if it converges, then to a state
of \textbf{marginal consistency}
$$
\sum_{X_{\del k\setminus i}} b(X_{\del k}) =
\sum_{X_{\del k'\setminus i}} b(X_{\del k'}) =
b(X_i)
$$
and to the minimum of the \textbf{Bethe approximation} of the free
energy (Yedidia, Freeman, \& Weiss, 2001)

~

\item We shouldn't be overly disappointed:
\begin{items}\tiny
\item if BP was exact on loopy graphs we could efficiently solve NP hard
   problems...

\item loopy BP is a \emph{very} interesting approximation to solving an
   NP hard problem
\end{items}

\item Ways to tackle the problems with BP convergence:
\begin{items}\tiny
\item Damping (Heskes, 2004: \emph{On the uniqueness of loopy belief
propagation fixed points})

\item CCCP (Yuille, 2002: \emph{CCCP algorithms to minimize the Bethe and
   Kikuchi free energies: Convergent alternatives to belief
   propagation})

\item Tree-reweighted MP (Kolmogorov, 2006: \emph{Convergent
   tree-reweighted message passing for energy minimization})
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Loopy BP application: Markov/Conditional Random Fields for computer vision}{

%% ~

%% \hspace*{-5mm}
%% \showh[.3]{bishop-mrf0}
%% \quad
%% \showh[.3]{bishop-mrf1}
%% \quad
%% \showh[.3]{vl4-mrf}
%% \hspace*{-5mm}

%% ~

%% \item In CRFs in Computer Vision

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{other applications}{

%% ~

%% \item inference for motion segmentation \small (Toussaint, Willert, BMVC 2007)

%% \input figs/v_and_l\quad
%% \includegraphics[width=.3\columnwidth]{bmvc/flower1}\quad
%% \includegraphics[width=.3\columnwidth]{bmvc/flower3}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Maximum a-posteriori (MAP) inference**}
\slide{Maximum a-posteriori (MAP) inference**}{

\small

\item Often we want to compute the most likely global assignment
$$X_{1:n}^\text{MAP} = \argmax_{X_{1:n}} P(X_{1:n})$$ of all random
variables. This is called MAP inference and can be solved by replacing
all $\sum$ by $\max$ in the message passing equations -- the algorithm
is called \textbf{Max-Product Algorithm} and is a
generalization of Dynamic Programming methods like \emph{Viterbi}
or \emph{Dijkstra}.

~

\item Application: \textbf{Conditional Random Fields} 
\begin{align*}
f(y,x)
&= \phi(y,x)^\T \b = \sum_{j=1}^k \phi_j(y_{\del j},x) \b_j 
 = \log\[ \prod_{j=1}^k e^{\phi_j(y_{\del j},x) \b_j} \] \\
\text{with prediction~}
&x \mapsto y^*(x) = \argmax_y f(x,y)
\end{align*}
Finding the $\argmax$ is a MAP inference problem! This is frequently
needed in the innerloop of CRF learning algorithms.


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Conditional random field**}
\slide{Conditional Random Fields**}{

\item The following are interchangable:

\cen{``Random Field'' $\oto$ ``Markov Random Field'' $\oto$ Factor
Graph}

~

\item Therefore, a CRF is a conditional factor graph:
\begin{items}
\item \emph{A CRF defines a
mapping from input $x$ to a factor graph over $y$}
\item Each feature $\phi_j(y_{\del j},x)$ depends only on a subset
$\del j$ of variables $y_{\del j}$
\item If $y_{\del j}$ are discrete, a feature $\phi_j(y_{\del j},x)$ is usually
an indicator feature (see lecture 03); the corresponding parameter
$\b_j$ is then one entry of a factor $f_k(y_{\del j})$ that couples
these variables
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Junction Tree Algorithm**}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Junction tree algorithm**}
\slide{Junction Tree Algorithm**}{

~

\item Many models have loops

\medskip{\small

Instead of applying loopy BP in the hope of getting a good approximation,
it is possible to convert every model into a tree by redefinition of
RVs. The Junction Tree Algorithms converts a loopy model into a tree.
}

~

\item Loops are resolved by defining larger variable groups
(\emph{separators}) on which messages are defined

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Junction Tree Example}{

\item Example:

\centerline{
\showhs{vl4-JT1}
\qquad
\showhs{vl4-JT2}
}

~

\item Join variable $B$ and $C$ to a single \textbf{separator}

\show[.25]{vl4-JT3}

This can be viewed as a variable substitution: rename the tuple
 $(B,C)$ as a single random variable

~

\item A single random variable may be part of multiple separators -- but
only along a \emph{running intersection}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Junction Tree Algorithm}{

\item Standard formulation: ~ \emph{Moralization \& Triangulation}

A \textbf{clique} is a fully connected subset of nodes in a graph.

1) Generate the factor graph (classically called ``moralization'')

2) Translate each factor to a clique: Generate the undirected graph

~~ where undirected edges connect all RVs of a factor

3) Triangulate the undirected graph. (This is the critical step!)

4) Translate each clique back to a factor; identify the separators

~~ between factors

~

\item Formulation in terms of variable elimination for a \emph{given} variable order:

1) Start with a factor graph

2) Choose an order of variable elimination (This is decided implicitly by trangulation above)

3) Keep track of the ``remaining $\mu$ terms'' (slide 14): which RVs

~~ would they depend on? $\to$ this identifies the separators

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Junction Tree Algorithm Example}{

~

\twocol{.5}{.5}{
\show{vl3-Jordan-FG2}
}{
\show{vl3-Jordan-JT}
}

~

~

\item If we eliminate in order 4, 6, 5, 1, 2, 3, we get remaining terms
$$(X_2),~ (X_2,X_5),~ (X_2,X_3),~ (X_2,X_3),~ (X_3)$$

which translates to the Junction Tree on the right


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Summary -- what we covered}{

%% \item Basic sampling methods for inference (rejection, importance,
%% Gibbs)

%% \item Message passing for
%% \begin{items}
%% \item exact inference on (junction) trees

%% \item approximate inference (``loopy BP'') on loopy graphs
%% \end{items}

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\sublecture{Sampling**}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Sampling}{

\item Read:

Andrieu
et al: \emph{An Introduction to MCMC for Machine Learning} (Machine
Learning, 2003)

~

\item Here I'll discuss only thee basic methods:
\begin{items}
\item Rejection sampling

\item Importance sampling

\item Gibbs sampling
\end{items}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Monte Carlo}
\slide{Monte Carlo methods}{

\item General, the term \emph{Monte Carlo simulation} refers to methods
that generate many i.i.d.\ random samples $x_i\sim P(x)$ from a
distribution $P(x)$. Using the samples one can estimate expectations
of anything that depends on $x$, e.g.\ $f(x)$:
$$\<f\> ~=~ \int_x P(x)~ f(x)~ dx ~\approx~ \frac{1}{N} \sum_{i=1}^N
f(x_i)$$

(In this view, Monte Carlo approximates an integral.)

~

\item Example: What is the probability that a solitair would come out
successful? (Original story by Stan Ulam.) Instead of trying to
analytically compute this, generate many random solitairs and count.

~

\item The method developed in the 40ies, where computers became
faster. Fermi, Ulam and von Neumann initiated the idea. von Neumann
called it ``Monte Carlo'' as a code name.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \key{Rejection sampling}
%% \slide{Rejection Sampling}{

%% \item How can we generate i.i.d.\ samples $x_i\sim p(x)$?

%% \item Assumptions:
%% \begin{items}
%% \item We can sample $x\sim q(x)$ from a simpler distribution $q(x)$
%% (e.g., uniform), called \textbf{proposal distribution}
%% \item We can numerically evaluate $p(x)$ for a specific $x$ (even if
%% we don't have an analytic expression of $p(x)$)
%% \item There exists $M$ such that $\forall_x: p(x) \le M q(x)$ (which
%% implies $q$ has larger or equal support as $p$)
%% \end{items}

%% ~

%% \item Rejection Sampling:
%% \begin{items}
%% \item Sample a candiate $x \sim q(x)$
%% \item With probability $\frac{p(x)}{M q(x)}$ accept $x$ and add to
%% $\SS$; otherwise reject
%% \item Repeat until $|\SS|=n$
%% \end{items}

%% \item This generates an unweighted sample set $\SS$ to approximate $p(x)$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Rejection sampling}
\slide{Rejection Sampling}{

\item We have a Bayesian Network with RVs $X_{1:n}$, some of which are
observed: $X_{obs}=y_{obs}$, $obs \subset \{1:n\}$

~

\item The goal is to compute marginal posteriors $P(X_i \|
   X_{obs}=y_{obs})$ conditioned on the observations.

~

\item We generate a set of $K$ (joint) samples of all 
variables
$$\SS = \{ x^k_{1:n} \}_{k=1}^K$$
Each sample $x^k_{1:n} = (x^k_1,x^k_2,..,x^k_n)$ is a list of instantiation of all RVs.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Rejection Sampling}{

\item To generate a single sample $x^k_{1:n}$:

\begin{enumerate}
\item Sort all RVs in topological order; start with $i=1$

\item Sample a value $x^k_i \sim P(X_i \| x^k_{\text{Parents}(i)})$ for the
$i$th RV conditional to the previous samples $x^k_{1:i\1}$

\item If $i \in obs$ compare the sampled value $x^k_i$ with the observation
$y_i$. \emph{Reject} and repeat from a) if the sample is not equal to
the observation.

\item Repeat with $i\gets i+1$ from 2.
\end{enumerate}

\item We compute the marginal probabilities from the sample set $\SS$:
$$P(X_i\=x \| X_{obs}=y_{obs}) \approx \frac{\text{count}_\SS(x^k_i =
 x)}{K}$$ or pair-wise marginals:
$$P(X_i\=x, X_j\=x' \| X_{obs}=y_{obs}) \approx \frac{\text{count}_\SS(x^k_i =
 x \wedge x^k_j =
 x')}{K}$$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Importance sampling}{

%% \item Assumptions:
%% \begin{items}
%% \item We can sample $x\sim q(x)$ from a simpler distribution $q(x)$
%% (e.g., uniform)
%% \item We can numerically evaluate $p(x)$ for a specific $x$ (even if
%% we don't have an analytic expression of $p(x)$)
%% \end{items}

%% \item Importance Sampling:
%% \begin{items}
%% \item Sample a candiate $x \sim q(x)$
%% \item Add the weighted sample $(x,\frac{p(x)}{q(x)})$ to $\SS$
%% \item Repeat $n$ times
%% \end{items}

%% \item This generates an weighted sample set $\SS$ to approximate
%% $p(x)$

%% The weights $w_i = \frac{p(x_i)}{q(x_i)}$ are
%% called \textbf{importance weights}

%% ~

%% \item Crucial for efficiency: a good choice of the proposal $q(x)$

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Importance Sampling}
\slide{Importance Sampling (with likelihood weighting)}{

\item Rejecting whole samples may become very inefficient in large
Bayes Nets!

~

\item New strategy: We generate a \textbf{weighted} sample set
%
$$\SS = \{ (x^k_{1:n}, w^k) \}_{k=1}^K$$
%
where each sample $x^k_{1:n}$ is associated with a weight $w^k$

~

\item In our case, we will choose the weights proportional to the
likelihood $P(X_{obs}=y_{obs} \| X_{1:n}\=x^k_{1:n})$ of the
observations conditional to the sample $x^k_{1:n}$

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Importance Sampling}{

\item To generate a single sample $(w^k,x^k_{1:n})$:

\begin{enumerate}
\item Sort all RVs in topological order; start with $i=1$ and $w^k=1$

\item a) If $i\not\in obs$, sample a value $x^k_i \sim P(X_i \|
x^k_{\text{Parents}(i)})$ for the $i$th RV conditional to the previous
samples $x^k_{1:i\1}$

b) If $i\in obs$, set the value $x^k_i = y_i$ and update the weight
according to likelihood $$w^k \gets w^k~ P(X_i\=y_i\|x^k_{1:i\1})$$

\item Repeat with $i\gets i+1$ from 2.
\end{enumerate}


\item We compute the marginal probabilities as:
$$P(X_i\=x \| X_{obs}=y_{obs}) \approx \frac{\sum_{k=1}^K w^k [x^k_i = x]}{\sum_{k=1}^K w^k}$$
and likewise pair-wise marginals, etc.\\
Notation: $[expr] = 1$ if $expr$ is true and zero otherwise


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\key{Gibbs Sampling}
\slide{Gibbs Sampling**}{

\item In Gibbs sampling we also generate a sample set $\SS$ -- but in
this case the samples are not independent from each other. The
next sample ``modifies'' the previous one:

\item First, all observed RVs are \emph{clamped} to their fixed value
$x^k_i = y_i$ for any $k$.

\item To generate the $(k+1)$th sample, iterate through the latent
variables $i \not\in obs$, updating:
\begin{align*}
x_i^{k+1}
 &\sim P(X_i \| x^k_{1:n\setminus i}) \\
 &\sim P(X_i \| x^k_1, x^k_2,.., x^k_{i\1}, x^k_{i\po}, .., x^k_n) \\
 &\sim P(X_i \| x^k_{\text{Parents}(i)})~ \prod_{j:i\in\text{Parents}(j)}
 P(X_j\=x^k_j \| X_i, x^k_{\text{Parents}(j)\setminus i})
\end{align*}

That is, each $x_i^{k+1}$ is \emph{resampled} conditional to the other
(neighboring) current sample values.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Gibbs Sampling**}{

\item As for rejection sampling, Gibbs sampling generates an
unweighted sample set $\SS$ which can directly be used to compute
marginals.

In practice, one often discards an initial set of samples (burn-in) to
avoid starting biases.

~

\item Gibbs sampling is a special case of \textbf{MCMC sampling.}

Roughly, MCMC means to invent a sampling process, where the next
sample may stochastically depend on the previous (Markov
property), \emph{such that} the final sample set is guaranteed to
correspond to $P(X_{1:n})$.

$\to$ \emph{An Introduction to MCMC for Machine
Learning}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{Sampling -- conclusions}{

\item Sampling algorithms are very simple, very general and very popular
\begin{items}
\item they equally work for continuous \& discrete RVs

\item one only needs to ensure/implement the ability to sample from conditional distributions, no further algebraic manipulations

\item MCMC theory can reduce required number of samples
\end{items}

~

\item In many cases exact and more efficient approximate inference is
possible by actually computing/manipulating whole distributions in the
algorithms instead of only samples.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% -- Expectation propagation

%% Variable Elimination (to compute only a single marginal)

%% -- 

%% \item if model is a tree: ~ inference in time
%%   linear in the number of nodes (Pearl, 1986); messages are passed up
%%   and down the tree; all the necessary computations can be carried out
%%   locally. HMMs (chains) are a special case of trees. Pearlâ€™s method
%%   also applies to polytrees (DAGS with no undirected cycles)

%% \item if model is not a tree: {\color{blue}clustering (grouping) of nodes to yield a
%%   tree of cliques (junction tree)} (Lauritzen and Spiegelhalter, 1988)
%% \end{items}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slide{What we didn't cover}{\label{lastpage}

~

\item A very promising line of research is solving inference problems
using mathematical programming. This unifies research in the areas of
optimization, mathematical programming and probabilistic inference.

\medskip\tiny

Linear Programming relaxations of MAP inference and CCCP methods are
great examples.

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\slidesfoot

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Belief Propagation remarks}{

%% \item Belief Propagation is also called \textbf{Sum-Product
%% algorithm} or \textbf{Message passing}.

%% ~

%% \item When replacing all $\sum$ by $\max$ in the BP equations, then we
%% get the so-called \textbf{Max-Product algorithm}

%% -- Max-Product does not compute marginal probabilities $P(X_i)$ but
%%    the global MAP $\argmax_{X_{1:n}} P(X_{1:n})$, that is, the most
%%    likely of all global configurations of all random variables.


%% ~

%% \item When computing in the log-domain, that is, compute log-messages
%%    $\log \mu$ instead of messages, the Max-Product algorithm becomes
%%    a \textbf{Max-Sum algorithm}

%% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{example}{

%% \centerline{\input{figs/vl2-asia}}

%% -- eliminate in order $D,B,S,L,A,T,X,E$

%% -- eliminate in order $E,$... (not good)

%% -- eliminate in order $D,X,A,S,B,L,T,E$

%% ~

%% \item on the Junction Tree, we can use BP (the special case
%%    factor-to-factor message equations) to do exact inference.


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{Variable Elimination example}{

%% ~

%% \shows[.7]{vl3-Jordan-BNet}
%% $$
%% P(x_{1:6})
%%  = P(x_1)~ P(x_2|x_1)~ P(x_3|x_1)~ P(x_4|x_2)~ P(x_5|x_3)~ P(x_6|x_2,x_5)
%% $$

%% ~

%% problem: compute $P(x_5)$

%% %% {\tiny

%% %% In the exercises: compute $P(x_1,x_6)$

%% %% }

%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \slide{digression: additive decomposable functions}{

%% \small

%% \item graphical models describe how a joint factors

%% -- factorization corresponds to independence (by def)

%% -- factors correspond to ``directly coupled/interacting'' variables

%% ~

%% \item take the neg-log of the joint:
%% \begin{align*}
%% E(X_{1:n})
%%  &:= -\log P(X_{1:n}) = \sum_{i=1}^k \phi_i(X_{C_i})
%%  \quad\text{with}\quad \phi_i =-\log f_i
%% \end{align*}
%% assigns an \emph{error} or \emph{energy} to every possible
%% configuration $x_{1:n}$

%% [Physics: at temperature $T$ an ensemble of particles is distributed as
%% $P(X_{1:n})\propto \exp(-E(X_{1:n})/T)$]

%% ~

%% \item $E(X_{1:n})$ is an additive decomposable function!

%% -- optimization: find $\argmin_{X_{1:n}} E(X_{1:n})$ 

%% -- additive decomposition makes optimization easier

%% -- expresses independence in the sense of optimization

%% -- optimization of $E$ closely related to inference in $P$

%% }

