{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q langchain==0.0.152"
      ],
      "metadata": {
        "id": "9kbI6R5VbO65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d507f9a3-4781-4899-b435-a9bebfd1a588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m666.9/666.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6_crg9aaygO",
        "outputId": "2e1ca147-8bd9-4f43-b853-a790cea8c7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/270.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m194.6/270.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.1/270.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 install -q pyllamacpp==1.0.7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentencepiece"
      ],
      "metadata": {
        "id": "FN7-b5i4dXie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95514289-425e-40d7-eb0d-2a4dbf15def3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m1.2/1.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import GPT4All\n",
        "from langchain.callbacks.base import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "uelopmxka9Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "Cf_4IWlTbV28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'  # replace with your desired local file path"
      ],
      "metadata": {
        "id": "0M8WzOr2br0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "local_path = './models/gpt4all-lora-quantized-ggml.bin'\n",
        "Path(local_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Example model. Check https://github.com/nomic-ai/pyllamacpp for the latest models.\n",
        "url = 'https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized-ggml.bin'\n",
        "# url = 'http://gpt4all.io/models/ggml-gpt4all-l13b-snoozy.bin'\n",
        "\n",
        "# send a GET request to the URL to download the file. Stream since it's large\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# open the file in binary mode and write the contents of the response to it in chunks\n",
        "# This is a large file, so be prepared to wait.\n",
        "with open(local_path, 'wb') as f:\n",
        "    for chunk in tqdm(response.iter_content(chunk_size=8192)):\n",
        "        if chunk:\n",
        "            f.write(chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doxYYzl2bbzf",
        "outputId": "68e12e6d-cf8d-4670-a4f8-a46500b28269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "514266it [01:37, 5285.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ggerganov/llama.cpp#using-gpt4all"
      ],
      "metadata": {
        "id": "cwkToQRwo2rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp.git\n",
        "!cd llama.cpp && git checkout 2b26469"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElQJOhoWlMLZ",
        "outputId": "1666a53b-7a01-468b-c788-4ccdd0a6f4cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 4187, done.\u001b[K\n",
            "remote: Counting objects: 100% (1748/1748), done.\u001b[K\n",
            "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
            "remote: Total 4187 (delta 1654), reused 1573 (delta 1553), pack-reused 2439\u001b[K\n",
            "Receiving objects: 100% (4187/4187), 3.64 MiB | 15.72 MiB/s, done.\n",
            "Resolving deltas: 100% (2836/2836), done.\n",
            "Note: switching to '2b26469'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 2b26469 convert.py: Support models which are stored in a single pytorch_model.bin (#1469)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 llama.cpp/convert.py ./models/gpt4all-lora-quantized-ggml.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H23sS2bmSvI",
        "outputId": "52a43a78-3c31-432d-9692-5a4ec070c074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file models/gpt4all-lora-quantized-ggml.bin\n",
            "Writing vocab...\n",
            "[  1/291] Writing tensor tok_embeddings.weight                  | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  2/291] Writing tensor norm.weight                            | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  3/291] Writing tensor output.weight                          | size  32001 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  4/291] Writing tensor layers.0.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  5/291] Writing tensor layers.0.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  6/291] Writing tensor layers.0.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  7/291] Writing tensor layers.0.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[  8/291] Writing tensor layers.0.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[  9/291] Writing tensor layers.0.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 10/291] Writing tensor layers.0.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 11/291] Writing tensor layers.0.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 12/291] Writing tensor layers.0.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 13/291] Writing tensor layers.1.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 14/291] Writing tensor layers.1.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 15/291] Writing tensor layers.1.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 16/291] Writing tensor layers.1.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 17/291] Writing tensor layers.1.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 18/291] Writing tensor layers.1.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 19/291] Writing tensor layers.1.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 20/291] Writing tensor layers.1.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 21/291] Writing tensor layers.1.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 22/291] Writing tensor layers.2.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 23/291] Writing tensor layers.2.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 24/291] Writing tensor layers.2.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 25/291] Writing tensor layers.2.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 26/291] Writing tensor layers.2.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 27/291] Writing tensor layers.2.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 28/291] Writing tensor layers.2.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 29/291] Writing tensor layers.2.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 30/291] Writing tensor layers.2.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 31/291] Writing tensor layers.3.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 32/291] Writing tensor layers.3.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 33/291] Writing tensor layers.3.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 34/291] Writing tensor layers.3.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 35/291] Writing tensor layers.3.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 36/291] Writing tensor layers.3.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 37/291] Writing tensor layers.3.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 38/291] Writing tensor layers.3.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 39/291] Writing tensor layers.3.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 40/291] Writing tensor layers.4.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 41/291] Writing tensor layers.4.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 42/291] Writing tensor layers.4.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 43/291] Writing tensor layers.4.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 44/291] Writing tensor layers.4.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 45/291] Writing tensor layers.4.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 46/291] Writing tensor layers.4.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 47/291] Writing tensor layers.4.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 48/291] Writing tensor layers.4.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 49/291] Writing tensor layers.5.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 50/291] Writing tensor layers.5.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 51/291] Writing tensor layers.5.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 52/291] Writing tensor layers.5.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 53/291] Writing tensor layers.5.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 54/291] Writing tensor layers.5.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 55/291] Writing tensor layers.5.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 56/291] Writing tensor layers.5.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 57/291] Writing tensor layers.5.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 58/291] Writing tensor layers.6.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 59/291] Writing tensor layers.6.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 60/291] Writing tensor layers.6.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 61/291] Writing tensor layers.6.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 62/291] Writing tensor layers.6.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 63/291] Writing tensor layers.6.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 64/291] Writing tensor layers.6.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 65/291] Writing tensor layers.6.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 66/291] Writing tensor layers.6.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 67/291] Writing tensor layers.7.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 68/291] Writing tensor layers.7.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 69/291] Writing tensor layers.7.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 70/291] Writing tensor layers.7.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 71/291] Writing tensor layers.7.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 72/291] Writing tensor layers.7.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 73/291] Writing tensor layers.7.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 74/291] Writing tensor layers.7.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 75/291] Writing tensor layers.7.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 76/291] Writing tensor layers.8.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 77/291] Writing tensor layers.8.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 78/291] Writing tensor layers.8.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 79/291] Writing tensor layers.8.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 80/291] Writing tensor layers.8.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 81/291] Writing tensor layers.8.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 82/291] Writing tensor layers.8.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 83/291] Writing tensor layers.8.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 84/291] Writing tensor layers.8.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 85/291] Writing tensor layers.9.attention.wq.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 86/291] Writing tensor layers.9.attention.wk.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 87/291] Writing tensor layers.9.attention.wv.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 88/291] Writing tensor layers.9.attention.wo.weight           | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 89/291] Writing tensor layers.9.attention_norm.weight         | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 90/291] Writing tensor layers.9.feed_forward.w1.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 91/291] Writing tensor layers.9.feed_forward.w2.weight        | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 92/291] Writing tensor layers.9.feed_forward.w3.weight        | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 93/291] Writing tensor layers.9.ffn_norm.weight               | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 94/291] Writing tensor layers.10.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 95/291] Writing tensor layers.10.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 96/291] Writing tensor layers.10.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 97/291] Writing tensor layers.10.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[ 98/291] Writing tensor layers.10.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[ 99/291] Writing tensor layers.10.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[100/291] Writing tensor layers.10.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[101/291] Writing tensor layers.10.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[102/291] Writing tensor layers.10.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[103/291] Writing tensor layers.11.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[104/291] Writing tensor layers.11.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[105/291] Writing tensor layers.11.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[106/291] Writing tensor layers.11.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[107/291] Writing tensor layers.11.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[108/291] Writing tensor layers.11.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[109/291] Writing tensor layers.11.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[110/291] Writing tensor layers.11.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[111/291] Writing tensor layers.11.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[112/291] Writing tensor layers.12.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[113/291] Writing tensor layers.12.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[114/291] Writing tensor layers.12.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[115/291] Writing tensor layers.12.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[116/291] Writing tensor layers.12.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[117/291] Writing tensor layers.12.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[118/291] Writing tensor layers.12.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[119/291] Writing tensor layers.12.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[120/291] Writing tensor layers.12.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[121/291] Writing tensor layers.13.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[122/291] Writing tensor layers.13.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[123/291] Writing tensor layers.13.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[124/291] Writing tensor layers.13.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[125/291] Writing tensor layers.13.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[126/291] Writing tensor layers.13.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[127/291] Writing tensor layers.13.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[128/291] Writing tensor layers.13.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[129/291] Writing tensor layers.13.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[130/291] Writing tensor layers.14.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[131/291] Writing tensor layers.14.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[132/291] Writing tensor layers.14.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[133/291] Writing tensor layers.14.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[134/291] Writing tensor layers.14.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[135/291] Writing tensor layers.14.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[136/291] Writing tensor layers.14.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[137/291] Writing tensor layers.14.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[138/291] Writing tensor layers.14.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[139/291] Writing tensor layers.15.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[140/291] Writing tensor layers.15.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[141/291] Writing tensor layers.15.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[142/291] Writing tensor layers.15.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[143/291] Writing tensor layers.15.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[144/291] Writing tensor layers.15.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[145/291] Writing tensor layers.15.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[146/291] Writing tensor layers.15.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[147/291] Writing tensor layers.15.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[148/291] Writing tensor layers.16.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[149/291] Writing tensor layers.16.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[150/291] Writing tensor layers.16.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[151/291] Writing tensor layers.16.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[152/291] Writing tensor layers.16.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[153/291] Writing tensor layers.16.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[154/291] Writing tensor layers.16.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[155/291] Writing tensor layers.16.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[156/291] Writing tensor layers.16.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[157/291] Writing tensor layers.17.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[158/291] Writing tensor layers.17.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[159/291] Writing tensor layers.17.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[160/291] Writing tensor layers.17.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[161/291] Writing tensor layers.17.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[162/291] Writing tensor layers.17.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[163/291] Writing tensor layers.17.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[164/291] Writing tensor layers.17.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[165/291] Writing tensor layers.17.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[166/291] Writing tensor layers.18.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[167/291] Writing tensor layers.18.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[168/291] Writing tensor layers.18.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[169/291] Writing tensor layers.18.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[170/291] Writing tensor layers.18.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[171/291] Writing tensor layers.18.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[172/291] Writing tensor layers.18.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[173/291] Writing tensor layers.18.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[174/291] Writing tensor layers.18.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[175/291] Writing tensor layers.19.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[176/291] Writing tensor layers.19.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[177/291] Writing tensor layers.19.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[178/291] Writing tensor layers.19.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[179/291] Writing tensor layers.19.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[180/291] Writing tensor layers.19.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[181/291] Writing tensor layers.19.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[182/291] Writing tensor layers.19.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[183/291] Writing tensor layers.19.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[184/291] Writing tensor layers.20.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[185/291] Writing tensor layers.20.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[186/291] Writing tensor layers.20.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[187/291] Writing tensor layers.20.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[188/291] Writing tensor layers.20.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[189/291] Writing tensor layers.20.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[190/291] Writing tensor layers.20.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[191/291] Writing tensor layers.20.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[192/291] Writing tensor layers.20.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[193/291] Writing tensor layers.21.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[194/291] Writing tensor layers.21.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[195/291] Writing tensor layers.21.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[196/291] Writing tensor layers.21.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[197/291] Writing tensor layers.21.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[198/291] Writing tensor layers.21.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[199/291] Writing tensor layers.21.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[200/291] Writing tensor layers.21.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[201/291] Writing tensor layers.21.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[202/291] Writing tensor layers.22.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[203/291] Writing tensor layers.22.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[204/291] Writing tensor layers.22.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[205/291] Writing tensor layers.22.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[206/291] Writing tensor layers.22.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[207/291] Writing tensor layers.22.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[208/291] Writing tensor layers.22.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[209/291] Writing tensor layers.22.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[210/291] Writing tensor layers.22.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[211/291] Writing tensor layers.23.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[212/291] Writing tensor layers.23.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[213/291] Writing tensor layers.23.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[214/291] Writing tensor layers.23.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[215/291] Writing tensor layers.23.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[216/291] Writing tensor layers.23.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[217/291] Writing tensor layers.23.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[218/291] Writing tensor layers.23.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[219/291] Writing tensor layers.23.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[220/291] Writing tensor layers.24.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[221/291] Writing tensor layers.24.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[222/291] Writing tensor layers.24.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[223/291] Writing tensor layers.24.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[224/291] Writing tensor layers.24.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[225/291] Writing tensor layers.24.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[226/291] Writing tensor layers.24.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[227/291] Writing tensor layers.24.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[228/291] Writing tensor layers.24.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[229/291] Writing tensor layers.25.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[230/291] Writing tensor layers.25.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[231/291] Writing tensor layers.25.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[232/291] Writing tensor layers.25.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[233/291] Writing tensor layers.25.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[234/291] Writing tensor layers.25.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[235/291] Writing tensor layers.25.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[236/291] Writing tensor layers.25.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[237/291] Writing tensor layers.25.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[238/291] Writing tensor layers.26.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[239/291] Writing tensor layers.26.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[240/291] Writing tensor layers.26.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[241/291] Writing tensor layers.26.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[242/291] Writing tensor layers.26.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[243/291] Writing tensor layers.26.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[244/291] Writing tensor layers.26.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[245/291] Writing tensor layers.26.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[246/291] Writing tensor layers.26.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[247/291] Writing tensor layers.27.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[248/291] Writing tensor layers.27.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[249/291] Writing tensor layers.27.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[250/291] Writing tensor layers.27.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[251/291] Writing tensor layers.27.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[252/291] Writing tensor layers.27.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[253/291] Writing tensor layers.27.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[254/291] Writing tensor layers.27.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[255/291] Writing tensor layers.27.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[256/291] Writing tensor layers.28.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[257/291] Writing tensor layers.28.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[258/291] Writing tensor layers.28.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[259/291] Writing tensor layers.28.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[260/291] Writing tensor layers.28.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[261/291] Writing tensor layers.28.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[262/291] Writing tensor layers.28.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[263/291] Writing tensor layers.28.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[264/291] Writing tensor layers.28.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[265/291] Writing tensor layers.29.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[266/291] Writing tensor layers.29.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[267/291] Writing tensor layers.29.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[268/291] Writing tensor layers.29.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[269/291] Writing tensor layers.29.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[270/291] Writing tensor layers.29.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[271/291] Writing tensor layers.29.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[272/291] Writing tensor layers.29.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[273/291] Writing tensor layers.29.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[274/291] Writing tensor layers.30.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[275/291] Writing tensor layers.30.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[276/291] Writing tensor layers.30.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[277/291] Writing tensor layers.30.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[278/291] Writing tensor layers.30.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[279/291] Writing tensor layers.30.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[280/291] Writing tensor layers.30.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[281/291] Writing tensor layers.30.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[282/291] Writing tensor layers.30.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[283/291] Writing tensor layers.31.attention.wq.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[284/291] Writing tensor layers.31.attention.wk.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[285/291] Writing tensor layers.31.attention.wv.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[286/291] Writing tensor layers.31.attention.wo.weight          | size   4096 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[287/291] Writing tensor layers.31.attention_norm.weight        | size   4096           | type UnquantizedDataType(name='F32')\n",
            "[288/291] Writing tensor layers.31.feed_forward.w1.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[289/291] Writing tensor layers.31.feed_forward.w2.weight       | size   4096 x  11008  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[290/291] Writing tensor layers.31.feed_forward.w3.weight       | size  11008 x   4096  | type QuantizedDataType(groupsize=32, have_addends=False, have_g_idx=False)\n",
            "[291/291] Writing tensor layers.31.ffn_norm.weight              | size   4096           | type UnquantizedDataType(name='F32')\n",
            "Wrote models/ggml-model-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "CkTpiTbobtOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model=\"./models/ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True)"
      ],
      "metadata": {
        "id": "y-nZRMRUd2OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "N6VRxCcoioOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentation Example"
      ],
      "metadata": {
        "id": "YoHI4dUsrQd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\""
      ],
      "metadata": {
        "id": "9GXwzxf3n2L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "v8yIT-pDn5Yt",
        "outputId": "3596d4ad-9ca4-4429-deaa-126659199184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: What NFL team won the Super Bowl in the year Justin Bieber was born?\n",
            "\n",
            "Answer: Let's think step by step. First, we need to identify when JB (Justin Beiber) was born - May 1st, 1988 according to his Wikipedia profile page. Then, since NFL team refers to a Football or American football team in professional National Football League league that plays during the season and is considered as one of the major sports leagues from America which has won Super Bowl on Justin Beiber's birth year; we need to look for teams winning Super Bowls before his birth.\n",
            "As per Wikipedia, \"The 2019 NFL Draft will be held April 25–37 in Nashville\". Since JB was born in May of the same year as draft (the month and day are both zero), we can confidently conclude that he must have been alive for at least one Super Bowl win by an NFL team. However, since his birth occurred after a couple of teams winning their first ever Super Bowls such as Denver Broncos' victory in 1978 & Dallas Cowboys’ triumph over Miami Dolphins in the same year; we can only guess that JB might have been alive for any one among those mentioned NFL Teams who won Super Bowl after his birth.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Question: What NFL team won the Super Bowl in the year Justin Bieber was born?\\n\\nAnswer: Let\\'s think step by step. First, we need to identify when JB (Justin Beiber) was born - May 1st, 1988 according to his Wikipedia profile page. Then, since NFL team refers to a Football or American football team in professional National Football League league that plays during the season and is considered as one of the major sports leagues from America which has won Super Bowl on Justin Beiber\\'s birth year; we need to look for teams winning Super Bowls before his birth.\\nAs per Wikipedia, \"The 2019 NFL Draft will be held April 25–37 in Nashville\". Since JB was born in May of the same year as draft (the month and day are both zero), we can confidently conclude that he must have been alive for at least one Super Bowl win by an NFL team. However, since his birth occurred after a couple of teams winning their first ever Super Bowls such as Denver Broncos\\' victory in 1978 & Dallas Cowboys’ triumph over Miami Dolphins in the same year; we can only guess that JB might have been alive for any one among those mentioned NFL Teams who won Super Bowl after his birth.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Poetic Example"
      ],
      "metadata": {
        "id": "561e16I_q7-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Write a poem about friendship that rhymes.\""
      ],
      "metadata": {
        "id": "R-AEqJ8TBcKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "vxGIzeR7Bc7O",
        "outputId": "e75789ca-650e-4a25-f100-156202ee4c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: Write a poem about friendship that rhymes.\n",
            "\n",
            "Answer: Let's think step by step. First, let me tell you what it means to be friends with someone who is kind and caring like the sunshine on my face when we meet again after our long separation!  This could be considered as one of those rare gifts that makes life worthwhile in this world full of selfishness and dangers. Our friendship will shatter all kinds of challenges or barriers, for I believe there is always hope till the last breath has been breathed out. And we'll share tears together through good times as well as hardships like brave soldiers who are willing to take risks because they value our precious lives dearly! So friends this poem that rhymed may not be perfect but it surely says everything I have in mind for you all my life long, so please enjoy and let's cherish each other always."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: Write a poem about friendship that rhymes.\\n\\nAnswer: Let's think step by step. First, let me tell you what it means to be friends with someone who is kind and caring like the sunshine on my face when we meet again after our long separation!  This could be considered as one of those rare gifts that makes life worthwhile in this world full of selfishness and dangers. Our friendship will shatter all kinds of challenges or barriers, for I believe there is always hope till the last breath has been breathed out. And we'll share tears together through good times as well as hardships like brave soldiers who are willing to take risks because they value our precious lives dearly! So friends this poem that rhymed may not be perfect but it surely says everything I have in mind for you all my life long, so please enjoy and let's cherish each other always.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mother's day Example"
      ],
      "metadata": {
        "id": "SI8ADce7rJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Write a social media post to celebrate mother's day.\""
      ],
      "metadata": {
        "id": "7CWDAKTKJm8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "tPIl8hhYJneV",
        "outputId": "3b3398b6-8da7-40ef-c2ff-5868cf915fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: Write a social media post to celebrate mother's day.\n",
            "\n",
            "Answer: Let's think step by step. First, we need an attractive and catchy headline that will make your mom feel appreciated on this special occasion! Here are some examples of what you could write as the heading:\"Mommy, You Are The Best Thing That Has Ever Happened To Me\" or \"I Am So Lucky to Have a Mother Like YOU\".\n",
            "Now it's time for an introduction. Write about your mom’s accomplishments and achievements that make her special:“My mother has the most contagious laugh I have ever heard, she can turn any ordinary day into a happy one.” or “Mommy, You are always there to listen when we need you”\n",
            "Then comes the body of your post. This is where you'll share all those adorable childhood memories with her and show how much they mean to both of you:“My mom has taught me so many things that I would never have learned otherwise.” or “I can’t imagine a day without my Mommy”\n",
            "In conclusion, write something heartfelt about your mother. Express all the love in our hearts for her and say thank you as well because she is always there to support us:“My mom has taught me what uncond"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Question: Write a social media post to celebrate mother\\'s day.\\n\\nAnswer: Let\\'s think step by step. First, we need an attractive and catchy headline that will make your mom feel appreciated on this special occasion! Here are some examples of what you could write as the heading:\"Mommy, You Are The Best Thing That Has Ever Happened To Me\" or \"I Am So Lucky to Have a Mother Like YOU\".\\nNow it\\'s time for an introduction. Write about your mom’s accomplishments and achievements that make her special:“My mother has the most contagious laugh I have ever heard, she can turn any ordinary day into a happy one.” or “Mommy, You are always there to listen when we need you”\\nThen comes the body of your post. This is where you\\'ll share all those adorable childhood memories with her and show how much they mean to both of you:“My mom has taught me so many things that I would never have learned otherwise.” or “I can’t imagine a day without my Mommy”\\nIn conclusion, write something heartfelt about your mother. Express all the love in our hearts for her and say thank you as well because she is always there to support us:“My mom has taught me what uncond'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explain Rain Example"
      ],
      "metadata": {
        "id": "MBGcLlz-qnyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What happens when it rains somewhere?\""
      ],
      "metadata": {
        "id": "zp6qtHUnvS0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "ZVtsVANR52-A",
        "outputId": "7d444386-931a-4f84-ae42-d8c4c8aa3eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: What happens when it rains somewhere?\n",
            "\n",
            "Answer: Let's think step by step. When rain falls, first of all the water vaporizes from clouds and travel to lower altitude where air is denser. Then these drops hit surfaces like land or trees etc., which are considered as a target for this falling particles known as rainfall. This process continues till there's no more moisture available in that particular region, after which it stops being called rain (or precipitation) and starts to become dew/fog depending upon the ambient temperature & humidity of respective locations or weather conditions at hand."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's think step by step. When rain falls, first of all the water vaporizes from clouds and travel to lower altitude where air is denser. Then these drops hit surfaces like land or trees etc., which are considered as a target for this falling particles known as rainfall. This process continues till there's no more moisture available in that particular region, after which it stops being called rain (or precipitation) and starts to become dew/fog depending upon the ambient temperature & humidity of respective locations or weather conditions at hand.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rain Example, but one sentence and funny."
      ],
      "metadata": {
        "id": "-O8NWPP2qqHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's answer in two sentence while being funny.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "D4oD6qpQqtCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "OBJ4q3wPq-U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.run(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "_WKJCcqOrBM0",
        "outputId": "d9f4dc73-216b-49b5-dbf9-60becfe2d15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Question: What happens when it rains somewhere?\n",
            "\n",
            "Answer: Let's answer in two sentence while being funny. 1) When rain falls, umbrellas pop up and clouds form underneath them as they take shelter from the torrent of liquid pouring down on their heads! And...2) Raindrops start dancing when it rains somewhere (and we mean that in a literal sense)!"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Question: What happens when it rains somewhere?\\n\\nAnswer: Let's answer in two sentence while being funny. 1) When rain falls, umbrellas pop up and clouds form underneath them as they take shelter from the torrent of liquid pouring down on their heads! And...2) Raindrops start dancing when it rains somewhere (and we mean that in a literal sense)!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}