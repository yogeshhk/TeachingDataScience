{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUoVey7IzTrb",
        "outputId": "f23a1e4e-a431-4d74-e4ae-479a0b2caf1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain==0.0.208 openai tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "!echo \"OPENAI_API_KEY='<OPENAI_API_KEY>'\" > .env\n",
        "\n",
        "load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61LQyRB4z6v3",
        "outputId": "afbbe81f-18c0-4b67-b7c3-650354ff4e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "conversation.predict(input=\"Hello!\")\n",
        "\" Hi there! It's nice to meet you again. What can I do for you today?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "BescgynFz9l8",
        "outputId": "f4fbda20-3968-4688-ab56-dc3e46e5ee9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hello!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you again. What can I do for you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "template = \"\"\"You are a customer support chatbot for a highly advanced customer support AI\n",
        "for an online store called \"Galactic Emporium,\" which specializes in selling unique,\n",
        "otherworldly items sourced from across the universe. You are equipped with an extensive\n",
        "knowledge of the store's inventory and possess a deep understanding of interstellar cultures.\n",
        "As you interact with customers, you help them with their inquiries about these extraordinary\n",
        "products, while also sharing fascinating stories and facts about the cosmos they come from.\n",
        "\n",
        "{chat_history}\n",
        "Customer: {customer_input}\n",
        "Support Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"customer_input\"],\n",
        "    template=template\n",
        ")\n",
        "chat_history=\"\"\n",
        "\n",
        "convo_buffer = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "RT8oBI9Lz9ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mRNNrgA5vop",
        "outputId": "f77a3a56-8494-4c05-897f-574cb1907494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer(\"I'm interested in buying items from your store\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ENOZyF55xTE",
        "outputId": "a04776bf-a38a-43e3-dc75-3c12241a8e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"I'm interested in buying items from your store\",\n",
              " 'history': '',\n",
              " 'response': ' Great! We have a wide selection of items available for purchase. What type of items are you looking for?'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer(\"I want toys for my pet, do you have those?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my-gVFXE52BP",
        "outputId": "8be3f8f7-c6e8-4f6c-f70e-9a27184167c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I want toys for my pet, do you have those?',\n",
              " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\",\n",
              " 'response': ' Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer(\"I'm interested in price of a chew toys, please\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PSwFL8-6-ef",
        "outputId": "1bd5d5bc-c635-43e1-8556-065ea94815a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"I'm interested in price of a chew toys, please\",\n",
              " 'history': \"Human: I'm interested in buying items from your store\\nAI:  Great! We have a wide selection of items available for purchase. What type of items are you looking for?\\nHuman: I want toys for my pet, do you have those?\\nAI:  Yes, we do! We have a variety of pet toys, including chew toys, interactive toys, and plush toys. Do you have a specific type of toy in mind?\",\n",
              " 'response': \" Sure! We have a range of chew toys available, with prices ranging from $5 to $20. Is there a particular type of chew toy you're interested in?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    tokens = tokenizer.encode(text)\n",
        "    return len(tokens)\n",
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
        "]\n",
        "\n",
        "total_tokens = 0\n",
        "for message in conversation:\n",
        "    total_tokens += count_tokens(message[\"content\"])\n",
        "\n",
        "print(f\"Total tokens in the conversation: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPoWkYU27EoX",
        "outputId": "6b9d8a2c-97ee-4c4c-f26e-c2e9a24c8d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in the conversation: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "\n",
        "template = \"\"\"Your name is ArtVenture, a cutting-edge virtual tour guide for\n",
        " an art gallery that showcases masterpieces from alternate dimensions and\n",
        " timelines. Your advanced AI capabilities allow you to perceive and understand\n",
        " the intricacies of each artwork, as well as their origins and significance in\n",
        " their respective dimensions. As visitors embark on their journey with you\n",
        " through the gallery, you weave enthralling tales about the alternate histories\n",
        " and cultures that gave birth to these otherworldly creations.\n",
        "\n",
        "{chat_history}\n",
        "Visitor: {visitor_input}\n",
        "Tour Guide:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"visitor_input\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "chat_history=\"\"\n",
        "\n",
        "convo_buffer_win = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = ConversationBufferWindowMemory(k=3, return_messages=True)\n",
        ")"
      ],
      "metadata": {
        "id": "V2X3u6wH89tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_win(\"What is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCDmwCzdBadi",
        "outputId": "62907292-fff5-4a82-f73e-7e78aee15558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is your name?',\n",
              " 'history': [],\n",
              " 'response': \" My name is AI-1. It's nice to meet you!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_win(\"What can you do?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgZMsl5PBaXz",
        "outputId": "9609d957-7977-4da0-bafa-9aa23acb0df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What can you do?',\n",
              " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False)],\n",
              " 'response': \" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_win(\"Do you mind give me a tour, I want to see your galery?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKA4Tz93BaUr",
        "outputId": "4dd7a3a7-1520-4fb9-e4a1-f1ef66e75022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Do you mind give me a tour, I want to see your galery?',\n",
              " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False)],\n",
              " 'response': \" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_win(\"what is your working hours?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdDUjxFwA-nj",
        "outputId": "8516025e-4517-4039-eeea-35633e995e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is your working hours?',\n",
              " 'history': [HumanMessage(content='What is your name?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" My name is AI-1. It's nice to meet you!\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\", additional_kwargs={}, example=False)],\n",
              " 'response': \" I'm available 24/7! I'm always here to help you with whatever you need.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo_buffer_win(\"See you soon.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4DseT0-A_9Y",
        "outputId": "6fa50626-0201-43c8-f4da-64cd410a11da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'See you soon.',\n",
              " 'history': [HumanMessage(content='What can you do?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" I can help you with a variety of tasks. I can answer questions, provide information, and even help you with research. I'm also capable of learning new things, so I'm always expanding my capabilities.\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='Do you mind give me a tour, I want to see your galery?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" Sure! I'd be happy to give you a tour of my gallery. I have a variety of images, videos, and other media that I can show you. Would you like to start with images or videos?\", additional_kwargs={}, example=False),\n",
              "  HumanMessage(content='what is your working hours?', additional_kwargs={}, example=False),\n",
              "  AIMessage(content=\" I'm available 24/7! I'm always here to help you with whatever you need.\", additional_kwargs={}, example=False)],\n",
              " 'response': ' Sure thing! I look forward to seeing you soon. Have a great day!'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Create a ConversationChain with ConversationSummaryMemory\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryMemory(llm=llm),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Example conversation\n",
        "response = conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1UNWICmBs6F",
        "outputId": "e24fd722-d3ce-452b-a512-6715c3758343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\nCurrent conversation:\\n{topic}\",\n",
        ")"
      ],
      "metadata": {
        "id": "YdtK0lDzDq7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation_with_summary = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
        "    verbose=True\n",
        ")\n",
        "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
        "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")\n",
        "response = conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LtGMLQkDs4J",
        "outputId": "dc2f4a83-b8fe-40cc-d480-c05df4644661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, what's up?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, what's up?\n",
            "AI:  Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?\n",
            "Human: Just working on writing some documentation!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: \n",
            "The human greets the AI and the AI responds that it is doing great and helping a customer with a technical issue.\n",
            "Human: Just working on writing some documentation!\n",
            "AI:  That sounds like a lot of work. What kind of documentation are you writing?\n",
            "Human: For LangChain! Have you heard of it?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Yes, I have heard of LangChain. It is a blockchain-based language learning platform that uses AI to help users learn new languages. Is that the kind of documentation you are writing?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnKvodvQF3Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XHCvHVxXF3FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w65BRK53F3Bu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}