{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_101/101-2-streamlit_document_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVU23yHni3Ux",
        "outputId": "f012d255-931e-4fc7-82f2-db6a33280c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.6/272.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.3/264.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/huggingface-hub/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain streamlit openai pypdf sentence_transformers docarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-YDsHXxN2DN"
      },
      "source": [
        "# Simple Streamlit Web App\n",
        "\n",
        "This app is based on langchainAI's streamlit repo found here - https://github.com/langchain-ai/streamlit-agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czbI0oXbiw6-",
        "outputId": "3941ccaf-aaff-46c6-ba7d-7883bc3bf650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing doc_search_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile doc_search_app.py\n",
        "import os\n",
        "import tempfile\n",
        "import streamlit as st\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "st.set_page_config(page_title=\"Austin LangChain: Chat with Documents\", page_icon=\"\")\n",
        "st.title(\" Austin LangChain : Chat with Documents\")\n",
        "\n",
        "\n",
        "@st.cache_resource(ttl=\"1h\")\n",
        "def configure_retriever(uploaded_files):\n",
        "    # Read documents\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploaded_files:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Split documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create embeddings and store in vectordb\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectordb = DocArrayInMemorySearch.from_documents(splits, embeddings)\n",
        "\n",
        "    # Define retriever\n",
        "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5, \"fetch_k\": 10})\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "class StreamHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container: st.delta_generator.DeltaGenerator, initial_text: str = \"\"):\n",
        "        self.container = container\n",
        "        self.text = initial_text\n",
        "\n",
        "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
        "        self.text += token\n",
        "        self.container.markdown(self.text)\n",
        "\n",
        "\n",
        "class PrintRetrievalHandler(BaseCallbackHandler):\n",
        "    def __init__(self, container):\n",
        "        self.container = container.expander(\"Context Retrieval\")\n",
        "\n",
        "    def on_retriever_start(self, query: str, **kwargs):\n",
        "        self.container.write(f\"**Question:** {query}\")\n",
        "\n",
        "    def on_retriever_end(self, documents, **kwargs):\n",
        "        # self.container.write(documents)\n",
        "        for idx, doc in enumerate(documents):\n",
        "            source = os.path.basename(doc.metadata[\"source\"])\n",
        "            self.container.write(f\"**Document {idx} from {source}**\")\n",
        "            self.container.markdown(doc.page_content)\n",
        "\n",
        "\n",
        "openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\n",
        "if not openai_api_key:\n",
        "    st.info(\"Please add your OpenAI API key to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "uploaded_files = st.sidebar.file_uploader(\n",
        "    label=\"Upload PDF files\", type=[\"pdf\"], accept_multiple_files=True\n",
        ")\n",
        "if not uploaded_files:\n",
        "    st.info(\"Please upload PDF documents to continue.\")\n",
        "    st.stop()\n",
        "\n",
        "retriever = configure_retriever(uploaded_files)\n",
        "\n",
        "# Setup memory for contextual conversation\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Setup LLM and QA chain\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key, temperature=0, streaming=True\n",
        ")\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm, retriever=retriever, memory=memory, verbose=True\n",
        ")\n",
        "\n",
        "if \"messages\" not in st.session_state or st.sidebar.button(\"Clear message history\"):\n",
        "    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"How can I help you?\"}]\n",
        "\n",
        "for msg in st.session_state.messages:\n",
        "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
        "\n",
        "user_query = st.chat_input(placeholder=\"Ask me anything!\")\n",
        "\n",
        "if user_query:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "    st.chat_message(\"user\").write(user_query)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        retrieval_handler = PrintRetrievalHandler(st.container())\n",
        "        stream_handler = StreamHandler(st.empty())\n",
        "        response = qa_chain.run(user_query, callbacks=[retrieval_handler, stream_handler])\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YeNLTTJSY5NM"
      },
      "outputs": [],
      "source": [
        "!streamlit run doc_search_app.py &>/content/doc_search_app_logs.txt &"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifux-mmzcTQK"
      },
      "source": [
        "## Find the IP of your instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQz0WaTTcTQK",
        "outputId": "dc400fcb-2be9-4912-dbf9-470bb14ca2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.125.212.181\n",
            "Copy this IP into the webpage that opens below\n"
          ]
        }
      ],
      "source": [
        "!curl ipv4.icanhazip.com\n",
        "!echo \"Copy this IP into the webpage that opens below\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XkNhk9abV29"
      },
      "source": [
        "## Expose the Streamlit app on port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DDegT-jZ2Q5",
        "outputId": "8421bc68-4df3-4c94-eaac-61ec84bf0939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 5.512s\n",
            "your url is: https://angry-news-report.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501\n",
        "!echo \"Click on the link, and paste the IP from above to authenticate\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
