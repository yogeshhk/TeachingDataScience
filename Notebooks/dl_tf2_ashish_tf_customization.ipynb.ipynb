{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_Customization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfindiamooc/tfindiamooc.github.io/blob/master/colabs/tf_customization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge8joAeDsBUf",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow Customization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyE0uES0sM8Y",
        "colab_type": "text"
      },
      "source": [
        "So far in this course, we used methods provided by build in TF APIs like tf.Keras and tf.Estimator.  While these constructs are sufficient to start any AI project, there could be situations where you may have to implement custom models, loss functions or metrics.  Tensorflow 2.0 provides support for extending its functionality.  In this module, we will learn how to customize TF 2.0 functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9dYQqTmr6lu",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "40d6b79e-ef4d-4bd6-a552-538a931359bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Import TF 2.0\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbfT1reDuZ66",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: Tensors and Accelerators\n",
        "\n",
        "### Tensors\n",
        "\n",
        "A Tensor is a multi-dimensional array. Similar to NumPy `ndarray` objects, `tf.Tensor` objects have a **data type** and a **shape**. Additionally, `tf.Tensor`s can *reside in accelerator memory (like a GPU)*. TensorFlow offers a rich library of operations ([tf.add](https://www.tensorflow.org/api_docs/python/tf/add), [tf.matmul](https://www.tensorflow.org/api_docs/python/tf/matmul), [tf.linalg.inv](https://www.tensorflow.org/api_docs/python/tf/linalg/inv) etc.) that consume and produce `tf.Tensor`s. These operations automatically convert native Python types, for example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqFEOkBIuyFr",
        "colab_type": "code",
        "outputId": "e6070291-a1be-4028-a67c-c1d038902c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# We can add two scalars\n",
        "print(tf.add(1, 2))   \n",
        "# returns tf.Tensor(3, shape=(), dtype=int32)\n",
        "\n",
        "# or add vectors\n",
        "print(tf.add([1, 2], [3, 4]))  \n",
        "# returns tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
        "\n",
        "# square the number \n",
        "print(tf.square(5))\n",
        "# returns tf.Tensor(25, shape=(), dtype=int32)\n",
        "\n",
        "# sum the elements in the list and return a scalar, which is sum of elements \n",
        "# in the list\n",
        "print(tf.reduce_sum([1, 2, 3]))  # 1 + 2 + 3\n",
        "# returns tf.Tensor(6, shape=(), dtype=int32) \n",
        "\n",
        "# Operator overloading is also supported\n",
        "print(tf.square(2) + tf.square(3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
            "tf.Tensor(25, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(13, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZQe7oYkr_cQ",
        "colab_type": "text"
      },
      "source": [
        "Recall that each tf.Tensor has a shape and a datatype:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6ooqEoBvlgC",
        "colab_type": "code",
        "outputId": "c8ca0739-37af-4dad-dbd7-3706a60cc687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = tf.matmul([[1]], [[2, 3]])\n",
        "print(x)\n",
        "print(\"Shape: %s\"%x.shape)\n",
        "print(\"Data type: %s\"%x.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[2 3]], shape=(1, 2), dtype=int32)\n",
            "Shape: (1, 2)\n",
            "Data type: <dtype: 'int32'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsezjyQ0v3j3",
        "colab_type": "text"
      },
      "source": [
        "The most obvious differences between NumPy arrays and `tf.Tensor`s are:\n",
        "\n",
        "1. Tensors can be backed by accelerator memory (like GPU, TPU).\n",
        "2. Tensors are immutable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wsIajowDhk",
        "colab_type": "text"
      },
      "source": [
        "### NumPy Compatibility\n",
        "\n",
        "Converting between a TensorFlow `tf.Tensor`s and a NumPy `ndarray` is easy:\n",
        "\n",
        "* TensorFlow operations automatically convert NumPy ndarrays to Tensors.\n",
        "* NumPy operations automatically convert Tensors to NumPy ndarrays.\n",
        "\n",
        "Tensors are explicitly converted to NumPy ndarrays using their `.numpy()` method. These conversions are typically cheap since the array and `tf.Tensor` share the underlying memory representation, if possible. However, sharing the underlying representation isn't always possible since the `tf.Tensor` may be hosted in GPU memory while NumPy arrays are always backed by host memory, and the conversion involves a copy from GPU to host memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_N6taDYv3DC",
        "colab_type": "code",
        "outputId": "78bebdf5-124d-452f-9736-ed828476de5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "ndarray = np.ones([3, 3])\n",
        "print (\"Content of array:\")\n",
        "print (ndarray)\n",
        "print ()\n",
        "\n",
        "print(\"TensorFlow operations convert numpy arrays to Tensors automatically\")\n",
        "tensor = tf.multiply(ndarray, 42)\n",
        "print(tensor)\n",
        "print ()\n",
        "\n",
        "print(\"And NumPy operations convert Tensors to numpy arrays automatically\")\n",
        "print(np.add(tensor, 1))\n",
        "print ()\n",
        "\n",
        "print(\"The .numpy() method explicitly converts a Tensor to a numpy array\")\n",
        "print(tensor.numpy())\n",
        "print ()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Content of array:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "\n",
            "TensorFlow operations convert numpy arrays to Tensors automatically\n",
            "tf.Tensor(\n",
            "[[42. 42. 42.]\n",
            " [42. 42. 42.]\n",
            " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
            "\n",
            "And NumPy operations convert Tensors to numpy arrays automatically\n",
            "[[43. 43. 43.]\n",
            " [43. 43. 43.]\n",
            " [43. 43. 43.]]\n",
            "\n",
            "The .numpy() method explicitly converts a Tensor to a numpy array\n",
            "[[42. 42. 42.]\n",
            " [42. 42. 42.]\n",
            " [42. 42. 42.]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-_Wu_yxwx38",
        "colab_type": "text"
      },
      "source": [
        "### GPU acceleration\n",
        "\n",
        "Many TensorFlow operations are accelerated using the GPU for computation. Without any annotations, TensorFlow automatically decides whether to use the GPU or CPU for an operation—copying the tensor between CPU and GPU memory, if necessary. Tensors produced by an operation are typically backed by the memory of the device on which the operation executed, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLICYe0-wypO",
        "colab_type": "code",
        "outputId": "b11bbe47-2333-4dab-bc75-88b0fafd8d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = tf.random.uniform([3, 3])\n",
        "\n",
        "print(\"Is there a GPU available: \"),\n",
        "print(tf.test.is_gpu_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is there a GPU available: \n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZdEIEEw5Kh",
        "colab_type": "text"
      },
      "source": [
        "Check if the tensor is stored in GPU with `tensor.device.endswith` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4qszstjw3jL",
        "colab_type": "code",
        "outputId": "662af8b4-1283-4df6-d3a8-0137dde48fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Is the Tensor on GPU #0:  \"),\n",
        "print(x.device.endswith('GPU:0'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is the Tensor on GPU #0:  \n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kefe8FcxxLXZ",
        "colab_type": "text"
      },
      "source": [
        "### How does it specify device names?\n",
        "\n",
        "The `Tensor.device` property provides a fully qualified string name of the device hosting the contents of the tensor. This name encodes many details, such as (i) an **identifier of the network address** of the host on which this program is executing and (ii) the **device within that host**. This is required for distributed execution of a TensorFlow program. The string ends with `GPU:<N>` if the tensor is placed on the `N`-th GPU on the host."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg8M1o1dxhcC",
        "colab_type": "text"
      },
      "source": [
        "### How can we place a tensor on GPU explicitely?\n",
        "\n",
        "In TensorFlow, *placement* refers to how individual operations are assigned (placed on) a device for execution. As mentioned, when there is no explicit guidance provided, TensorFlow automatically decides which device to execute an operation and copies tensors to that device, if needed. However, TensorFlow operations can be explicitly placed on specific devices using the `tf.device` context manager, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAGX5e-nxKXc",
        "colab_type": "code",
        "outputId": "9f9f40ab-9c01-4b26-f4a4-81afc965f834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "## We will perform matrix multiplication on CPU and GPU and compare the timings.\n",
        "\n",
        "import time\n",
        "\n",
        "# Function for matrix multiplication with itself 10 times.\n",
        "def time_matmul(x):\n",
        "  start = time.time()\n",
        "  \n",
        "  # Perform multiplication 100 times.\n",
        "  for loop in range(100):\n",
        "    tf.matmul(x, x)\n",
        "\n",
        "  result = time.time()-start\n",
        "\n",
        "  print(\"100 loops: {:0.2f}ms\".format(1000*result))\n",
        "\n",
        "# Force execution on CPU \n",
        "print(\"On CPU:\")\n",
        "with tf.device(\"CPU:0\"):\n",
        "  x = tf.random.uniform([1000, 1000])\n",
        "  assert x.device.endswith(\"CPU:0\")\n",
        "  time_matmul(x)\n",
        "\n",
        "print()\n",
        "\n",
        "# Force execution on GPU #0 if available by first placing the tensor on GPU.\n",
        "if tf.test.is_gpu_available():\n",
        "  print(\"On GPU:\")\n",
        "  with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n",
        "    x = tf.random.uniform([1000, 1000])\n",
        "    assert x.device.endswith(\"GPU:0\")\n",
        "    time_matmul(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On CPU:\n",
            "100 loops: 2690.45ms\n",
            "\n",
            "On GPU:\n",
            "100 loops: 4.02ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23o01dSszBSc",
        "colab_type": "text"
      },
      "source": [
        "## Part 2: Customize tf.Keras Sequential API\n",
        "\n",
        "Customization opportunities:\n",
        "* Model\n",
        "* Loss function\n",
        "* Training Loop\n",
        "* Metrics\n",
        "\n",
        "Let's learn how we can carry out these customizations in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb4DK6Tpz4Qh",
        "colab_type": "text"
      },
      "source": [
        "### Writing custom layers\n",
        "\n",
        "Layers provide higher level of abstraction for implementing ML models. Many machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGqC06U60tFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In the tf.keras.layers package, layers are objects. To construct a layer,\n",
        "# simply construct the object. Most layers take as a first argument the number\n",
        "# of output dimensions / channels.\n",
        "layer = tf.keras.layers.Dense(100)\n",
        "\n",
        "# The number of input dimensions is often unnecessary, as it can be inferred\n",
        "# the first time the layer is used, but it can be provided if you want to\n",
        "# specify it manually, which is useful in some complex models.\n",
        "layer = tf.keras.layers.Dense(10, input_shape=(None, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sps4NdM705bP",
        "colab_type": "text"
      },
      "source": [
        "Example layers are Dense (a fully-connected layer),\n",
        "Conv2D, LSTM, BatchNormalization, Dropout, and many others.  The full list can be found in [the documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFa0xbyv1Luq",
        "colab_type": "code",
        "outputId": "5a661873-eaf2-4eb4-cda5-7e3844dc9d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# To use a layer, simply call it.\n",
        "layer(tf.zeros([10, 5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=712, shape=(10, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh1aRQ6f1MMD",
        "colab_type": "code",
        "outputId": "34d45905-4a27-4b21-db2e-0aaadf0d8e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Layers have many useful methods. For example, you can inspect all variables\n",
        "# in a layer using `layer.variables` and trainable variables using\n",
        "# `layer.trainable_variables`. In this case a fully-connected layer\n",
        "# will have variables for weights and biases.\n",
        "layer.variables"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
              " array([[-0.10956973, -0.32350817,  0.33780086, -0.355431  , -0.595714  ,\n",
              "         -0.3596113 , -0.19393778, -0.04922861,  0.22064978, -0.2343215 ],\n",
              "        [-0.06548589, -0.14383852, -0.146968  ,  0.36432648, -0.49617139,\n",
              "         -0.22814971,  0.11680913, -0.54332936,  0.12799722,  0.53414744],\n",
              "        [-0.33322757,  0.31441182, -0.5343494 ,  0.4118405 ,  0.44340605,\n",
              "          0.46069974, -0.23017013, -0.17695966, -0.4530635 , -0.49714383],\n",
              "        [-0.30850297, -0.14670593,  0.3785556 , -0.03918439,  0.43342572,\n",
              "          0.12621808,  0.5813835 , -0.22612536, -0.43294474, -0.54165995],\n",
              "        [ 0.29879034,  0.01249349, -0.5123859 , -0.06109631,  0.14176685,\n",
              "         -0.14670292, -0.23124829,  0.4585678 ,  0.6174125 , -0.52170664]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOL_ycwv1ORr",
        "colab_type": "code",
        "outputId": "4c0bb5ca-8f34-4336-dd14-7c74d36b4865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# The variables are also accessible through nice accessors\n",
        "layer.kernel, layer.bias"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Variable 'dense_1/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
              " array([[-0.10956973, -0.32350817,  0.33780086, -0.355431  , -0.595714  ,\n",
              "         -0.3596113 , -0.19393778, -0.04922861,  0.22064978, -0.2343215 ],\n",
              "        [-0.06548589, -0.14383852, -0.146968  ,  0.36432648, -0.49617139,\n",
              "         -0.22814971,  0.11680913, -0.54332936,  0.12799722,  0.53414744],\n",
              "        [-0.33322757,  0.31441182, -0.5343494 ,  0.4118405 ,  0.44340605,\n",
              "          0.46069974, -0.23017013, -0.17695966, -0.4530635 , -0.49714383],\n",
              "        [-0.30850297, -0.14670593,  0.3785556 , -0.03918439,  0.43342572,\n",
              "          0.12621808,  0.5813835 , -0.22612536, -0.43294474, -0.54165995],\n",
              "        [ 0.29879034,  0.01249349, -0.5123859 , -0.06109631,  0.14176685,\n",
              "         -0.14670292, -0.23124829,  0.4585678 ,  0.6174125 , -0.52170664]],\n",
              "       dtype=float32)>,\n",
              " <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1R8z8Rv2Yys",
        "colab_type": "text"
      },
      "source": [
        "### Implementing custom layers\n",
        "The best way to implement your own layer is extending the tf.keras.Layer class and implementing:\n",
        "  *  `__init__` , where you can do all input-independent initialization\n",
        "  * `build`, where you know the shapes of the input tensors and can do the rest of the initialization\n",
        "  * `call`, where you do the forward computation\n",
        "\n",
        "Note that you don't have to wait until `build` is called to create your variables, you can also create them in `__init__`. However, \n",
        "* The advantage of creating them in `build` is that it enables **late variable creation** based on the shape of the inputs the layer will operate on. \n",
        "* On the other hand, creating variables in `__init__` would mean that **shapes required to create the variables will need to be explicitly specified**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJO3Aorn24gg",
        "colab_type": "code",
        "outputId": "e67a0e4d-2ee9-4534-a770-b4d0eb892090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "class MyDenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_outputs):\n",
        "    super(MyDenseLayer, self).__init__()\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_weight(\"kernel\",\n",
        "                                    shape=[int(input_shape[-1]),\n",
        "                                           self.num_outputs])\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.matmul(input, self.kernel)\n",
        "\n",
        "layer = MyDenseLayer(10)\n",
        "print(layer(tf.zeros([10, 5])))\n",
        "print(layer.trainable_variables)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n",
            "[<tf.Variable 'my_dense_layer_3/kernel:0' shape=(5, 10) dtype=float32, numpy=\n",
            "array([[-0.31258893,  0.38091224,  0.41181916, -0.61967224,  0.17612201,\n",
            "         0.06819528,  0.15260917,  0.38091964,  0.3039915 ,  0.2989155 ],\n",
            "       [ 0.47723454,  0.3802932 , -0.14199075,  0.5533703 ,  0.03621048,\n",
            "         0.17587095, -0.3106733 , -0.21331221,  0.33207327, -0.3939585 ],\n",
            "       [ 0.33359325,  0.5275132 ,  0.16026789, -0.11183548,  0.39218384,\n",
            "        -0.3064334 , -0.22519907,  0.10764974, -0.56869465,  0.5773694 ],\n",
            "       [-0.5526781 ,  0.10882616,  0.10456622, -0.27670157, -0.06649667,\n",
            "         0.07259607,  0.28208548,  0.3483944 ,  0.4844193 , -0.5252708 ],\n",
            "       [-0.45750678, -0.36479905,  0.19719434,  0.09904104,  0.61380774,\n",
            "         0.20908588, -0.5946978 ,  0.34502304,  0.61252993, -0.12381524]],\n",
            "      dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkBFCZ6J4cyI",
        "colab_type": "text"
      },
      "source": [
        "## Part 3: Automatic Differentiation\n",
        "\n",
        "Automatic differentiation is a key technique for optimizing ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQTcTknw4viF",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Gradient tapes\n",
        "\n",
        "TensorFlow provides the [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) API for automatic differentiation - computing the gradient of a computation with respect to its input variables. \n",
        "* Tensorflow \"records\" all operations executed inside the context of a `tf.GradientTape` onto a \"tape\". \n",
        "* Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzigoMfx8lyo",
        "colab_type": "text"
      },
      "source": [
        "For each example shown here, we will construct a computation graph and work out the derivatives.  We compare the manually calculated derivatives with automatic differentiation. \n",
        "\n",
        "Any computation wrapped in a tape is recorded, and after the operations have happened you can request gradients from any Tensor to any variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DmH74el8jYo",
        "colab_type": "code",
        "outputId": "dfaa5461-81e6-4295-f8c6-cc47034b7853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x = tf.Variable(1.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x ** 2\n",
        "  z = y ** 2\n",
        "dz_dx, dz_dy = tape.gradient(z, [x, y])\n",
        "print(dz_dx.numpy())  # 4.0\n",
        "print(dz_dy.numpy())  # 2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.0\n",
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6BUJilo8tpm",
        "colab_type": "text"
      },
      "source": [
        "If you want to request a gradient from a tensor to another tensor, then you need to tell the tape to watch the source tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aag8d8U8xQJ",
        "colab_type": "code",
        "outputId": "476283ef-df9e-4a8a-d385-ebe48360d520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = tf.constant(1.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)  # Since `x` is not a variable we must explicitly \"watch\" it.\n",
        "  y = x ** 2\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())  # 2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaoS1Ze89Fbx",
        "colab_type": "text"
      },
      "source": [
        "By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a `persistent` gradient tape. This allows multiple calls to the `gradient()` method as resources are released when the tape object is garbage collected. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DinblQ0e9IDa",
        "colab_type": "code",
        "outputId": "15f6603d-a609-4872-ea5b-5f6e812ae562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "  t.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n",
        "dy_dx = t.gradient(y, x)  # 6.0\n",
        "\n",
        "print (dy_dx)\n",
        "print (dz_dy)\n",
        "print (dz_dx)\n",
        "\n",
        "del t  # Drop the reference to the tape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "tf.Tensor(2.0, shape=(), dtype=float32)\n",
            "tf.Tensor(108.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JLWc0bv9iWr",
        "colab_type": "text"
      },
      "source": [
        "### Recording control flow\n",
        "\n",
        "Because tapes record operations as they are executed, Python control flow (using `if`s and `while`s for example) is naturally handled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtM7dlSL9jdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x, y):\n",
        "  output = 1.0\n",
        "  for i in range(y):\n",
        "    if i > 1 and i < 5:\n",
        "      output = tf.multiply(output, x)\n",
        "  return output\n",
        "\n",
        "def grad(x, y):\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch(x)\n",
        "    out = f(x, y)\n",
        "  return t.gradient(out, x)\n",
        "\n",
        "x = tf.convert_to_tensor(2.0)\n",
        "\n",
        "assert grad(x, 6).numpy() == 12.0\n",
        "assert grad(x, 5).numpy() == 12.0\n",
        "assert grad(x, 4).numpy() == 4.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu8_oh3j9ms0",
        "colab_type": "text"
      },
      "source": [
        "### Higher-order gradients\n",
        "\n",
        "Operations inside of the `GradientTape` context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBmxDlos9tkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
        "\n",
        "with tf.GradientTape() as t:\n",
        "  with tf.GradientTape() as t2:\n",
        "    y = x * x * x\n",
        "  # Compute the gradient inside the 't' context manager\n",
        "  # which means the gradient computation is differentiable as well.\n",
        "  dy_dx = t2.gradient(y, x)\n",
        "d2y_dx2 = t.gradient(dy_dx, x)\n",
        "\n",
        "assert dy_dx.numpy() == 3.0\n",
        "assert d2y_dx2.numpy() == 6.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7Qg4NUJ9tOd",
        "colab_type": "text"
      },
      "source": [
        "In this section, we covered gradient computation in TensorFlow. With that we have enough of the primitives required to build and train neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5nxaxyfCUpo",
        "colab_type": "text"
      },
      "source": [
        "## Part 4: Custom Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShLqQyM2EAl9",
        "colab_type": "text"
      },
      "source": [
        "Let's train neural networks from the first principle so as to acquire strong foundational understanding of the concepts.  We use `tf.Variable` to represent weights in a model.  A `tf.Variable` object stores a value and implicitly reads from this stored value. There are operations (`tf.assign_sub`, `tf.scatter_update`, etc.) that manipulate the value stored in a TensorFlow variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SHvo-wpEhQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = tf.Variable(1.0)\n",
        "# Use Python's `assert` as a debugging statement to test the condition\n",
        "assert v.numpy() == 1.0\n",
        "\n",
        "# Reassign the value `v`\n",
        "v.assign(3.0)\n",
        "assert v.numpy() == 3.0\n",
        "\n",
        "# Use `v` in a TensorFlow `tf.square()` operation and reassign\n",
        "v.assign(tf.square(v))\n",
        "assert v.numpy() == 9.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMOIgpU1FSHv",
        "colab_type": "text"
      },
      "source": [
        "### Fit a linear model\n",
        "\n",
        "Let's use the concepts you have learned so far—`Tensor`, `Variable`, and `GradientTape`—to build and train a simple model. This typically involves a few steps:\n",
        "\n",
        "1. Define the model.\n",
        "2. Define a loss function.\n",
        "3. Obtain training data.\n",
        "4. Run through the training data and use an \"optimizer\" to adjust the variables to fit the data.\n",
        "\n",
        "Here, you'll create a simple linear model, `f(x) = x * W + b`, which has two variables: `W` (weights) and `b` (bias). You'll synthesize data such that a well trained model would have `W = 3.0` and `b = 2.0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlhnnZ_hFf1b",
        "colab_type": "text"
      },
      "source": [
        "### Define the model\n",
        "\n",
        "Let's define a simple class to encapsulate the variables and the computation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNY-GbPeFgnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    # Initialize the weights to `5.0` and the bias to `0.0`\n",
        "    # In practice, these should be initialized to random values (for example, with `tf.random.normal`)\n",
        "    self.W = tf.Variable(5.0)\n",
        "    self.b = tf.Variable(0.0)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.W * x + self.b\n",
        "\n",
        "model = Model()\n",
        "\n",
        "assert model(3.0).numpy() == 15.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tinKaU_AFjQi",
        "colab_type": "text"
      },
      "source": [
        "### Define a loss function\n",
        "\n",
        "A loss function measures how well the output of a model for a given input matches the target output. The goal is to minimize this difference during training. Let's use the standard L2 loss, also known as the least square errors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPebWIUOFp-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(predicted_y, target_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apDUwbicF9Vp",
        "colab_type": "text"
      },
      "source": [
        "### Obtain training data\n",
        "\n",
        "First, synthesize the training data by adding random Gaussian (Normal) noise to the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzsc88N_F-SL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRUE_W = 3.0\n",
        "TRUE_b = 2.0\n",
        "NUM_EXAMPLES = 1000\n",
        "\n",
        "inputs  = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "noise   = tf.random.normal(shape=[NUM_EXAMPLES])\n",
        "outputs = inputs * TRUE_W + TRUE_b + noise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdY4fFjyFtLA",
        "colab_type": "text"
      },
      "source": [
        "Before training the model, visualize the loss value by plotting the model's predictions in red and the training data in blue:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Xho-bCFwKM",
        "colab_type": "code",
        "outputId": "1388a33a-bf36-4007-e5f0-b564eab3d6a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(inputs, outputs, c='b')\n",
        "plt.scatter(inputs, model(inputs), c='r')\n",
        "plt.show()\n",
        "\n",
        "print('Current loss: %1.6f' % loss(model(inputs), outputs).numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFxEZEQr4IiIjQgFfRGREKOCLiIyI/w82+6X+6TMPzwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Current loss: 9.094963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffhROcZ1GD5i",
        "colab_type": "text"
      },
      "source": [
        "### Define a training loop\n",
        "\n",
        "With the network and training data, train the model using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) to update the weights variable (`W`) and the bias variable (`b`) to reduce the loss. There are many variants of the gradient descent scheme that are captured in `tf.train.Optimizer`—our recommended implementation. But in the spirit of building from first principles, here you will implement the basic math yourself with the help of `tf.GradientTape` for automatic differentiation and `tf.assign_sub` for decrementing a value (which combines `tf.assign` and `tf.sub`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jfq7vM1GKpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "  dW, db = t.gradient(current_loss, [model.W, model.b])\n",
        "  model.W.assign_sub(learning_rate * dW)\n",
        "  model.b.assign_sub(learning_rate * db)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v64BoMGUGRvb",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's repeatedly run through the training data and see how W and b evolve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPyFZB-nGT9w",
        "colab_type": "code",
        "outputId": "3215ab56-fff3-44d5-f5ec-78f8170506ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "model = Model()\n",
        "\n",
        "# Collect the history of W-values and b-values to plot later\n",
        "Ws, bs = [], []\n",
        "epochs = range(10)\n",
        "for epoch in epochs:\n",
        "  Ws.append(model.W.numpy())\n",
        "  bs.append(model.b.numpy())\n",
        "  current_loss = loss(model(inputs), outputs)\n",
        "\n",
        "  train(model, inputs, outputs, learning_rate=0.1)\n",
        "  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n",
        "        (epoch, Ws[-1], bs[-1], current_loss))\n",
        "\n",
        "# Let's plot it all\n",
        "plt.plot(epochs, Ws, 'r',\n",
        "         epochs, bs, 'b')\n",
        "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
        "         [TRUE_b] * len(epochs), 'b--')\n",
        "plt.legend(['W', 'b', 'True W', 'True b'])\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0: W=5.00 b=0.00, loss=9.09496\n",
            "Epoch  1: W=4.59 b=0.41, loss=6.10919\n",
            "Epoch  2: W=4.27 b=0.73, loss=4.21764\n",
            "Epoch  3: W=4.01 b=0.99, loss=3.01930\n",
            "Epoch  4: W=3.80 b=1.20, loss=2.26011\n",
            "Epoch  5: W=3.64 b=1.36, loss=1.77912\n",
            "Epoch  6: W=3.51 b=1.49, loss=1.47439\n",
            "Epoch  7: W=3.40 b=1.59, loss=1.28133\n",
            "Epoch  8: W=3.32 b=1.67, loss=1.15900\n",
            "Epoch  9: W=3.25 b=1.74, loss=1.08150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZvOsVZEHcE1",
        "colab_type": "text"
      },
      "source": [
        "## Homework\n",
        "\n",
        "Run [IRIS flower classification](https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough) colab notebook for end-to-end ML process understanding."
      ]
    }
  ]
}