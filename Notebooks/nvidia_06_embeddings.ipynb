{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDbRWyvFguRc"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U3dZ6VIZEUn"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\" style=\"text-align:center;\">**Notebook 6:** Embedding Models and Semantic Reasoning</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In our previous notebook, we progressed our efforts towards the large document domain and learned how to apply our same techniques for dealing with a large base of relevant content! Along the way, we noted some challenges — specifically in the area of on-the-fly interpretation — that we still couldn't solve with our existing techniques. We will use this notebook to shift our focus to a different approach to help us approach this objective: **embedding models**.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Familiarize yourself with embeddings — numerical representations of words, phrases, or documents — and how they enable deep learning models to process semantic meanings.\n",
    "\n",
    "- Learn how to apply these embedding models in large-scale document processing, enhancing our existing methods of document summarization and knowledge extraction.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- How do embeddings contribute to a deeper understanding of document chunks, especially when dealing with inconsistencies or text conversion issues?\n",
    "\n",
    "- How do we strike the right balance between detail and computational efficiency with embedding models? Are there ways to rephrase or canonicalize (standardize) your queries with LLMs? Alternatively, can we fine-tune them for specialized tasks?\n",
    "\n",
    "- How can embedding models complement the running state chains and knowledge bases we've previously developed? *(Topic of next the notebook)*\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68012,
     "status": "ok",
     "timestamp": 1703280292779,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "mo36Q8zRZ5i_",
    "outputId": "46ae6923-36d2-4a7f-a8f2-33f73755bd90"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10907,
     "status": "ok",
     "timestamp": 1703280303605,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "sOS0EYnPZ6yH",
    "outputId": "2e640e6f-3557-4654-b11f-b9aaf6f61e8c"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhnjt1nEsiwO"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Refreshing On Embedding Models\n",
    "\n",
    "In this section, we'll review the ideas surrounding natural language processing with deep learning to define what embedding models are and how they're related to the tools we've taken for granted thus far.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Understanding Latent Embeddings**\n",
    "\n",
    "Latent embeddings represent the middle ground in deep learning networks, bridging the gap between input and output. For instance, let's consider a lightweight two-layer network designed to classify [MNIST digits](https://en.wikipedia.org/wiki/MNIST_database). For this, the inputs and outputs might be flattened images and one-hot probability vectors, respectively. Then, the values produced by the first layer in this setup are the latent embeddings of the image which, by optimization, converge into useful representations for the final layer to use. This makes them **semantically rich embeddings** that may not be human-interpretable but can still be leveraged for their raw vector properties.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Word Embeddings: The Building Blocks of Language Models**\n",
    "\n",
    "Word embeddings are high-dimensional vector representations of individual words, forming the backbone of deep language models. These embeddings are created through an optimization process within an end-to-end pipeline tailored for specific tasks. [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is a prominent standalone example for those interested. In practical terms, a token from a language model's vocabulary gets mapped from a one-dimensional token index to an $d$-dimensional token embedding:\n",
    "\n",
    "$$\\text{Token Index} \\in \\mathbb{Z}_{0+} \\to \\text{Token Vector} \\in R^{v} \\to \\text{Token Embedding} \\in \\mathbb{R}^{d}$$\n",
    "\n",
    "For a sequence of $n$ tokens, this mapping extends to the entire sequence:\n",
    "\n",
    "$$\\text{Token Vector Sequence} \\in R^{n\\times v} \\to \\text{Embedding Sequence} \\in \\mathbb{R}^{n\\times d}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Sentence/Document Embeddings: Capturing Context and Meaning**\n",
    "\n",
    "When dealing with sentences or entire documents, embeddings play a crucial role in capturing context, meanings, and interactions between elements. Practically all large language models leverage a transformer-like architecture to generate these sentence/document embeddings.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Decoder Models in Language Generation**\n",
    "\n",
    "Decoder models, commonly used in chatbots and other language generation tasks, start by taking a sequence of tokens as input. They embed these tokens into a latent sequence, applying unidirectional reasoning to focus on a specific part of the output sequence. From this focused, semantically dense point, the model predicts the next token in the sequence:\n",
    "\n",
    "$$$$\n",
    "$$\\text{[ Next-Token Generation ]}$$\n",
    "$$\\text{Embedding Sequence} \\in \\mathbb{R}^{n\\times d} \\to \\text{Latent Sequence} \\in \\mathbb{R}^{n\\times d}$$\n",
    "$$(\\text{Latent Sequence})[\\text{last entry}] \\in \\mathbb{R}^{d} \\to \\text{Token Prediction} \\in \\mathbb{R}^{v}$$\n",
    "$$$$\n",
    "\n",
    "This process continues, collapsing the token prediction from a vector to a realized token and building up a predicted sequence until a termination condition — such as a length limit or stop token — gets satisfied.\n",
    "\n",
    "$$$$\n",
    "$$\\text{[ Autoregressive Generation ]}$$\n",
    "$$(\\text{Original + Predicted Embedding Sequence}) \\in \\mathbb{R}^{(n+1)*e} \\to \\text{Token Prediction} \\in \\mathbb{R}^{v}$$\n",
    "$$\\vdots$$\n",
    "$$(\\text{Original + Predicted Embedding Sequence}) \\in \\mathbb{R}^{(n+m)*e} \\to \\text{Token Prediction} \\in \\mathbb{R}^{v}$$\n",
    "$$$$\n",
    "<br>\n",
    "\n",
    "### **Encoder Models For Sequence Encoding**\n",
    "\n",
    "Encoder models use a bidirectional architecture, making them suitable for different types of tasks compared to decoder models. They are particularly effective in tasks like token or sequence prediction. Letting $c$ be a number of classes or regressional values:\n",
    "\n",
    "$$$$\n",
    "$$\\text{[ Per-Token Prediction ]}$$\n",
    "$$\\text{Embedding Sequence} \\in \\mathbb{R}^{n\\times d} \\to \\text{Latent Sequence} \\in \\mathbb{R}^{n\\times d} \\to \\text{Per-Token Predictions} \\in \\mathbb{R}^{n\\times c}$$\n",
    "\n",
    "$$$$\n",
    "$$\\text{[ Full-Sequence Prediction ]}$$\n",
    "$$\\text{Token Sequence} \\in \\mathbb{R}^{n\\times d} \\to \\text{Latent Sequence} \\in \\mathbb{R}^{n\\times d}$$\n",
    "$$(\\text{Latent Sequence})[\\text{first entry}] \\in \\mathbb{R}^{d} \\to \\text{Sequence Prediction} \\in \\mathbb{R}^{c}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/encoder-decoder.png\" width=1200px/>\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1lhswkAgb5TlDxezg3qDNZQKbOMGFz7H5\" width=1200px/> -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VVkFBPFUU0v"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Using An NVIDIAEmbeddings Model\n",
    "\n",
    "For this notebook, we will use an embedding model with two pathways: shorter-form \"query\" and longer-form \"passage\" pathways. In doing so, we will see how to properly reason about its outputs and utilize it to complement our existing LLM toolset!\n",
    "\n",
    "To pull in our model, we will once again take advantage of the AI foundation model endpoints and its integration with LangChain! This part of the notebook will closely resemble the [official NVIDIAEmbeddings documentation](https://python.langchain.com/docs/integrations/text_embedding/nvidia_ai_endpoints).\n",
    "\n",
    "\n",
    "At the time of writing, the current recommended model for this task is the [NVIDIA Retrieval QA Embedding model](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/nvolve-29k), which is a fine-tuned variant of the [E5-Large embedding model](https://huggingface.co/intfloat/e5-large) listed under the `nvolveqa_40k` designation. This designation may have changed, so check over the available list and identify the current recommendation.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Identifying Our Model**\n",
    "\n",
    "Among your available models, you should find an embedding model suitable for discriminating between human-readable passages. After identifying the endpoint of interest, create an `NVIDIAEmbeddings` instance that will connect to it and see what methods it provides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0cox65gUj3O"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "## Update the model name as necessary\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baJff0HGUv9K"
   },
   "source": [
    "### **Optimizing Data Embedding with the API**\n",
    "\n",
    "In leveraging LangChain's `Embeddings` interface with our NVIDIAEmbeddings model, we focus on the dual pathways of embedding both **queries** and **documents**. This distinction plays a pivotal role in how the data is processed and utilized in retrieval-based applications:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Query Embedding**\n",
    "- **Purpose**: Designed for embedding shorter-form or question-like material, such as a simple statement or a question.\n",
    "- **Method**: Utilizes `embed_query` for embedding each query individually.\n",
    "- **Role in Retrieval**: Functions as the \"key,\" facilitating the search or query process in a document retrieval framework.\n",
    "- **Usage Pattern**: Embedded dynamically, as needed, for comparison against a pre-processed collection of document embeddings.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Document Embedding**\n",
    "- **Purpose**: Tailored for longer-form or response-like content, including document chunks or paragraphs.\n",
    "- **Method**: Employs `embed_documents` for batch processing of documents.\n",
    "- **Role in Retrieval**: Acts as the \"value,\" representing the searchable content within the retrieval system.\n",
    "- **Usage Pattern**: Typically embedded en masse as a pre-processing step, creating a repository of document embeddings for future querying.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Underlying Similarities and Practical Application**\n",
    "\n",
    "Though distinct in their application, both processes share a core functionality: they process textual content into semantically rich, vectorized representations. The choice between `embed_query` and `embed_documents` hinges on the nature of the content and the intended use within the retrieval system.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Exploration with Example \"Queries\" and \"Documents\"**\n",
    "\n",
    "Let's consider a set of example queries and documents to kickstart our exploration and understand these processes in action. These examples are carefully selected to highlight interesting properties and showcase our embedding model's capabilities for general text reasoning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQr_-4fjUyZa"
   },
   "outputs": [],
   "source": [
    "# Example queries and documents\n",
    "queries = [\n",
    "    \"What's the weather like in Komchatka?\",\n",
    "    \"What kinds of food is Italy known for?\",\n",
    "    \"What's my name? I bet you don't remember...\",\n",
    "    \"What's the point of life anyways?\",\n",
    "    \"The point of life is to have fun :D\"\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"Komchatka's weather is cold, with long, severe winters.\",\n",
    "    \"Italy is famous for pasta, pizza, gelato, and espresso.\",\n",
    "    \"I can't recall personal names, only provide information.\",\n",
    "    \"Life's purpose varies, often seen as personal fulfillment.\",\n",
    "    \"Enjoying life's moments is indeed a wonderful approach.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6PPz3YRU4Pn"
   },
   "source": [
    "We can encode these passages via either the query or the document pathways. Since the method signatures differ due to their intended use cases, the syntax will be a bit different between the two options:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4346,
     "status": "ok",
     "timestamp": 1703280307941,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "64KDS0jgU555",
    "outputId": "bb12a52c-8478-4f3c-fafa-337ffd78b1c0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Embedding the queries\n",
    "q_embeddings = [embedder.embed_query(query) for query in queries]\n",
    "\n",
    "# Embedding the documents\n",
    "d_embeddings = embedder.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4UEVqS6U7_l"
   },
   "source": [
    "With our embeddings in hand, we can do a simple similarity check on the results to see which documents would have triggered as reasonable answers in a retrieval task. When your entries are ready, run the following code block to visualize the cross-similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 2589,
     "status": "ok",
     "timestamp": 1703280310522,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "WF3Exs6JU9ky",
    "outputId": "4a97aa87-6e06-42bf-f0ca-1689b0878f92"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def plot_cross_similarity_matrix(emb1, emb2):\n",
    "    # Compute the similarity matrix between embeddings1 and embeddings2\n",
    "    cross_similarity_matrix = cosine_similarity(np.array(emb1), np.array(emb2))\n",
    "\n",
    "    # Plotting the cross-similarity matrix\n",
    "    plt.imshow(cross_similarity_matrix, cmap='Greens', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Cross-Similarity Matrix\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_cross_similarity_matrix(q_embeddings, d_embeddings)\n",
    "plt.xlabel(\"Query Embeddings\")\n",
    "plt.ylabel(\"Document Embeddings\")\n",
    "plt.show()\n",
    "\n",
    "# queries = [\n",
    "#     \"What's the weather like in Komchatka?\",\n",
    "#     \"What kinds of food is Italy known for?\",\n",
    "#     \"What's my name? I bet you don't remember...\",\n",
    "#     \"What's the point of life anyways?\",\n",
    "#     \"The point of life is to have fun :D\"]\n",
    "# documents = [\n",
    "#     \"Komchatka's weather is cold, with long, severe winters.\",\n",
    "#     \"Italy is famous for pasta, pizza, gelato, and espresso.\",\n",
    "#     \"I can't recall personal names, only provide information.\",\n",
    "#     \"Life's purpose varies, often seen as personal fulfillment.\",\n",
    "#     \"Enjoying life's moments is indeed a wonderful approach.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KJgJi-1pBGv"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3: [Exercise]** A Synthetic - But More Realistic - Example\n",
    "\n",
    "Observe that the entries that could be perceived as good input/output pairs trigger relatively high similarity on embedding. It is worth mentioning that, depending on the encoder model convergence, the query and document pathway may or may not be significantly different:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 2955,
     "status": "ok",
     "timestamp": 1703280313466,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "ug1wCynFVLTu",
    "outputId": "0a2b59ef-6412-4a4c-af3a-cdac1f953e23"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_cross_similarity_matrix(\n",
    "    q_embeddings,\n",
    "    [embedder.embed_query(doc) for doc in documents]\n",
    ")\n",
    "plt.xlabel(\"Query Embeddings (of queries)\")\n",
    "plt.ylabel(\"Query Embeddings (of documents)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PZfE43OVM5i"
   },
   "source": [
    "The real utility of having a \"bi-encoder\" **in general** is that the second encoder can be trained to remain consistent with the first even if the format of the input starts deviating drastically. To help illustrate this, we can flesh out our documents into longer-form variations and try the same experiment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39297,
     "status": "ok",
     "timestamp": 1703280352758,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "WcWDupQoVORf",
    "outputId": "b31815cd-9883-49fb-9aa7-0b3c83b6aeff"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "instruct_llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "expound_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\",\n",
    "        \"Generate part of a longer story that could reasonably answer all\"\n",
    "        \" of these questions somewhere in its contents: {questions}\\n\"\n",
    "        \" Make sure the passage only answers the following concretely: {q1}.\"\n",
    "        \" Give it some weird formatting, and try not to answer the others.\"\n",
    "    ),\n",
    "])\n",
    "\n",
    "###############################################################################################\n",
    "## BEGIN TODO\n",
    "\n",
    "## TODO: flesh out documents into a more verbose form by implementing the expound_chain \n",
    "##  which takes advantage of the prompt and llm provided above.\n",
    "\n",
    "expound_chain = {}\n",
    "\n",
    "longer_docs = []\n",
    "for i, q in enumerate(queries):\n",
    "    ## TODO: Invoke the expound_chain pipeline as appropriate\n",
    "    longer_doc = \"\"\n",
    "    pprint(\"\\n\".join([f\"Document {i+1}:\", \"-\"*64, longer_doc]))\n",
    "    longer_docs += [longer_doc]\n",
    "\n",
    "## END TODO\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnMf8OoeVQJU"
   },
   "source": [
    "-----\n",
    "\n",
    "When you're happy with the longer-form documents, please run the code below to see how the embeddings compare. The results may be similar, but there is at least some mathematically-optimized relationship that might help improve retrieval results at a larger scale. This result will also vary depending on the bi-encoder model, so keep that in mind.\n",
    "\n",
    "In general, it's a good idea to default towards using the opposite encoder for its advertised use case and gravitate closer towards using the same encoder when you assume that the things you're comparing are similar in form and modality. To be clear, the deviation for **the model we tested** (nvolve-style) was only slightly different between the two options, but it's still a consideration to make.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 6551,
     "status": "ok",
     "timestamp": 1703280359298,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "9dNLQoGdVRnV",
    "outputId": "3e891ae7-4795-4887-dbf2-3789ba1035f0"
   },
   "outputs": [],
   "source": [
    "## At the time of writing, our embedding model supports up to 2048 tokens...\n",
    "longer_docs_cut = [doc[:2048] for doc in longer_docs]\n",
    "\n",
    "q_long_embs = [embedder._embed([doc], model_type='query')[0] for doc in longer_docs_cut]\n",
    "d_long_embs = [embedder._embed([doc], model_type='passage')[0] for doc in longer_docs_cut]\n",
    "\n",
    "## The difference for any particular example may be very small.\n",
    "## We've raised the similarity matrix to the power of 5 to try and spot a difference.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_cross_similarity_matrix(q_embeddings, q_long_embs)\n",
    "plt.xlabel(\"Query Embeddings (of queries)\")\n",
    "plt.ylabel(\"Query Embeddings (of long documents)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_cross_similarity_matrix(q_embeddings, d_long_embs)\n",
    "plt.xlabel(\"Query Embeddings (of queries)\")\n",
    "plt.ylabel(\"Document Embeddings (of long documents)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4ZQa2eRnx3C"
   },
   "source": [
    "<br>\n",
    "\n",
    "**NOTE:** To see two drastically different bi-encoder components, consider checking out the CLIP model in the [AI Foundation Models](https://catalog.ngc.nvidia.com/ai-foundation-models) directory. This bi-encoder pair synergizes over a much wider modality gap to connect the image and text modalities instead of the query and document ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjP76N4o1bUi"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Embeddings For Semantic Guardrails\n",
    "\n",
    "In the next notebook, we will start to use higher-level utilities which will take our embedding model and use it under the hood. With that being said, there are several important concepts that can still be explored while the raw methods are still fresh!\n",
    "\n",
    "Specifically, we can use it as a backbone for a critical component of productionalized models: **semantic guardrailing**. Specifically, we can use the embeddings to filter out messages that are unlikely to be useful (or are actively harmful) for our chatbot to answer!\n",
    "\n",
    "**This exercise has been segmented into [**`64_guardrails.ipynb`**](64_guardrails.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0fFoZEVnI94"
   },
   "source": [
    "-----\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "By the end of this notebook, you should be familiar with the value proposition of a semantic embedding model and understand how we can use it to search through a dataset for relevant information!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "2. **[Advanced]** If you have the time, please check out and try to complete the **Notebook 6.4**, which covers semantic guardrails.\n",
    "3. Continue to the next video, which will discuss **Retrieval with Vectorstores**.\n",
    "4. After the video, go on to the corresponding notebook on **Retrieval with Vectorstores**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZynySFaVuLs"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
